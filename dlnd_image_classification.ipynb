{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 4\n",
    "sample_id = 9999\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    a = 0\n",
    "    b = 1\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + ( ( (x - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min ) )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    y = np.zeros((len(x), 10))\n",
    "    for i in range(len(x)):\n",
    "        y[i,x[i]] = 1\n",
    "    return y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # tf.placeholder(dtype, shape=None, name=None)\n",
    "    s = [None] + list(image_shape)\n",
    "    return tf.placeholder(tf.float32, shape=s, name='x')\n",
    "\"\"\"\n",
    "Notes:\n",
    "image_shape is a tuple(32,32,3).\n",
    "So currently in the following part,\n",
    "\n",
    "shape = [None, image_shape]\n",
    "you are actually doing this, and this is why you got the error.\n",
    "\n",
    "shape = [None, (32, 32, 3)]\n",
    "Instead, you want to change like following.\n",
    "\n",
    "shape = [None, 32, 32, 3]\n",
    "How can you fix this?\n",
    "\n",
    "Hint: [None] + list(tuple)\n",
    "Here are my suggestions and hints.\n",
    "Please use list concatenation( e.g. [1] + [2, 3, 4] will become [1, 2, 3, 4])\n",
    "Hint 1: image_shape is tuple of (32, 32, 3)\n",
    "Hint 2: Try [None] + list(tuple)\n",
    "\n",
    "I ended up using individual indecies: (None, tuple[0], tuple[1], tuple[2])\n",
    "\n",
    "You can also do it this way :\n",
    "tf.placeholder(tf.float32, shape = [None, *image_shape], name = \"x\")\n",
    "The * operator unpacks the tuple.\n",
    "\"\"\"\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # tf.placeholder(dtype, shape=None, name=None)\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,name=\"keep_prob\") # dropout (keep probability)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
    "    # TODO: Implement Function\n",
    "    # Create the weight and bias using conv_ksize, conv_num_outputs and the shape of x_tensor.\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0],    # height\n",
    "                                     conv_ksize[1],             # width\n",
    "                                     x_tensor.get_shape().as_list()[-1],   # input_depth\n",
    "                                     conv_num_outputs,], # output_depth\n",
    "                                     mean=0.0,\n",
    "                                     stddev=0.04,\n",
    "                                     dtype=tf.float32, \n",
    "                                     seed=None,\n",
    "                                     name=None))   \n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs)) # use number of outputs for the conv layer\n",
    "    \n",
    "    # conv_strides (batch, height, width, depth)\n",
    "    # set batch = 1 and depth = 1 (first and last positions should be 1s)\n",
    "    \n",
    "    # Apply a convolution to x_tensor using weight and conv_strides.\n",
    "    # We recommend you use same padding, but you're welcome to use any padding.\n",
    "    conv_layer0 = tf.nn.conv2d(x_tensor, \n",
    "                               weight, \n",
    "                               strides=[1, conv_strides[0], conv_strides[1], 1], \n",
    "                               padding='SAME')\n",
    "    \n",
    "    # Add bias\n",
    "    conv_layer1 = tf.nn.bias_add(conv_layer0, bias)\n",
    "    \n",
    "    # Apply activation function\n",
    "    # A non-linear function, the ReLU or rectified linear unit. \n",
    "    # The ReLU function is 0 for negative inputs and x for all inputs x>0. \n",
    "    conv_layer2 = tf.nn.relu(conv_layer1)\n",
    "    \n",
    "    # Apply Max Pooling using pool_ksize and pool_strides.\n",
    "    conv_layer3 = tf.nn.max_pool(conv_layer2, \n",
    "                                 ksize=[1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                                 strides=[1,pool_strides[0],pool_strides[1],1], \n",
    "                                 padding='SAME')\n",
    "\n",
    "    return conv_layer3 \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10, 30, 6)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # tf.contrib.layers.flatten(*args, **kwargs)\n",
    "    batch_size = -1\n",
    "    width = x_tensor.get_shape().as_list()[1]\n",
    "    height= x_tensor.get_shape().as_list()[2]\n",
    "    depth = x_tensor.get_shape().as_list()[3]\n",
    "    \n",
    "    print(x_tensor.get_shape()[:])\n",
    "    image_flat_size = width*height*depth\n",
    "    return tf.reshape(x_tensor, [batch_size, image_flat_size])\n",
    "\"\"\"\n",
    "Flatten is an operation as name implies should contain single array as output combining all other dimensions. \n",
    "For example if input is [W,H,D] then it would become [W*H*D]. Since batch size indicate the current samples \n",
    "that are proceed, it should remain intact. I.e. for every sample, there should be feature vector.\n",
    "\n",
    "Now based on above explanation, we know that if input shape is [batch_size, W, H, D] then output should be \n",
    "[batch_size, W*H*D]. In order to achieve this you can use tf.reshape. Here [-1, W*H*D] means \n",
    "[first_dimension, second_dimension]. Having -1 in the expression means that code will automatically \n",
    "calculate the value based on the available dimension. To explain in depth, tf knows that dimension is \n",
    "[batch_size*W*H*D] and in that [W*H*D] is specified, so it calculates the value and kept it where -1 is. \n",
    "Finally [-1, W*H*D] is nothing but [batch_size, W*H*D]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weights = tf.Variable(tf.truncated_normal((x_tensor.get_shape().as_list()[1], num_outputs)))\n",
    "    biases = tf.Variable(tf.zeros(num_outputs))\n",
    "    layer = tf.add(tf.matmul(x_tensor, weights), biases)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Output Layer - class prediction \n",
    "    weights = tf.Variable(tf.truncated_normal((x_tensor.get_shape().as_list()[1], num_outputs)))\n",
    "    biases = tf.Variable(tf.zeros(num_outputs))\n",
    "    out = tf.add(tf.matmul(x_tensor, weights), biases)\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 8, 8, 64)\n",
      "(?, 8, 8, 64)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    \n",
    "    x_tensor = x\n",
    "    conv_num_outputs = 64\n",
    "    conv_ksize = (5, 5)\n",
    "    conv_strides = (2, 2)\n",
    "    pool_ksize = (3, 3)\n",
    "    pool_strides = (2, 2)\n",
    "    \n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv2d_layer = conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flatten_layer = flatten(conv2d_layer)\n",
    "    #flat_layer_kp = tf.nn.dropout(flatten_layer, keep_prob)  # don't include this between layers as accuracy stays low.\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    num_outputs = 10\n",
    "    fully_connected = fully_conn(flatten_layer, num_outputs)\n",
    "    # fully_connected_kp = tf.nn.dropout(fully_connected, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    # TODO: return output\n",
    "    return output(fully_connected, num_outputs)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Run optimization \n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: 0.5})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})\n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.})\n",
    "    \n",
    "    print('loss', loss, 'accuracy', valid_acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBatch size is not about memory alone. You would have seen that in most the problem until now batch size is \\nconsidered as one| of the hyper parameters in tuning the network. So, that gives an inituation that changing \\nbatch size does affect the performance.\\n\\nGradient is updated after processing inputs of size batch. If you recollect, gradient will help you in finding \\nthe global minima. If the batch size is too large or it it occupies the complete training data, there is a \\nhigh probablity that you will end in the local minima. If you consider a batch size of one, then it is so noisely. \\nBut if you consider, batch size of 128 or 256, it is better than one but still noiser, when compared to complete \\ntraining data. But this noisiness help to get out local minima. So, pratically batch size are never taken as complete \\ntraining data both for memory and performance reason.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 220\n",
    "batch_size =784\n",
    "keep_probability = 0.5\n",
    "\n",
    "\"\"\"\n",
    "Batch size is not about memory alone. You would have seen that in most the problem until now batch size is \n",
    "considered as one| of the hyper parameters in tuning the network. So, that gives an inituation that changing \n",
    "batch size does affect the performance.\n",
    "\n",
    "Gradient is updated after processing inputs of size batch. If you recollect, gradient will help you in finding \n",
    "the global minima. If the batch size is too large or it it occupies the complete training data, there is a \n",
    "high probablity that you will end in the local minima. If you consider a batch size of one, then it is so noisely. \n",
    "But if you consider, batch size of 128 or 256, it is better than one but still noiser, when compared to complete \n",
    "training data. But this noisiness help to get out local minima. So, pratically batch size are never taken as complete \n",
    "training data both for memory and performance reason.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss 7.74553 accuracy 0.1092\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss 5.72819 accuracy 0.129\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss 4.61136 accuracy 0.1508\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss 4.00494 accuracy 0.1502\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss 3.51817 accuracy 0.1534\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss 3.21781 accuracy 0.1624\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss 2.94035 accuracy 0.1778\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss 2.70971 accuracy 0.1862\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss 2.54636 accuracy 0.1948\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss 2.42474 accuracy 0.2006\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss 2.3324 accuracy 0.2004\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss 2.25269 accuracy 0.2208\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss 2.21022 accuracy 0.2324\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss 2.16774 accuracy 0.244\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss 2.13358 accuracy 0.2478\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss 2.11074 accuracy 0.259\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss 2.06732 accuracy 0.2682\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss 2.03435 accuracy 0.2686\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss 2.00658 accuracy 0.282\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss 1.98632 accuracy 0.2802\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss 1.95615 accuracy 0.29\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss 1.93442 accuracy 0.2946\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss 1.91604 accuracy 0.3014\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss 1.88951 accuracy 0.3104\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss 1.86801 accuracy 0.309\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss 1.84099 accuracy 0.3194\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss 1.82994 accuracy 0.32\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss 1.80521 accuracy 0.3288\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss 1.78879 accuracy 0.3284\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss 1.76978 accuracy 0.3336\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss 1.75303 accuracy 0.3358\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss 1.75998 accuracy 0.3396\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss 1.73594 accuracy 0.3424\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss 1.71112 accuracy 0.3464\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss 1.70893 accuracy 0.3548\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss 1.67903 accuracy 0.3594\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss 1.67086 accuracy 0.3584\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss 1.65355 accuracy 0.3708\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss 1.64816 accuracy 0.362\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss 1.6286 accuracy 0.3738\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss 1.61522 accuracy 0.3736\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss 1.60023 accuracy 0.3778\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss 1.59014 accuracy 0.3808\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss 1.57845 accuracy 0.383\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss 1.55632 accuracy 0.3896\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss 1.54474 accuracy 0.3902\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss 1.53218 accuracy 0.3938\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss 1.51967 accuracy 0.3954\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss 1.50901 accuracy 0.3964\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss 1.49907 accuracy 0.3994\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss 1.48734 accuracy 0.4012\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss 1.47914 accuracy 0.4012\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss 1.46744 accuracy 0.4052\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss 1.46063 accuracy 0.4076\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss 1.45157 accuracy 0.4102\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss 1.44055 accuracy 0.412\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss 1.43562 accuracy 0.4144\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss 1.42865 accuracy 0.4188\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss 1.4224 accuracy 0.419\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss 1.4139 accuracy 0.4208\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss 1.4046 accuracy 0.4224\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss 1.39741 accuracy 0.4228\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss 1.39069 accuracy 0.4226\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss 1.38365 accuracy 0.4246\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss 1.37978 accuracy 0.4258\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss 1.36998 accuracy 0.428\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss 1.3683 accuracy 0.43\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss 1.36154 accuracy 0.4306\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss 1.36219 accuracy 0.4298\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss 1.35996 accuracy 0.4324\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss 1.353 accuracy 0.4356\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss 1.34534 accuracy 0.4356\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss 1.33898 accuracy 0.4342\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss 1.33585 accuracy 0.4336\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss 1.32752 accuracy 0.4374\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss 1.31853 accuracy 0.4358\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss 1.31058 accuracy 0.4386\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss 1.31308 accuracy 0.4372\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss 1.29845 accuracy 0.439\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss 1.29468 accuracy 0.4386\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss 1.28487 accuracy 0.441\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss 1.28165 accuracy 0.4396\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss 1.26787 accuracy 0.4416\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss 1.26824 accuracy 0.4438\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss 1.25742 accuracy 0.4416\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss 1.25051 accuracy 0.443\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss 1.24564 accuracy 0.4426\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss 1.24068 accuracy 0.4438\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss 1.23511 accuracy 0.4432\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss 1.22615 accuracy 0.4436\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss 1.22435 accuracy 0.4444\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss 1.21762 accuracy 0.446\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss 1.21274 accuracy 0.4458\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss 1.21241 accuracy 0.446\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss 1.20582 accuracy 0.4492\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss 1.20579 accuracy 0.4488\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss 1.20186 accuracy 0.447\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss 1.20351 accuracy 0.4464\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss 1.19239 accuracy 0.448\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss 1.19278 accuracy 0.4496\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss 1.19236 accuracy 0.4532\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss 1.19137 accuracy 0.4588\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss 1.18445 accuracy 0.4652\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss 1.1786 accuracy 0.4686\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss 1.16867 accuracy 0.4706\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss 1.16047 accuracy 0.4708\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss 1.15192 accuracy 0.4704\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss 1.14454 accuracy 0.4732\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss 1.13742 accuracy 0.4724\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss 1.13547 accuracy 0.4758\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss 1.13537 accuracy 0.4708\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss 1.13064 accuracy 0.4744\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss 1.12168 accuracy 0.4722\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss 1.11913 accuracy 0.4714\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss 1.11563 accuracy 0.471\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss 1.1122 accuracy 0.4758\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss 1.10969 accuracy 0.4738\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss 1.10346 accuracy 0.474\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss 1.09675 accuracy 0.4748\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss 1.09833 accuracy 0.475\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss 1.09326 accuracy 0.4758\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss 1.08781 accuracy 0.4744\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss 1.08555 accuracy 0.4736\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss 1.08399 accuracy 0.476\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss 1.08169 accuracy 0.4778\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss 1.07482 accuracy 0.48\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss 1.07117 accuracy 0.4802\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss 1.06644 accuracy 0.483\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss 1.06159 accuracy 0.481\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss 1.06075 accuracy 0.4804\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss 1.05365 accuracy 0.4762\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss 1.05201 accuracy 0.48\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss 1.04835 accuracy 0.4806\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss 1.0445 accuracy 0.4814\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss 1.03999 accuracy 0.4804\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss 1.03871 accuracy 0.4818\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss 1.03482 accuracy 0.4828\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss 1.0273 accuracy 0.485\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss 1.01907 accuracy 0.4894\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss 1.01817 accuracy 0.4846\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss 1.01454 accuracy 0.4838\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss 1.01181 accuracy 0.484\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss 1.00999 accuracy 0.4828\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss 1.0094 accuracy 0.4818\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss 1.00237 accuracy 0.483\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss 0.999465 accuracy 0.484\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss 0.992475 accuracy 0.4828\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss 0.98747 accuracy 0.4836\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss 0.984991 accuracy 0.4836\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss 0.976726 accuracy 0.484\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss 0.976132 accuracy 0.4844\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss 0.968366 accuracy 0.4856\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss 0.96355 accuracy 0.485\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss 0.959801 accuracy 0.486\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss 0.955986 accuracy 0.483\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss 0.955395 accuracy 0.4854\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss 0.951112 accuracy 0.4844\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss 0.942413 accuracy 0.4886\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss 0.9456 accuracy 0.484\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss 0.937955 accuracy 0.486\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss 0.933378 accuracy 0.4856\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss 0.933683 accuracy 0.4852\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss 0.933071 accuracy 0.4834\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss 0.932014 accuracy 0.4838\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss 0.923013 accuracy 0.4876\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss 0.923003 accuracy 0.4864\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss 0.921672 accuracy 0.487\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss 0.919926 accuracy 0.4854\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss 0.912512 accuracy 0.4888\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss 0.913163 accuracy 0.4874\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss 0.910619 accuracy 0.4894\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss 0.908671 accuracy 0.488\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss 0.902219 accuracy 0.4908\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss 0.897273 accuracy 0.494\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss 0.893824 accuracy 0.4938\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss 0.888095 accuracy 0.4944\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss 0.885034 accuracy 0.4936\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss 0.878044 accuracy 0.4972\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss 0.87642 accuracy 0.4962\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss 0.869614 accuracy 0.4954\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss 0.864936 accuracy 0.4976\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss 0.862053 accuracy 0.4942\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss 0.857907 accuracy 0.4972\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss 0.860482 accuracy 0.4926\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss 0.860056 accuracy 0.494\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss 0.861414 accuracy 0.4944\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss 0.851479 accuracy 0.4938\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss 0.846563 accuracy 0.4942\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss 0.844058 accuracy 0.495\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss 0.840648 accuracy 0.4976\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss 0.836148 accuracy 0.4986\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss 0.83184 accuracy 0.4984\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss 0.826682 accuracy 0.5002\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss 0.825298 accuracy 0.499\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss 0.823316 accuracy 0.4986\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss 0.822701 accuracy 0.4976\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss 0.818352 accuracy 0.4992\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss 0.811733 accuracy 0.5004\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss 0.809653 accuracy 0.5022\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss 0.806114 accuracy 0.5024\n",
      "Epoch 201, CIFAR-10 Batch 1:  loss 0.802545 accuracy 0.5014\n",
      "Epoch 202, CIFAR-10 Batch 1:  loss 0.798819 accuracy 0.5014\n",
      "Epoch 203, CIFAR-10 Batch 1:  loss 0.795505 accuracy 0.5024\n",
      "Epoch 204, CIFAR-10 Batch 1:  loss 0.794456 accuracy 0.502\n",
      "Epoch 205, CIFAR-10 Batch 1:  loss 0.791132 accuracy 0.5026\n",
      "Epoch 206, CIFAR-10 Batch 1:  loss 0.790904 accuracy 0.5022\n",
      "Epoch 207, CIFAR-10 Batch 1:  loss 0.786711 accuracy 0.5014\n",
      "Epoch 208, CIFAR-10 Batch 1:  loss 0.784676 accuracy 0.503\n",
      "Epoch 209, CIFAR-10 Batch 1:  loss 0.781564 accuracy 0.5024\n",
      "Epoch 210, CIFAR-10 Batch 1:  loss 0.778649 accuracy 0.5048\n",
      "Epoch 211, CIFAR-10 Batch 1:  loss 0.777307 accuracy 0.5042\n",
      "Epoch 212, CIFAR-10 Batch 1:  loss 0.775055 accuracy 0.5046\n",
      "Epoch 213, CIFAR-10 Batch 1:  loss 0.771822 accuracy 0.5024\n",
      "Epoch 214, CIFAR-10 Batch 1:  loss 0.768298 accuracy 0.504\n",
      "Epoch 215, CIFAR-10 Batch 1:  loss 0.765303 accuracy 0.5042\n",
      "Epoch 216, CIFAR-10 Batch 1:  loss 0.765074 accuracy 0.5042\n",
      "Epoch 217, CIFAR-10 Batch 1:  loss 0.762717 accuracy 0.505\n",
      "Epoch 218, CIFAR-10 Batch 1:  loss 0.762875 accuracy 0.5034\n",
      "Epoch 219, CIFAR-10 Batch 1:  loss 0.760872 accuracy 0.502\n",
      "Epoch 220, CIFAR-10 Batch 1:  loss 0.758584 accuracy 0.5046\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss 8.97683 accuracy 0.126\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss 5.53483 accuracy 0.1394\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss 4.3919 accuracy 0.1608\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss 3.64882 accuracy 0.1698\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss 3.37083 accuracy 0.1802\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss 3.10828 accuracy 0.197\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss 2.96377 accuracy 0.198\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss 2.54376 accuracy 0.2112\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss 2.56432 accuracy 0.2172\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss 2.52975 accuracy 0.224\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss 2.4298 accuracy 0.2312\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss 2.35488 accuracy 0.226\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss 2.16103 accuracy 0.2424\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss 2.20539 accuracy 0.2444\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss 2.19422 accuracy 0.2556\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss 2.17082 accuracy 0.2558\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss 2.12916 accuracy 0.2614\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss 2.05819 accuracy 0.268\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss 2.02465 accuracy 0.2736\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss 2.0354 accuracy 0.2648\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss 2.04908 accuracy 0.267\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss 2.01032 accuracy 0.2828\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss 1.98862 accuracy 0.2856\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss 1.92324 accuracy 0.2998\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss 1.95964 accuracy 0.2886\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss 1.98093 accuracy 0.2834\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss 1.93099 accuracy 0.291\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss 1.92617 accuracy 0.31\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss 1.86872 accuracy 0.3158\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss 1.90752 accuracy 0.3028\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss 1.93585 accuracy 0.2996\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss 1.87564 accuracy 0.3062\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss 1.86708 accuracy 0.3282\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss 1.82037 accuracy 0.3302\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss 1.86633 accuracy 0.3276\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss 1.89791 accuracy 0.313\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss 1.83065 accuracy 0.3178\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss 1.81513 accuracy 0.342\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss 1.78367 accuracy 0.3454\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss 1.84535 accuracy 0.3418\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss 1.85922 accuracy 0.331\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss 1.79283 accuracy 0.3342\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss 1.76471 accuracy 0.3514\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss 1.75013 accuracy 0.3526\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss 1.80312 accuracy 0.351\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss 1.83964 accuracy 0.3516\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss 1.7658 accuracy 0.3424\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss 1.72185 accuracy 0.358\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss 1.70115 accuracy 0.3682\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss 1.75865 accuracy 0.3632\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss 1.79536 accuracy 0.371\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss 1.73653 accuracy 0.3606\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss 1.68012 accuracy 0.371\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss 1.66979 accuracy 0.382\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss 1.72189 accuracy 0.371\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss 1.76442 accuracy 0.381\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss 1.71061 accuracy 0.3704\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss 1.64429 accuracy 0.3754\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss 1.63512 accuracy 0.385\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss 1.68793 accuracy 0.3778\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss 1.73619 accuracy 0.3886\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss 1.68212 accuracy 0.3768\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss 1.61069 accuracy 0.3806\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss 1.61184 accuracy 0.3956\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss 1.66241 accuracy 0.3906\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss 1.71522 accuracy 0.3932\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss 1.65796 accuracy 0.3806\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss 1.58302 accuracy 0.387\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss 1.59108 accuracy 0.404\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss 1.64922 accuracy 0.3986\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss 1.69753 accuracy 0.4002\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss 1.63405 accuracy 0.3884\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss 1.56202 accuracy 0.3994\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss 1.58443 accuracy 0.4082\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss 1.61604 accuracy 0.4042\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss 1.67905 accuracy 0.4054\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss 1.62206 accuracy 0.3962\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss 1.55163 accuracy 0.3968\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss 1.57151 accuracy 0.405\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss 1.61358 accuracy 0.4056\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss 1.64937 accuracy 0.4118\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss 1.58364 accuracy 0.4052\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss 1.53772 accuracy 0.4028\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss 1.55284 accuracy 0.4182\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss 1.58143 accuracy 0.4112\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss 1.63551 accuracy 0.4114\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss 1.57328 accuracy 0.406\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss 1.51856 accuracy 0.409\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss 1.53525 accuracy 0.4208\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss 1.56584 accuracy 0.4154\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss 1.61955 accuracy 0.4208\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss 1.54659 accuracy 0.4082\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss 1.50382 accuracy 0.4156\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss 1.52513 accuracy 0.426\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss 1.55298 accuracy 0.4182\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss 1.59843 accuracy 0.4258\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss 1.52961 accuracy 0.4162\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss 1.48128 accuracy 0.4234\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss 1.50402 accuracy 0.4262\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss 1.54091 accuracy 0.4242\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss 1.57297 accuracy 0.43\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss 1.5081 accuracy 0.4204\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss 1.47024 accuracy 0.4278\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss 1.48435 accuracy 0.4258\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss 1.51653 accuracy 0.4254\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss 1.55337 accuracy 0.4416\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss 1.48227 accuracy 0.435\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss 1.429 accuracy 0.4198\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss 1.47285 accuracy 0.434\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss 1.50694 accuracy 0.4322\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss 1.55095 accuracy 0.4402\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss 1.47571 accuracy 0.4312\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss 1.42872 accuracy 0.4368\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss 1.44045 accuracy 0.4434\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss 1.48412 accuracy 0.4372\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss 1.50362 accuracy 0.4484\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss 1.45082 accuracy 0.4346\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss 1.43094 accuracy 0.4304\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss 1.41572 accuracy 0.4452\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss 1.47684 accuracy 0.4426\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss 1.487 accuracy 0.4518\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss 1.4443 accuracy 0.446\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss 1.43914 accuracy 0.4312\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss 1.40036 accuracy 0.452\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss 1.4563 accuracy 0.4462\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss 1.47378 accuracy 0.4562\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss 1.42364 accuracy 0.4492\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss 1.40079 accuracy 0.4444\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss 1.38839 accuracy 0.453\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss 1.42425 accuracy 0.4546\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss 1.45212 accuracy 0.4584\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss 1.40847 accuracy 0.452\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss 1.38152 accuracy 0.4436\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss 1.36852 accuracy 0.4578\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss 1.42466 accuracy 0.455\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss 1.43925 accuracy 0.4606\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss 1.41151 accuracy 0.4552\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss 1.35956 accuracy 0.4526\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss 1.35022 accuracy 0.4586\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss 1.38516 accuracy 0.465\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss 1.43008 accuracy 0.4588\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss 1.38706 accuracy 0.4664\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss 1.34741 accuracy 0.4638\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss 1.32825 accuracy 0.4626\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss 1.37676 accuracy 0.4704\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss 1.41585 accuracy 0.4606\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss 1.37985 accuracy 0.471\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss 1.33012 accuracy 0.4596\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss 1.33666 accuracy 0.4656\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss 1.35392 accuracy 0.4766\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss 1.40284 accuracy 0.4634\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss 1.36488 accuracy 0.4734\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss 1.30871 accuracy 0.4712\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss 1.32022 accuracy 0.4726\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss 1.34003 accuracy 0.4826\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss 1.40329 accuracy 0.4652\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss 1.37677 accuracy 0.4772\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss 1.29311 accuracy 0.4654\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss 1.295 accuracy 0.4768\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss 1.3271 accuracy 0.4816\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss 1.37363 accuracy 0.47\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss 1.34785 accuracy 0.476\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss 1.27754 accuracy 0.471\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss 1.2813 accuracy 0.4768\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss 1.30585 accuracy 0.4854\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss 1.3722 accuracy 0.4778\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss 1.34098 accuracy 0.4808\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss 1.26163 accuracy 0.4762\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss 1.27724 accuracy 0.477\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss 1.29524 accuracy 0.4902\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss 1.34492 accuracy 0.4792\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss 1.33674 accuracy 0.478\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss 1.26975 accuracy 0.4744\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss 1.29101 accuracy 0.4762\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss 1.28794 accuracy 0.4912\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss 1.32222 accuracy 0.4908\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss 1.31303 accuracy 0.4796\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss 1.25343 accuracy 0.4844\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss 1.26705 accuracy 0.488\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss 1.26669 accuracy 0.4994\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss 1.33149 accuracy 0.4916\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss 1.30346 accuracy 0.4828\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss 1.25507 accuracy 0.4868\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss 1.25783 accuracy 0.4864\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss 1.27263 accuracy 0.4986\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss 1.32321 accuracy 0.4914\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss 1.30153 accuracy 0.4788\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss 1.25807 accuracy 0.4786\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss 1.24135 accuracy 0.4918\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss 1.26922 accuracy 0.5012\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss 1.32219 accuracy 0.4848\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss 1.28803 accuracy 0.4808\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss 1.2531 accuracy 0.4874\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss 1.23406 accuracy 0.4984\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss 1.2548 accuracy 0.502\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss 1.29984 accuracy 0.4944\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss 1.27864 accuracy 0.4888\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss 1.24091 accuracy 0.4996\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss 1.21237 accuracy 0.5044\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss 1.23441 accuracy 0.5044\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss 1.28914 accuracy 0.4992\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss 1.26235 accuracy 0.4944\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss 1.22104 accuracy 0.4994\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss 1.20886 accuracy 0.5106\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss 1.22428 accuracy 0.5096\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss 1.28762 accuracy 0.4992\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss 1.24511 accuracy 0.4986\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss 1.21958 accuracy 0.4982\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss 1.2041 accuracy 0.513\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss 1.21251 accuracy 0.511\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss 1.27251 accuracy 0.5048\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss 1.24598 accuracy 0.4978\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss 1.22853 accuracy 0.498\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss 1.19771 accuracy 0.5118\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss 1.20883 accuracy 0.5108\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss 1.27022 accuracy 0.5098\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss 1.23282 accuracy 0.5064\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss 1.20359 accuracy 0.5082\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss 1.18648 accuracy 0.5144\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss 1.19813 accuracy 0.5114\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss 1.25639 accuracy 0.5106\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss 1.21475 accuracy 0.5078\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss 1.19535 accuracy 0.511\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss 1.19478 accuracy 0.5104\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss 1.19405 accuracy 0.518\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss 1.25479 accuracy 0.515\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss 1.22623 accuracy 0.5082\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss 1.17364 accuracy 0.5132\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss 1.19045 accuracy 0.5134\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss 1.17576 accuracy 0.517\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss 1.24049 accuracy 0.5148\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss 1.21744 accuracy 0.5082\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss 1.16448 accuracy 0.516\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss 1.17242 accuracy 0.5184\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss 1.17064 accuracy 0.5252\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss 1.23471 accuracy 0.5188\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss 1.20229 accuracy 0.5064\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss 1.17325 accuracy 0.5172\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss 1.17418 accuracy 0.5228\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss 1.16954 accuracy 0.5198\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss 1.22897 accuracy 0.5228\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss 1.19915 accuracy 0.5014\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss 1.15546 accuracy 0.5184\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss 1.16674 accuracy 0.5192\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss 1.16764 accuracy 0.5198\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss 1.2162 accuracy 0.5212\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss 1.17231 accuracy 0.5174\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss 1.17321 accuracy 0.517\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss 1.17695 accuracy 0.517\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss 1.13977 accuracy 0.5364\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss 1.22767 accuracy 0.5258\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss 1.17416 accuracy 0.52\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss 1.16078 accuracy 0.5244\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss 1.15992 accuracy 0.518\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss 1.14006 accuracy 0.5338\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss 1.21874 accuracy 0.531\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss 1.18257 accuracy 0.5108\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss 1.14077 accuracy 0.5336\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss 1.13977 accuracy 0.5298\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss 1.13027 accuracy 0.5354\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss 1.22192 accuracy 0.5254\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss 1.15219 accuracy 0.5266\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss 1.15355 accuracy 0.5252\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss 1.13757 accuracy 0.529\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss 1.13017 accuracy 0.5356\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss 1.20548 accuracy 0.5312\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss 1.15393 accuracy 0.5226\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss 1.1535 accuracy 0.5254\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss 1.13137 accuracy 0.5296\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss 1.1199 accuracy 0.5428\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss 1.20694 accuracy 0.5314\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss 1.13256 accuracy 0.5336\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss 1.1414 accuracy 0.5276\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss 1.12839 accuracy 0.5314\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss 1.11316 accuracy 0.5386\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss 1.19526 accuracy 0.5342\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss 1.12386 accuracy 0.5378\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss 1.13334 accuracy 0.5352\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss 1.11373 accuracy 0.536\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss 1.09894 accuracy 0.5432\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss 1.18617 accuracy 0.534\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss 1.12767 accuracy 0.5262\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss 1.11316 accuracy 0.5346\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss 1.10507 accuracy 0.5428\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss 1.09361 accuracy 0.5414\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss 1.16875 accuracy 0.541\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss 1.10698 accuracy 0.5364\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss 1.11591 accuracy 0.531\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss 1.10904 accuracy 0.542\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss 1.08807 accuracy 0.5502\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss 1.16338 accuracy 0.5492\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss 1.10138 accuracy 0.5352\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss 1.10318 accuracy 0.5358\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss 1.08913 accuracy 0.5478\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss 1.09018 accuracy 0.5486\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss 1.15409 accuracy 0.5488\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss 1.08514 accuracy 0.5382\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss 1.09993 accuracy 0.5354\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss 1.09971 accuracy 0.5442\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss 1.09444 accuracy 0.5466\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss 1.15607 accuracy 0.5434\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss 1.09223 accuracy 0.5402\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss 1.09506 accuracy 0.5368\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss 1.08171 accuracy 0.5518\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss 1.08938 accuracy 0.5518\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss 1.14349 accuracy 0.546\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss 1.07908 accuracy 0.5406\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss 1.09815 accuracy 0.53\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss 1.07357 accuracy 0.5506\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss 1.07951 accuracy 0.5542\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss 1.14679 accuracy 0.5476\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss 1.07057 accuracy 0.5416\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss 1.06452 accuracy 0.5476\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss 1.06292 accuracy 0.5536\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss 1.06692 accuracy 0.5532\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss 1.13946 accuracy 0.5512\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss 1.05946 accuracy 0.541\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss 1.07536 accuracy 0.5356\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss 1.05988 accuracy 0.5532\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss 1.0552 accuracy 0.5558\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss 1.13767 accuracy 0.5546\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss 1.06924 accuracy 0.5436\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss 1.06264 accuracy 0.5384\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss 1.05209 accuracy 0.5588\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss 1.05408 accuracy 0.558\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss 1.12851 accuracy 0.5554\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss 1.0508 accuracy 0.5462\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss 1.0564 accuracy 0.541\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss 1.04873 accuracy 0.5568\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss 1.04465 accuracy 0.5606\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss 1.12145 accuracy 0.5536\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss 1.04193 accuracy 0.5466\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss 1.05472 accuracy 0.5466\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss 1.04019 accuracy 0.553\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss 1.03848 accuracy 0.5628\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss 1.12806 accuracy 0.5552\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss 1.05339 accuracy 0.5414\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss 1.05256 accuracy 0.541\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss 1.03277 accuracy 0.5638\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss 1.0407 accuracy 0.5618\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss 1.11417 accuracy 0.563\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss 1.04262 accuracy 0.5462\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss 1.05676 accuracy 0.5402\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss 1.04275 accuracy 0.5604\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss 1.03641 accuracy 0.5684\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss 1.1077 accuracy 0.5592\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss 1.04111 accuracy 0.5424\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss 1.05275 accuracy 0.5484\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss 1.04635 accuracy 0.5562\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss 1.02863 accuracy 0.5676\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss 1.11322 accuracy 0.5578\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss 1.04494 accuracy 0.5446\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss 1.02128 accuracy 0.5472\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss 1.02381 accuracy 0.563\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss 1.027 accuracy 0.5664\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss 1.10087 accuracy 0.5628\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss 1.03023 accuracy 0.556\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss 1.02184 accuracy 0.5538\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss 1.02144 accuracy 0.5646\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss 1.02753 accuracy 0.5724\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss 1.09313 accuracy 0.562\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss 1.01412 accuracy 0.5538\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss 1.02359 accuracy 0.5516\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss 1.02064 accuracy 0.5624\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss 1.01253 accuracy 0.5726\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss 1.08755 accuracy 0.563\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss 1.0056 accuracy 0.5608\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss 1.01922 accuracy 0.5498\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss 1.0253 accuracy 0.562\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss 1.01532 accuracy 0.5698\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss 1.08267 accuracy 0.562\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss 1.00697 accuracy 0.5616\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss 1.00266 accuracy 0.5612\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss 1.0132 accuracy 0.5654\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss 1.00911 accuracy 0.5698\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss 1.08208 accuracy 0.5598\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss 1.00616 accuracy 0.561\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss 1.00393 accuracy 0.5554\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss 1.00787 accuracy 0.5644\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss 1.00178 accuracy 0.5734\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss 1.08597 accuracy 0.564\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss 1.00114 accuracy 0.5632\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss 1.00527 accuracy 0.5604\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss 1.00027 accuracy 0.5674\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss 1.00446 accuracy 0.575\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss 1.06913 accuracy 0.5614\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss 0.995957 accuracy 0.567\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss 0.992531 accuracy 0.5614\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss 1.00372 accuracy 0.5704\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss 0.994577 accuracy 0.5734\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss 1.07215 accuracy 0.5622\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss 0.984189 accuracy 0.5668\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss 0.984863 accuracy 0.561\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss 0.97757 accuracy 0.5722\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss 0.992288 accuracy 0.5708\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss 1.06671 accuracy 0.567\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss 1.00269 accuracy 0.5612\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss 0.98789 accuracy 0.5642\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss 0.991303 accuracy 0.5696\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss 0.980424 accuracy 0.5782\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss 1.06127 accuracy 0.5694\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss 0.978667 accuracy 0.5712\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss 0.976268 accuracy 0.5672\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss 0.977361 accuracy 0.577\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss 0.980433 accuracy 0.5738\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss 1.06939 accuracy 0.5696\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss 0.981957 accuracy 0.562\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss 0.97419 accuracy 0.569\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss 0.981086 accuracy 0.5756\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss 0.987812 accuracy 0.5766\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss 1.04945 accuracy 0.5686\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss 0.963752 accuracy 0.5734\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss 0.962694 accuracy 0.5688\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss 0.970529 accuracy 0.5744\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss 0.972289 accuracy 0.5764\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss 1.06688 accuracy 0.5638\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss 0.955989 accuracy 0.5756\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss 0.961613 accuracy 0.5706\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss 0.979916 accuracy 0.5796\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss 0.9795 accuracy 0.5736\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss 1.05608 accuracy 0.5712\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss 0.959415 accuracy 0.5828\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss 0.982702 accuracy 0.5642\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss 0.975588 accuracy 0.5748\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss 0.970471 accuracy 0.5756\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss 1.04964 accuracy 0.569\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss 0.945303 accuracy 0.5804\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss 0.968851 accuracy 0.5648\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss 0.974043 accuracy 0.576\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss 0.966874 accuracy 0.5826\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss 1.02458 accuracy 0.5714\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss 0.928631 accuracy 0.5804\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss 0.97486 accuracy 0.5638\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss 0.960163 accuracy 0.5818\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss 0.957682 accuracy 0.5786\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss 1.03221 accuracy 0.5712\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss 0.935848 accuracy 0.5836\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss 0.9406 accuracy 0.5752\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss 0.952963 accuracy 0.582\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss 0.957729 accuracy 0.5774\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss 1.01306 accuracy 0.5744\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss 0.937405 accuracy 0.5826\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss 0.938018 accuracy 0.5746\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss 0.940034 accuracy 0.5818\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss 0.94431 accuracy 0.578\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss 1.0081 accuracy 0.5692\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss 0.934249 accuracy 0.582\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss 0.91713 accuracy 0.5852\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss 0.944642 accuracy 0.578\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss 0.952396 accuracy 0.5734\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss 0.992042 accuracy 0.5686\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss 0.937604 accuracy 0.5784\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss 0.903013 accuracy 0.5906\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss 0.937053 accuracy 0.5806\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss 0.947165 accuracy 0.5784\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss 0.99817 accuracy 0.5728\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss 0.938525 accuracy 0.5798\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss 0.895622 accuracy 0.592\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss 0.930562 accuracy 0.5842\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss 0.934021 accuracy 0.5824\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss 0.998011 accuracy 0.5762\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss 0.941148 accuracy 0.5768\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss 0.890134 accuracy 0.5924\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss 0.934969 accuracy 0.5804\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss 0.929871 accuracy 0.5846\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss 0.988015 accuracy 0.5754\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss 0.926042 accuracy 0.5814\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss 0.888339 accuracy 0.5956\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss 0.918161 accuracy 0.585\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss 0.919819 accuracy 0.5832\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss 0.988786 accuracy 0.5762\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss 0.925261 accuracy 0.5812\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss 0.886738 accuracy 0.5972\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss 0.9211 accuracy 0.584\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss 0.918463 accuracy 0.5856\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss 0.990322 accuracy 0.5752\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss 0.927549 accuracy 0.5806\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss 0.886319 accuracy 0.594\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss 0.925598 accuracy 0.5836\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss 0.916526 accuracy 0.5878\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss 0.993665 accuracy 0.5754\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss 0.91836 accuracy 0.5832\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss 0.88296 accuracy 0.5924\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss 0.915756 accuracy 0.5836\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss 0.910957 accuracy 0.59\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss 0.98443 accuracy 0.5778\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss 0.915404 accuracy 0.5848\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss 0.882517 accuracy 0.5886\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss 0.907591 accuracy 0.5852\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss 0.908643 accuracy 0.5922\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss 1.0001 accuracy 0.5718\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss 0.911856 accuracy 0.5822\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss 0.884155 accuracy 0.5928\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss 0.908717 accuracy 0.5858\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss 0.907944 accuracy 0.5948\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss 0.984903 accuracy 0.5762\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss 0.908778 accuracy 0.586\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss 0.877878 accuracy 0.59\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss 0.900502 accuracy 0.5916\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss 0.900887 accuracy 0.5924\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss 0.984278 accuracy 0.574\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss 0.908688 accuracy 0.5808\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss 0.873966 accuracy 0.5916\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss 0.895307 accuracy 0.592\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss 0.893487 accuracy 0.5932\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss 0.982667 accuracy 0.5736\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss 0.891039 accuracy 0.5816\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss 0.872144 accuracy 0.5904\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss 0.888689 accuracy 0.5922\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss 0.898317 accuracy 0.5912\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss 0.981615 accuracy 0.5774\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss 0.893448 accuracy 0.5818\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss 0.876722 accuracy 0.5916\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss 0.883694 accuracy 0.5938\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss 0.892006 accuracy 0.5928\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss 0.975664 accuracy 0.5736\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss 0.890143 accuracy 0.5806\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss 0.878528 accuracy 0.5916\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss 0.877198 accuracy 0.5952\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss 0.891992 accuracy 0.5906\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss 0.953708 accuracy 0.5816\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss 0.886204 accuracy 0.5832\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss 0.864338 accuracy 0.5922\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss 0.86931 accuracy 0.5938\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss 0.887162 accuracy 0.5916\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss 0.966795 accuracy 0.5794\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss 0.878753 accuracy 0.5848\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss 0.871002 accuracy 0.5912\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss 0.86389 accuracy 0.5976\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss 0.87913 accuracy 0.5954\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss 0.966146 accuracy 0.584\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss 0.871243 accuracy 0.5868\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss 0.882384 accuracy 0.5848\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss 0.872247 accuracy 0.5922\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss 0.871194 accuracy 0.5976\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss 0.965221 accuracy 0.5834\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss 0.876473 accuracy 0.581\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss 0.880977 accuracy 0.5834\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss 0.874927 accuracy 0.5946\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss 0.869185 accuracy 0.6006\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss 0.953699 accuracy 0.5822\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss 0.881388 accuracy 0.5832\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss 0.885655 accuracy 0.584\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss 0.878974 accuracy 0.5898\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss 0.874287 accuracy 0.5974\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss 0.955386 accuracy 0.5816\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss 0.891426 accuracy 0.5774\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss 0.870643 accuracy 0.5868\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss 0.873084 accuracy 0.5968\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss 0.872064 accuracy 0.5944\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss 0.962888 accuracy 0.5866\n",
      "Epoch 111, CIFAR-10 Batch 2:  loss 0.887471 accuracy 0.5802\n",
      "Epoch 111, CIFAR-10 Batch 3:  loss 0.869091 accuracy 0.5924\n",
      "Epoch 111, CIFAR-10 Batch 4:  loss 0.876772 accuracy 0.5914\n",
      "Epoch 111, CIFAR-10 Batch 5:  loss 0.866527 accuracy 0.6014\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss 0.940622 accuracy 0.5874\n",
      "Epoch 112, CIFAR-10 Batch 2:  loss 0.883864 accuracy 0.5848\n",
      "Epoch 112, CIFAR-10 Batch 3:  loss 0.854955 accuracy 0.5976\n",
      "Epoch 112, CIFAR-10 Batch 4:  loss 0.862537 accuracy 0.5936\n",
      "Epoch 112, CIFAR-10 Batch 5:  loss 0.869529 accuracy 0.597\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss 0.942768 accuracy 0.5846\n",
      "Epoch 113, CIFAR-10 Batch 2:  loss 0.877962 accuracy 0.5828\n",
      "Epoch 113, CIFAR-10 Batch 3:  loss 0.865347 accuracy 0.5964\n",
      "Epoch 113, CIFAR-10 Batch 4:  loss 0.878566 accuracy 0.58\n",
      "Epoch 113, CIFAR-10 Batch 5:  loss 0.867062 accuracy 0.599\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss 0.937664 accuracy 0.5838\n",
      "Epoch 114, CIFAR-10 Batch 2:  loss 0.873314 accuracy 0.581\n",
      "Epoch 114, CIFAR-10 Batch 3:  loss 0.849123 accuracy 0.5994\n",
      "Epoch 114, CIFAR-10 Batch 4:  loss 0.873487 accuracy 0.5798\n",
      "Epoch 114, CIFAR-10 Batch 5:  loss 0.86089 accuracy 0.5946\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss 0.946864 accuracy 0.5728\n",
      "Epoch 115, CIFAR-10 Batch 2:  loss 0.862001 accuracy 0.5828\n",
      "Epoch 115, CIFAR-10 Batch 3:  loss 0.846635 accuracy 0.597\n",
      "Epoch 115, CIFAR-10 Batch 4:  loss 0.87237 accuracy 0.5818\n",
      "Epoch 115, CIFAR-10 Batch 5:  loss 0.85967 accuracy 0.5964\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss 0.952147 accuracy 0.5746\n",
      "Epoch 116, CIFAR-10 Batch 2:  loss 0.84431 accuracy 0.5998\n",
      "Epoch 116, CIFAR-10 Batch 3:  loss 0.843732 accuracy 0.6018\n",
      "Epoch 116, CIFAR-10 Batch 4:  loss 0.877357 accuracy 0.5814\n",
      "Epoch 116, CIFAR-10 Batch 5:  loss 0.861352 accuracy 0.6\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss 0.947851 accuracy 0.5746\n",
      "Epoch 117, CIFAR-10 Batch 2:  loss 0.835955 accuracy 0.6012\n",
      "Epoch 117, CIFAR-10 Batch 3:  loss 0.838908 accuracy 0.597\n",
      "Epoch 117, CIFAR-10 Batch 4:  loss 0.855036 accuracy 0.5908\n",
      "Epoch 117, CIFAR-10 Batch 5:  loss 0.858885 accuracy 0.5972\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss 0.92548 accuracy 0.576\n",
      "Epoch 118, CIFAR-10 Batch 2:  loss 0.823521 accuracy 0.6012\n",
      "Epoch 118, CIFAR-10 Batch 3:  loss 0.844537 accuracy 0.596\n",
      "Epoch 118, CIFAR-10 Batch 4:  loss 0.848196 accuracy 0.59\n",
      "Epoch 118, CIFAR-10 Batch 5:  loss 0.849393 accuracy 0.5976\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss 0.927054 accuracy 0.5778\n",
      "Epoch 119, CIFAR-10 Batch 2:  loss 0.822551 accuracy 0.6046\n",
      "Epoch 119, CIFAR-10 Batch 3:  loss 0.832786 accuracy 0.6002\n",
      "Epoch 119, CIFAR-10 Batch 4:  loss 0.845038 accuracy 0.5908\n",
      "Epoch 119, CIFAR-10 Batch 5:  loss 0.85086 accuracy 0.5976\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss 0.90751 accuracy 0.582\n",
      "Epoch 120, CIFAR-10 Batch 2:  loss 0.806662 accuracy 0.6024\n",
      "Epoch 120, CIFAR-10 Batch 3:  loss 0.838714 accuracy 0.5934\n",
      "Epoch 120, CIFAR-10 Batch 4:  loss 0.846562 accuracy 0.592\n",
      "Epoch 120, CIFAR-10 Batch 5:  loss 0.842715 accuracy 0.6006\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss 0.904805 accuracy 0.581\n",
      "Epoch 121, CIFAR-10 Batch 2:  loss 0.804433 accuracy 0.6042\n",
      "Epoch 121, CIFAR-10 Batch 3:  loss 0.832269 accuracy 0.5958\n",
      "Epoch 121, CIFAR-10 Batch 4:  loss 0.845427 accuracy 0.592\n",
      "Epoch 121, CIFAR-10 Batch 5:  loss 0.83487 accuracy 0.6022\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss 0.889172 accuracy 0.584\n",
      "Epoch 122, CIFAR-10 Batch 2:  loss 0.797463 accuracy 0.603\n",
      "Epoch 122, CIFAR-10 Batch 3:  loss 0.833941 accuracy 0.5966\n",
      "Epoch 122, CIFAR-10 Batch 4:  loss 0.829899 accuracy 0.5934\n",
      "Epoch 122, CIFAR-10 Batch 5:  loss 0.834201 accuracy 0.6022\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss 0.890508 accuracy 0.5868\n",
      "Epoch 123, CIFAR-10 Batch 2:  loss 0.795243 accuracy 0.6036\n",
      "Epoch 123, CIFAR-10 Batch 3:  loss 0.829645 accuracy 0.5994\n",
      "Epoch 123, CIFAR-10 Batch 4:  loss 0.833284 accuracy 0.5918\n",
      "Epoch 123, CIFAR-10 Batch 5:  loss 0.840376 accuracy 0.5996\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss 0.883129 accuracy 0.5862\n",
      "Epoch 124, CIFAR-10 Batch 2:  loss 0.794001 accuracy 0.6098\n",
      "Epoch 124, CIFAR-10 Batch 3:  loss 0.824333 accuracy 0.6026\n",
      "Epoch 124, CIFAR-10 Batch 4:  loss 0.831761 accuracy 0.5966\n",
      "Epoch 124, CIFAR-10 Batch 5:  loss 0.824371 accuracy 0.6036\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss 0.877234 accuracy 0.5868\n",
      "Epoch 125, CIFAR-10 Batch 2:  loss 0.793549 accuracy 0.6036\n",
      "Epoch 125, CIFAR-10 Batch 3:  loss 0.826443 accuracy 0.5986\n",
      "Epoch 125, CIFAR-10 Batch 4:  loss 0.825096 accuracy 0.6028\n",
      "Epoch 125, CIFAR-10 Batch 5:  loss 0.818094 accuracy 0.6018\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss 0.875829 accuracy 0.5898\n",
      "Epoch 126, CIFAR-10 Batch 2:  loss 0.784988 accuracy 0.6074\n",
      "Epoch 126, CIFAR-10 Batch 3:  loss 0.823704 accuracy 0.6004\n",
      "Epoch 126, CIFAR-10 Batch 4:  loss 0.820977 accuracy 0.5968\n",
      "Epoch 126, CIFAR-10 Batch 5:  loss 0.816833 accuracy 0.6038\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss 0.87777 accuracy 0.5918\n",
      "Epoch 127, CIFAR-10 Batch 2:  loss 0.778392 accuracy 0.6088\n",
      "Epoch 127, CIFAR-10 Batch 3:  loss 0.819718 accuracy 0.5998\n",
      "Epoch 127, CIFAR-10 Batch 4:  loss 0.81776 accuracy 0.5988\n",
      "Epoch 127, CIFAR-10 Batch 5:  loss 0.811288 accuracy 0.603\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss 0.862419 accuracy 0.5916\n",
      "Epoch 128, CIFAR-10 Batch 2:  loss 0.773588 accuracy 0.6116\n",
      "Epoch 128, CIFAR-10 Batch 3:  loss 0.82061 accuracy 0.6034\n",
      "Epoch 128, CIFAR-10 Batch 4:  loss 0.814552 accuracy 0.6004\n",
      "Epoch 128, CIFAR-10 Batch 5:  loss 0.811043 accuracy 0.603\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss 0.863403 accuracy 0.594\n",
      "Epoch 129, CIFAR-10 Batch 2:  loss 0.785025 accuracy 0.613\n",
      "Epoch 129, CIFAR-10 Batch 3:  loss 0.815333 accuracy 0.5978\n",
      "Epoch 129, CIFAR-10 Batch 4:  loss 0.823706 accuracy 0.6008\n",
      "Epoch 129, CIFAR-10 Batch 5:  loss 0.814972 accuracy 0.6004\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss 0.84988 accuracy 0.5932\n",
      "Epoch 130, CIFAR-10 Batch 2:  loss 0.771853 accuracy 0.6092\n",
      "Epoch 130, CIFAR-10 Batch 3:  loss 0.813873 accuracy 0.5982\n",
      "Epoch 130, CIFAR-10 Batch 4:  loss 0.824751 accuracy 0.599\n",
      "Epoch 130, CIFAR-10 Batch 5:  loss 0.805956 accuracy 0.6016\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss 0.850128 accuracy 0.5934\n",
      "Epoch 131, CIFAR-10 Batch 2:  loss 0.779992 accuracy 0.6118\n",
      "Epoch 131, CIFAR-10 Batch 3:  loss 0.800392 accuracy 0.5996\n",
      "Epoch 131, CIFAR-10 Batch 4:  loss 0.814181 accuracy 0.6002\n",
      "Epoch 131, CIFAR-10 Batch 5:  loss 0.796753 accuracy 0.6062\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss 0.846125 accuracy 0.5948\n",
      "Epoch 132, CIFAR-10 Batch 2:  loss 0.772493 accuracy 0.6122\n",
      "Epoch 132, CIFAR-10 Batch 3:  loss 0.799288 accuracy 0.5996\n",
      "Epoch 132, CIFAR-10 Batch 4:  loss 0.808653 accuracy 0.6014\n",
      "Epoch 132, CIFAR-10 Batch 5:  loss 0.788855 accuracy 0.6078\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss 0.842581 accuracy 0.5934\n",
      "Epoch 133, CIFAR-10 Batch 2:  loss 0.76569 accuracy 0.6084\n",
      "Epoch 133, CIFAR-10 Batch 3:  loss 0.797976 accuracy 0.6\n",
      "Epoch 133, CIFAR-10 Batch 4:  loss 0.81291 accuracy 0.6012\n",
      "Epoch 133, CIFAR-10 Batch 5:  loss 0.788955 accuracy 0.6066\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss 0.832422 accuracy 0.5972\n",
      "Epoch 134, CIFAR-10 Batch 2:  loss 0.765534 accuracy 0.6142\n",
      "Epoch 134, CIFAR-10 Batch 3:  loss 0.794916 accuracy 0.5964\n",
      "Epoch 134, CIFAR-10 Batch 4:  loss 0.809759 accuracy 0.6036\n",
      "Epoch 134, CIFAR-10 Batch 5:  loss 0.785273 accuracy 0.6064\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss 0.837095 accuracy 0.5992\n",
      "Epoch 135, CIFAR-10 Batch 2:  loss 0.765789 accuracy 0.6138\n",
      "Epoch 135, CIFAR-10 Batch 3:  loss 0.804642 accuracy 0.5964\n",
      "Epoch 135, CIFAR-10 Batch 4:  loss 0.808126 accuracy 0.604\n",
      "Epoch 135, CIFAR-10 Batch 5:  loss 0.787497 accuracy 0.6054\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss 0.827005 accuracy 0.5974\n",
      "Epoch 136, CIFAR-10 Batch 2:  loss 0.764279 accuracy 0.6118\n",
      "Epoch 136, CIFAR-10 Batch 3:  loss 0.795868 accuracy 0.5974\n",
      "Epoch 136, CIFAR-10 Batch 4:  loss 0.809349 accuracy 0.6\n",
      "Epoch 136, CIFAR-10 Batch 5:  loss 0.784353 accuracy 0.6092\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss 0.818069 accuracy 0.603\n",
      "Epoch 137, CIFAR-10 Batch 2:  loss 0.757034 accuracy 0.61\n",
      "Epoch 137, CIFAR-10 Batch 3:  loss 0.793206 accuracy 0.5956\n",
      "Epoch 137, CIFAR-10 Batch 4:  loss 0.804694 accuracy 0.601\n",
      "Epoch 137, CIFAR-10 Batch 5:  loss 0.781387 accuracy 0.6142\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss 0.821259 accuracy 0.598\n",
      "Epoch 138, CIFAR-10 Batch 2:  loss 0.763683 accuracy 0.6102\n",
      "Epoch 138, CIFAR-10 Batch 3:  loss 0.790156 accuracy 0.599\n",
      "Epoch 138, CIFAR-10 Batch 4:  loss 0.812271 accuracy 0.595\n",
      "Epoch 138, CIFAR-10 Batch 5:  loss 0.790038 accuracy 0.611\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss 0.825991 accuracy 0.5986\n",
      "Epoch 139, CIFAR-10 Batch 2:  loss 0.757746 accuracy 0.611\n",
      "Epoch 139, CIFAR-10 Batch 3:  loss 0.784106 accuracy 0.6\n",
      "Epoch 139, CIFAR-10 Batch 4:  loss 0.809633 accuracy 0.5938\n",
      "Epoch 139, CIFAR-10 Batch 5:  loss 0.78437 accuracy 0.6132\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss 0.829791 accuracy 0.5944\n",
      "Epoch 140, CIFAR-10 Batch 2:  loss 0.757898 accuracy 0.6108\n",
      "Epoch 140, CIFAR-10 Batch 3:  loss 0.779322 accuracy 0.5988\n",
      "Epoch 140, CIFAR-10 Batch 4:  loss 0.803914 accuracy 0.5946\n",
      "Epoch 140, CIFAR-10 Batch 5:  loss 0.776645 accuracy 0.6098\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss 0.828124 accuracy 0.5934\n",
      "Epoch 141, CIFAR-10 Batch 2:  loss 0.763307 accuracy 0.6104\n",
      "Epoch 141, CIFAR-10 Batch 3:  loss 0.769702 accuracy 0.6046\n",
      "Epoch 141, CIFAR-10 Batch 4:  loss 0.80203 accuracy 0.5914\n",
      "Epoch 141, CIFAR-10 Batch 5:  loss 0.775081 accuracy 0.6102\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss 0.824521 accuracy 0.5968\n",
      "Epoch 142, CIFAR-10 Batch 2:  loss 0.759832 accuracy 0.6132\n",
      "Epoch 142, CIFAR-10 Batch 3:  loss 0.772394 accuracy 0.6012\n",
      "Epoch 142, CIFAR-10 Batch 4:  loss 0.798401 accuracy 0.5962\n",
      "Epoch 142, CIFAR-10 Batch 5:  loss 0.771477 accuracy 0.6126\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss 0.823498 accuracy 0.5988\n",
      "Epoch 143, CIFAR-10 Batch 2:  loss 0.754514 accuracy 0.6142\n",
      "Epoch 143, CIFAR-10 Batch 3:  loss 0.756067 accuracy 0.6054\n",
      "Epoch 143, CIFAR-10 Batch 4:  loss 0.786425 accuracy 0.5968\n",
      "Epoch 143, CIFAR-10 Batch 5:  loss 0.761566 accuracy 0.6108\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss 0.813878 accuracy 0.5996\n",
      "Epoch 144, CIFAR-10 Batch 2:  loss 0.753205 accuracy 0.6096\n",
      "Epoch 144, CIFAR-10 Batch 3:  loss 0.75471 accuracy 0.6024\n",
      "Epoch 144, CIFAR-10 Batch 4:  loss 0.77961 accuracy 0.6\n",
      "Epoch 144, CIFAR-10 Batch 5:  loss 0.764156 accuracy 0.6142\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss 0.813836 accuracy 0.6008\n",
      "Epoch 145, CIFAR-10 Batch 2:  loss 0.741757 accuracy 0.6144\n",
      "Epoch 145, CIFAR-10 Batch 3:  loss 0.756595 accuracy 0.605\n",
      "Epoch 145, CIFAR-10 Batch 4:  loss 0.781557 accuracy 0.6004\n",
      "Epoch 145, CIFAR-10 Batch 5:  loss 0.765505 accuracy 0.6106\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss 0.809711 accuracy 0.6006\n",
      "Epoch 146, CIFAR-10 Batch 2:  loss 0.7406 accuracy 0.6134\n",
      "Epoch 146, CIFAR-10 Batch 3:  loss 0.753232 accuracy 0.6064\n",
      "Epoch 146, CIFAR-10 Batch 4:  loss 0.775967 accuracy 0.5988\n",
      "Epoch 146, CIFAR-10 Batch 5:  loss 0.760952 accuracy 0.6116\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss 0.809156 accuracy 0.5978\n",
      "Epoch 147, CIFAR-10 Batch 2:  loss 0.738247 accuracy 0.6142\n",
      "Epoch 147, CIFAR-10 Batch 3:  loss 0.746656 accuracy 0.6042\n",
      "Epoch 147, CIFAR-10 Batch 4:  loss 0.775593 accuracy 0.5982\n",
      "Epoch 147, CIFAR-10 Batch 5:  loss 0.754702 accuracy 0.6132\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss 0.805364 accuracy 0.5994\n",
      "Epoch 148, CIFAR-10 Batch 2:  loss 0.73129 accuracy 0.6134\n",
      "Epoch 148, CIFAR-10 Batch 3:  loss 0.743912 accuracy 0.6042\n",
      "Epoch 148, CIFAR-10 Batch 4:  loss 0.778593 accuracy 0.5988\n",
      "Epoch 148, CIFAR-10 Batch 5:  loss 0.749862 accuracy 0.6114\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss 0.804942 accuracy 0.5968\n",
      "Epoch 149, CIFAR-10 Batch 2:  loss 0.73223 accuracy 0.612\n",
      "Epoch 149, CIFAR-10 Batch 3:  loss 0.744942 accuracy 0.6052\n",
      "Epoch 149, CIFAR-10 Batch 4:  loss 0.77635 accuracy 0.6006\n",
      "Epoch 149, CIFAR-10 Batch 5:  loss 0.742479 accuracy 0.613\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss 0.802907 accuracy 0.5992\n",
      "Epoch 150, CIFAR-10 Batch 2:  loss 0.727978 accuracy 0.6126\n",
      "Epoch 150, CIFAR-10 Batch 3:  loss 0.741022 accuracy 0.6058\n",
      "Epoch 150, CIFAR-10 Batch 4:  loss 0.773445 accuracy 0.6002\n",
      "Epoch 150, CIFAR-10 Batch 5:  loss 0.739215 accuracy 0.6156\n",
      "Epoch 151, CIFAR-10 Batch 1:  loss 0.79721 accuracy 0.5992\n",
      "Epoch 151, CIFAR-10 Batch 2:  loss 0.723741 accuracy 0.6144\n",
      "Epoch 151, CIFAR-10 Batch 3:  loss 0.738029 accuracy 0.604\n",
      "Epoch 151, CIFAR-10 Batch 4:  loss 0.770811 accuracy 0.6022\n",
      "Epoch 151, CIFAR-10 Batch 5:  loss 0.726154 accuracy 0.6176\n",
      "Epoch 152, CIFAR-10 Batch 1:  loss 0.786985 accuracy 0.602\n",
      "Epoch 152, CIFAR-10 Batch 2:  loss 0.721127 accuracy 0.613\n",
      "Epoch 152, CIFAR-10 Batch 3:  loss 0.730411 accuracy 0.6066\n",
      "Epoch 152, CIFAR-10 Batch 4:  loss 0.770311 accuracy 0.6014\n",
      "Epoch 152, CIFAR-10 Batch 5:  loss 0.728003 accuracy 0.6178\n",
      "Epoch 153, CIFAR-10 Batch 1:  loss 0.793204 accuracy 0.5982\n",
      "Epoch 153, CIFAR-10 Batch 2:  loss 0.725623 accuracy 0.612\n",
      "Epoch 153, CIFAR-10 Batch 3:  loss 0.735628 accuracy 0.601\n",
      "Epoch 153, CIFAR-10 Batch 4:  loss 0.76147 accuracy 0.6016\n",
      "Epoch 153, CIFAR-10 Batch 5:  loss 0.723679 accuracy 0.617\n",
      "Epoch 154, CIFAR-10 Batch 1:  loss 0.782589 accuracy 0.5994\n",
      "Epoch 154, CIFAR-10 Batch 2:  loss 0.724585 accuracy 0.613\n",
      "Epoch 154, CIFAR-10 Batch 3:  loss 0.7361 accuracy 0.6004\n",
      "Epoch 154, CIFAR-10 Batch 4:  loss 0.759524 accuracy 0.6032\n",
      "Epoch 154, CIFAR-10 Batch 5:  loss 0.71918 accuracy 0.6186\n",
      "Epoch 155, CIFAR-10 Batch 1:  loss 0.775325 accuracy 0.6034\n",
      "Epoch 155, CIFAR-10 Batch 2:  loss 0.72205 accuracy 0.6108\n",
      "Epoch 155, CIFAR-10 Batch 3:  loss 0.732823 accuracy 0.599\n",
      "Epoch 155, CIFAR-10 Batch 4:  loss 0.747859 accuracy 0.6058\n",
      "Epoch 155, CIFAR-10 Batch 5:  loss 0.714405 accuracy 0.617\n",
      "Epoch 156, CIFAR-10 Batch 1:  loss 0.763713 accuracy 0.607\n",
      "Epoch 156, CIFAR-10 Batch 2:  loss 0.718957 accuracy 0.6126\n",
      "Epoch 156, CIFAR-10 Batch 3:  loss 0.732992 accuracy 0.5974\n",
      "Epoch 156, CIFAR-10 Batch 4:  loss 0.748484 accuracy 0.6104\n",
      "Epoch 156, CIFAR-10 Batch 5:  loss 0.716499 accuracy 0.615\n",
      "Epoch 157, CIFAR-10 Batch 1:  loss 0.762851 accuracy 0.6074\n",
      "Epoch 157, CIFAR-10 Batch 2:  loss 0.714887 accuracy 0.6134\n",
      "Epoch 157, CIFAR-10 Batch 3:  loss 0.727566 accuracy 0.5974\n",
      "Epoch 157, CIFAR-10 Batch 4:  loss 0.740783 accuracy 0.6066\n",
      "Epoch 157, CIFAR-10 Batch 5:  loss 0.708418 accuracy 0.6162\n",
      "Epoch 158, CIFAR-10 Batch 1:  loss 0.757779 accuracy 0.6106\n",
      "Epoch 158, CIFAR-10 Batch 2:  loss 0.710069 accuracy 0.6148\n",
      "Epoch 158, CIFAR-10 Batch 3:  loss 0.715858 accuracy 0.601\n",
      "Epoch 158, CIFAR-10 Batch 4:  loss 0.735085 accuracy 0.606\n",
      "Epoch 158, CIFAR-10 Batch 5:  loss 0.709417 accuracy 0.6168\n",
      "Epoch 159, CIFAR-10 Batch 1:  loss 0.750136 accuracy 0.608\n",
      "Epoch 159, CIFAR-10 Batch 2:  loss 0.705827 accuracy 0.6116\n",
      "Epoch 159, CIFAR-10 Batch 3:  loss 0.714644 accuracy 0.5988\n",
      "Epoch 159, CIFAR-10 Batch 4:  loss 0.732903 accuracy 0.6102\n",
      "Epoch 159, CIFAR-10 Batch 5:  loss 0.705734 accuracy 0.6152\n",
      "Epoch 160, CIFAR-10 Batch 1:  loss 0.749964 accuracy 0.6126\n",
      "Epoch 160, CIFAR-10 Batch 2:  loss 0.703678 accuracy 0.6124\n",
      "Epoch 160, CIFAR-10 Batch 3:  loss 0.705679 accuracy 0.6038\n",
      "Epoch 160, CIFAR-10 Batch 4:  loss 0.731911 accuracy 0.6094\n",
      "Epoch 160, CIFAR-10 Batch 5:  loss 0.701498 accuracy 0.6164\n",
      "Epoch 161, CIFAR-10 Batch 1:  loss 0.747422 accuracy 0.6116\n",
      "Epoch 161, CIFAR-10 Batch 2:  loss 0.699811 accuracy 0.6136\n",
      "Epoch 161, CIFAR-10 Batch 3:  loss 0.711264 accuracy 0.6\n",
      "Epoch 161, CIFAR-10 Batch 4:  loss 0.72588 accuracy 0.6076\n",
      "Epoch 161, CIFAR-10 Batch 5:  loss 0.699736 accuracy 0.6144\n",
      "Epoch 162, CIFAR-10 Batch 1:  loss 0.74282 accuracy 0.612\n",
      "Epoch 162, CIFAR-10 Batch 2:  loss 0.697001 accuracy 0.611\n",
      "Epoch 162, CIFAR-10 Batch 3:  loss 0.696871 accuracy 0.6048\n",
      "Epoch 162, CIFAR-10 Batch 4:  loss 0.722858 accuracy 0.6114\n",
      "Epoch 162, CIFAR-10 Batch 5:  loss 0.693174 accuracy 0.6148\n",
      "Epoch 163, CIFAR-10 Batch 1:  loss 0.741028 accuracy 0.6126\n",
      "Epoch 163, CIFAR-10 Batch 2:  loss 0.696089 accuracy 0.615\n",
      "Epoch 163, CIFAR-10 Batch 3:  loss 0.695596 accuracy 0.6038\n",
      "Epoch 163, CIFAR-10 Batch 4:  loss 0.721671 accuracy 0.6084\n",
      "Epoch 163, CIFAR-10 Batch 5:  loss 0.694075 accuracy 0.6158\n",
      "Epoch 164, CIFAR-10 Batch 1:  loss 0.737303 accuracy 0.6086\n",
      "Epoch 164, CIFAR-10 Batch 2:  loss 0.691776 accuracy 0.613\n",
      "Epoch 164, CIFAR-10 Batch 3:  loss 0.689525 accuracy 0.6028\n",
      "Epoch 164, CIFAR-10 Batch 4:  loss 0.716075 accuracy 0.6112\n",
      "Epoch 164, CIFAR-10 Batch 5:  loss 0.68767 accuracy 0.6184\n",
      "Epoch 165, CIFAR-10 Batch 1:  loss 0.733779 accuracy 0.6084\n",
      "Epoch 165, CIFAR-10 Batch 2:  loss 0.691636 accuracy 0.6118\n",
      "Epoch 165, CIFAR-10 Batch 3:  loss 0.688766 accuracy 0.605\n",
      "Epoch 165, CIFAR-10 Batch 4:  loss 0.713407 accuracy 0.6096\n",
      "Epoch 165, CIFAR-10 Batch 5:  loss 0.68706 accuracy 0.6166\n",
      "Epoch 166, CIFAR-10 Batch 1:  loss 0.734132 accuracy 0.6094\n",
      "Epoch 166, CIFAR-10 Batch 2:  loss 0.693258 accuracy 0.6138\n",
      "Epoch 166, CIFAR-10 Batch 3:  loss 0.692716 accuracy 0.6046\n",
      "Epoch 166, CIFAR-10 Batch 4:  loss 0.712187 accuracy 0.6116\n",
      "Epoch 166, CIFAR-10 Batch 5:  loss 0.683419 accuracy 0.6152\n",
      "Epoch 167, CIFAR-10 Batch 1:  loss 0.731232 accuracy 0.6128\n",
      "Epoch 167, CIFAR-10 Batch 2:  loss 0.70278 accuracy 0.6154\n",
      "Epoch 167, CIFAR-10 Batch 3:  loss 0.688313 accuracy 0.603\n",
      "Epoch 167, CIFAR-10 Batch 4:  loss 0.705514 accuracy 0.6132\n",
      "Epoch 167, CIFAR-10 Batch 5:  loss 0.685964 accuracy 0.614\n",
      "Epoch 168, CIFAR-10 Batch 1:  loss 0.724288 accuracy 0.6112\n",
      "Epoch 168, CIFAR-10 Batch 2:  loss 0.694899 accuracy 0.6144\n",
      "Epoch 168, CIFAR-10 Batch 3:  loss 0.683858 accuracy 0.6018\n",
      "Epoch 168, CIFAR-10 Batch 4:  loss 0.702655 accuracy 0.6122\n",
      "Epoch 168, CIFAR-10 Batch 5:  loss 0.68625 accuracy 0.615\n",
      "Epoch 169, CIFAR-10 Batch 1:  loss 0.721401 accuracy 0.613\n",
      "Epoch 169, CIFAR-10 Batch 2:  loss 0.691415 accuracy 0.6122\n",
      "Epoch 169, CIFAR-10 Batch 3:  loss 0.680682 accuracy 0.6062\n",
      "Epoch 169, CIFAR-10 Batch 4:  loss 0.699147 accuracy 0.615\n",
      "Epoch 169, CIFAR-10 Batch 5:  loss 0.682624 accuracy 0.6166\n",
      "Epoch 170, CIFAR-10 Batch 1:  loss 0.720965 accuracy 0.61\n",
      "Epoch 170, CIFAR-10 Batch 2:  loss 0.695154 accuracy 0.6104\n",
      "Epoch 170, CIFAR-10 Batch 3:  loss 0.66923 accuracy 0.6102\n",
      "Epoch 170, CIFAR-10 Batch 4:  loss 0.695258 accuracy 0.6158\n",
      "Epoch 170, CIFAR-10 Batch 5:  loss 0.677946 accuracy 0.6184\n",
      "Epoch 171, CIFAR-10 Batch 1:  loss 0.716138 accuracy 0.6122\n",
      "Epoch 171, CIFAR-10 Batch 2:  loss 0.693548 accuracy 0.6122\n",
      "Epoch 171, CIFAR-10 Batch 3:  loss 0.670411 accuracy 0.6104\n",
      "Epoch 171, CIFAR-10 Batch 4:  loss 0.694871 accuracy 0.613\n",
      "Epoch 171, CIFAR-10 Batch 5:  loss 0.676882 accuracy 0.6178\n",
      "Epoch 172, CIFAR-10 Batch 1:  loss 0.713029 accuracy 0.612\n",
      "Epoch 172, CIFAR-10 Batch 2:  loss 0.694656 accuracy 0.6154\n",
      "Epoch 172, CIFAR-10 Batch 3:  loss 0.660618 accuracy 0.6084\n",
      "Epoch 172, CIFAR-10 Batch 4:  loss 0.690391 accuracy 0.617\n",
      "Epoch 172, CIFAR-10 Batch 5:  loss 0.66799 accuracy 0.6198\n",
      "Epoch 173, CIFAR-10 Batch 1:  loss 0.712162 accuracy 0.6108\n",
      "Epoch 173, CIFAR-10 Batch 2:  loss 0.684744 accuracy 0.6118\n",
      "Epoch 173, CIFAR-10 Batch 3:  loss 0.671074 accuracy 0.6094\n",
      "Epoch 173, CIFAR-10 Batch 4:  loss 0.686735 accuracy 0.6166\n",
      "Epoch 173, CIFAR-10 Batch 5:  loss 0.674565 accuracy 0.6202\n",
      "Epoch 174, CIFAR-10 Batch 1:  loss 0.706692 accuracy 0.6114\n",
      "Epoch 174, CIFAR-10 Batch 2:  loss 0.684155 accuracy 0.6172\n",
      "Epoch 174, CIFAR-10 Batch 3:  loss 0.674331 accuracy 0.6092\n",
      "Epoch 174, CIFAR-10 Batch 4:  loss 0.685204 accuracy 0.6144\n",
      "Epoch 174, CIFAR-10 Batch 5:  loss 0.671911 accuracy 0.6182\n",
      "Epoch 175, CIFAR-10 Batch 1:  loss 0.708036 accuracy 0.6094\n",
      "Epoch 175, CIFAR-10 Batch 2:  loss 0.683233 accuracy 0.615\n",
      "Epoch 175, CIFAR-10 Batch 3:  loss 0.680187 accuracy 0.607\n",
      "Epoch 175, CIFAR-10 Batch 4:  loss 0.685381 accuracy 0.6172\n",
      "Epoch 175, CIFAR-10 Batch 5:  loss 0.671997 accuracy 0.6176\n",
      "Epoch 176, CIFAR-10 Batch 1:  loss 0.700825 accuracy 0.6122\n",
      "Epoch 176, CIFAR-10 Batch 2:  loss 0.680072 accuracy 0.6176\n",
      "Epoch 176, CIFAR-10 Batch 3:  loss 0.674192 accuracy 0.6066\n",
      "Epoch 176, CIFAR-10 Batch 4:  loss 0.677691 accuracy 0.6188\n",
      "Epoch 176, CIFAR-10 Batch 5:  loss 0.670713 accuracy 0.6158\n",
      "Epoch 177, CIFAR-10 Batch 1:  loss 0.695693 accuracy 0.6144\n",
      "Epoch 177, CIFAR-10 Batch 2:  loss 0.681427 accuracy 0.6176\n",
      "Epoch 177, CIFAR-10 Batch 3:  loss 0.666082 accuracy 0.606\n",
      "Epoch 177, CIFAR-10 Batch 4:  loss 0.671627 accuracy 0.6202\n",
      "Epoch 177, CIFAR-10 Batch 5:  loss 0.670927 accuracy 0.6154\n",
      "Epoch 178, CIFAR-10 Batch 1:  loss 0.695063 accuracy 0.6152\n",
      "Epoch 178, CIFAR-10 Batch 2:  loss 0.679488 accuracy 0.6154\n",
      "Epoch 178, CIFAR-10 Batch 3:  loss 0.657946 accuracy 0.607\n",
      "Epoch 178, CIFAR-10 Batch 4:  loss 0.670881 accuracy 0.6208\n",
      "Epoch 178, CIFAR-10 Batch 5:  loss 0.664855 accuracy 0.616\n",
      "Epoch 179, CIFAR-10 Batch 1:  loss 0.691963 accuracy 0.6144\n",
      "Epoch 179, CIFAR-10 Batch 2:  loss 0.681434 accuracy 0.6186\n",
      "Epoch 179, CIFAR-10 Batch 3:  loss 0.652797 accuracy 0.609\n",
      "Epoch 179, CIFAR-10 Batch 4:  loss 0.665737 accuracy 0.6198\n",
      "Epoch 179, CIFAR-10 Batch 5:  loss 0.65966 accuracy 0.6196\n",
      "Epoch 180, CIFAR-10 Batch 1:  loss 0.690304 accuracy 0.6128\n",
      "Epoch 180, CIFAR-10 Batch 2:  loss 0.68109 accuracy 0.617\n",
      "Epoch 180, CIFAR-10 Batch 3:  loss 0.648621 accuracy 0.61\n",
      "Epoch 180, CIFAR-10 Batch 4:  loss 0.668113 accuracy 0.6188\n",
      "Epoch 180, CIFAR-10 Batch 5:  loss 0.660575 accuracy 0.619\n",
      "Epoch 181, CIFAR-10 Batch 1:  loss 0.689778 accuracy 0.6154\n",
      "Epoch 181, CIFAR-10 Batch 2:  loss 0.678681 accuracy 0.6146\n",
      "Epoch 181, CIFAR-10 Batch 3:  loss 0.638583 accuracy 0.6132\n",
      "Epoch 181, CIFAR-10 Batch 4:  loss 0.677684 accuracy 0.6174\n",
      "Epoch 181, CIFAR-10 Batch 5:  loss 0.653471 accuracy 0.6212\n",
      "Epoch 182, CIFAR-10 Batch 1:  loss 0.690668 accuracy 0.6106\n",
      "Epoch 182, CIFAR-10 Batch 2:  loss 0.68704 accuracy 0.6118\n",
      "Epoch 182, CIFAR-10 Batch 3:  loss 0.634621 accuracy 0.6138\n",
      "Epoch 182, CIFAR-10 Batch 4:  loss 0.680241 accuracy 0.6172\n",
      "Epoch 182, CIFAR-10 Batch 5:  loss 0.653153 accuracy 0.6212\n",
      "Epoch 183, CIFAR-10 Batch 1:  loss 0.699568 accuracy 0.6062\n",
      "Epoch 183, CIFAR-10 Batch 2:  loss 0.691259 accuracy 0.6108\n",
      "Epoch 183, CIFAR-10 Batch 3:  loss 0.640266 accuracy 0.6184\n",
      "Epoch 183, CIFAR-10 Batch 4:  loss 0.676564 accuracy 0.6164\n",
      "Epoch 183, CIFAR-10 Batch 5:  loss 0.657476 accuracy 0.616\n",
      "Epoch 184, CIFAR-10 Batch 1:  loss 0.700566 accuracy 0.6056\n",
      "Epoch 184, CIFAR-10 Batch 2:  loss 0.696793 accuracy 0.608\n",
      "Epoch 184, CIFAR-10 Batch 3:  loss 0.641692 accuracy 0.6138\n",
      "Epoch 184, CIFAR-10 Batch 4:  loss 0.667212 accuracy 0.6198\n",
      "Epoch 184, CIFAR-10 Batch 5:  loss 0.657935 accuracy 0.616\n",
      "Epoch 185, CIFAR-10 Batch 1:  loss 0.692984 accuracy 0.6106\n",
      "Epoch 185, CIFAR-10 Batch 2:  loss 0.696254 accuracy 0.6062\n",
      "Epoch 185, CIFAR-10 Batch 3:  loss 0.64427 accuracy 0.6156\n",
      "Epoch 185, CIFAR-10 Batch 4:  loss 0.676528 accuracy 0.6188\n",
      "Epoch 185, CIFAR-10 Batch 5:  loss 0.658808 accuracy 0.6206\n",
      "Epoch 186, CIFAR-10 Batch 1:  loss 0.695822 accuracy 0.6108\n",
      "Epoch 186, CIFAR-10 Batch 2:  loss 0.697111 accuracy 0.606\n",
      "Epoch 186, CIFAR-10 Batch 3:  loss 0.633604 accuracy 0.6134\n",
      "Epoch 186, CIFAR-10 Batch 4:  loss 0.665516 accuracy 0.6146\n",
      "Epoch 186, CIFAR-10 Batch 5:  loss 0.651342 accuracy 0.6188\n",
      "Epoch 187, CIFAR-10 Batch 1:  loss 0.686667 accuracy 0.6144\n",
      "Epoch 187, CIFAR-10 Batch 2:  loss 0.687195 accuracy 0.607\n",
      "Epoch 187, CIFAR-10 Batch 3:  loss 0.641921 accuracy 0.6146\n",
      "Epoch 187, CIFAR-10 Batch 4:  loss 0.664857 accuracy 0.6188\n",
      "Epoch 187, CIFAR-10 Batch 5:  loss 0.646516 accuracy 0.6216\n",
      "Epoch 188, CIFAR-10 Batch 1:  loss 0.678398 accuracy 0.616\n",
      "Epoch 188, CIFAR-10 Batch 2:  loss 0.685703 accuracy 0.6068\n",
      "Epoch 188, CIFAR-10 Batch 3:  loss 0.640285 accuracy 0.6172\n",
      "Epoch 188, CIFAR-10 Batch 4:  loss 0.658755 accuracy 0.6184\n",
      "Epoch 188, CIFAR-10 Batch 5:  loss 0.641452 accuracy 0.622\n",
      "Epoch 189, CIFAR-10 Batch 1:  loss 0.678836 accuracy 0.6174\n",
      "Epoch 189, CIFAR-10 Batch 2:  loss 0.676031 accuracy 0.6096\n",
      "Epoch 189, CIFAR-10 Batch 3:  loss 0.630945 accuracy 0.6196\n",
      "Epoch 189, CIFAR-10 Batch 4:  loss 0.659993 accuracy 0.6174\n",
      "Epoch 189, CIFAR-10 Batch 5:  loss 0.642708 accuracy 0.6194\n",
      "Epoch 190, CIFAR-10 Batch 1:  loss 0.678027 accuracy 0.6164\n",
      "Epoch 190, CIFAR-10 Batch 2:  loss 0.668865 accuracy 0.6094\n",
      "Epoch 190, CIFAR-10 Batch 3:  loss 0.632587 accuracy 0.6174\n",
      "Epoch 190, CIFAR-10 Batch 4:  loss 0.652019 accuracy 0.6192\n",
      "Epoch 190, CIFAR-10 Batch 5:  loss 0.633855 accuracy 0.6208\n",
      "Epoch 191, CIFAR-10 Batch 1:  loss 0.676994 accuracy 0.6142\n",
      "Epoch 191, CIFAR-10 Batch 2:  loss 0.664556 accuracy 0.6106\n",
      "Epoch 191, CIFAR-10 Batch 3:  loss 0.627221 accuracy 0.6198\n",
      "Epoch 191, CIFAR-10 Batch 4:  loss 0.649717 accuracy 0.6174\n",
      "Epoch 191, CIFAR-10 Batch 5:  loss 0.630226 accuracy 0.6196\n",
      "Epoch 192, CIFAR-10 Batch 1:  loss 0.674479 accuracy 0.617\n",
      "Epoch 192, CIFAR-10 Batch 2:  loss 0.658906 accuracy 0.6152\n",
      "Epoch 192, CIFAR-10 Batch 3:  loss 0.625713 accuracy 0.6184\n",
      "Epoch 192, CIFAR-10 Batch 4:  loss 0.64397 accuracy 0.6198\n",
      "Epoch 192, CIFAR-10 Batch 5:  loss 0.624266 accuracy 0.6218\n",
      "Epoch 193, CIFAR-10 Batch 1:  loss 0.67405 accuracy 0.618\n",
      "Epoch 193, CIFAR-10 Batch 2:  loss 0.651397 accuracy 0.6174\n",
      "Epoch 193, CIFAR-10 Batch 3:  loss 0.618209 accuracy 0.6194\n",
      "Epoch 193, CIFAR-10 Batch 4:  loss 0.635892 accuracy 0.6174\n",
      "Epoch 193, CIFAR-10 Batch 5:  loss 0.622798 accuracy 0.6224\n",
      "Epoch 194, CIFAR-10 Batch 1:  loss 0.670997 accuracy 0.6172\n",
      "Epoch 194, CIFAR-10 Batch 2:  loss 0.654683 accuracy 0.6168\n",
      "Epoch 194, CIFAR-10 Batch 3:  loss 0.618132 accuracy 0.6198\n",
      "Epoch 194, CIFAR-10 Batch 4:  loss 0.638542 accuracy 0.62\n",
      "Epoch 194, CIFAR-10 Batch 5:  loss 0.619065 accuracy 0.6238\n",
      "Epoch 195, CIFAR-10 Batch 1:  loss 0.668805 accuracy 0.618\n",
      "Epoch 195, CIFAR-10 Batch 2:  loss 0.648452 accuracy 0.6176\n",
      "Epoch 195, CIFAR-10 Batch 3:  loss 0.617525 accuracy 0.6194\n",
      "Epoch 195, CIFAR-10 Batch 4:  loss 0.636371 accuracy 0.6192\n",
      "Epoch 195, CIFAR-10 Batch 5:  loss 0.617287 accuracy 0.6216\n",
      "Epoch 196, CIFAR-10 Batch 1:  loss 0.664135 accuracy 0.6196\n",
      "Epoch 196, CIFAR-10 Batch 2:  loss 0.646582 accuracy 0.6192\n",
      "Epoch 196, CIFAR-10 Batch 3:  loss 0.617284 accuracy 0.6182\n",
      "Epoch 196, CIFAR-10 Batch 4:  loss 0.630153 accuracy 0.6192\n",
      "Epoch 196, CIFAR-10 Batch 5:  loss 0.615869 accuracy 0.622\n",
      "Epoch 197, CIFAR-10 Batch 1:  loss 0.664263 accuracy 0.617\n",
      "Epoch 197, CIFAR-10 Batch 2:  loss 0.643226 accuracy 0.6182\n",
      "Epoch 197, CIFAR-10 Batch 3:  loss 0.611545 accuracy 0.6192\n",
      "Epoch 197, CIFAR-10 Batch 4:  loss 0.625444 accuracy 0.619\n",
      "Epoch 197, CIFAR-10 Batch 5:  loss 0.611335 accuracy 0.6226\n",
      "Epoch 198, CIFAR-10 Batch 1:  loss 0.659916 accuracy 0.6188\n",
      "Epoch 198, CIFAR-10 Batch 2:  loss 0.640809 accuracy 0.6182\n",
      "Epoch 198, CIFAR-10 Batch 3:  loss 0.611393 accuracy 0.6174\n",
      "Epoch 198, CIFAR-10 Batch 4:  loss 0.629772 accuracy 0.6186\n",
      "Epoch 198, CIFAR-10 Batch 5:  loss 0.609044 accuracy 0.6216\n",
      "Epoch 199, CIFAR-10 Batch 1:  loss 0.658732 accuracy 0.6188\n",
      "Epoch 199, CIFAR-10 Batch 2:  loss 0.64258 accuracy 0.6158\n",
      "Epoch 199, CIFAR-10 Batch 3:  loss 0.611384 accuracy 0.6186\n",
      "Epoch 199, CIFAR-10 Batch 4:  loss 0.62192 accuracy 0.619\n",
      "Epoch 199, CIFAR-10 Batch 5:  loss 0.606532 accuracy 0.6222\n",
      "Epoch 200, CIFAR-10 Batch 1:  loss 0.655617 accuracy 0.6174\n",
      "Epoch 200, CIFAR-10 Batch 2:  loss 0.639561 accuracy 0.6192\n",
      "Epoch 200, CIFAR-10 Batch 3:  loss 0.603254 accuracy 0.619\n",
      "Epoch 200, CIFAR-10 Batch 4:  loss 0.620248 accuracy 0.6202\n",
      "Epoch 200, CIFAR-10 Batch 5:  loss 0.604503 accuracy 0.6214\n",
      "Epoch 201, CIFAR-10 Batch 1:  loss 0.651034 accuracy 0.6204\n",
      "Epoch 201, CIFAR-10 Batch 2:  loss 0.635581 accuracy 0.6194\n",
      "Epoch 201, CIFAR-10 Batch 3:  loss 0.604985 accuracy 0.619\n",
      "Epoch 201, CIFAR-10 Batch 4:  loss 0.622644 accuracy 0.6208\n",
      "Epoch 201, CIFAR-10 Batch 5:  loss 0.605132 accuracy 0.6216\n",
      "Epoch 202, CIFAR-10 Batch 1:  loss 0.652439 accuracy 0.6202\n",
      "Epoch 202, CIFAR-10 Batch 2:  loss 0.636301 accuracy 0.618\n",
      "Epoch 202, CIFAR-10 Batch 3:  loss 0.603909 accuracy 0.6212\n",
      "Epoch 202, CIFAR-10 Batch 4:  loss 0.618398 accuracy 0.6204\n",
      "Epoch 202, CIFAR-10 Batch 5:  loss 0.600728 accuracy 0.6232\n",
      "Epoch 203, CIFAR-10 Batch 1:  loss 0.649443 accuracy 0.618\n",
      "Epoch 203, CIFAR-10 Batch 2:  loss 0.634906 accuracy 0.6162\n",
      "Epoch 203, CIFAR-10 Batch 3:  loss 0.60347 accuracy 0.6194\n",
      "Epoch 203, CIFAR-10 Batch 4:  loss 0.616514 accuracy 0.6208\n",
      "Epoch 203, CIFAR-10 Batch 5:  loss 0.596968 accuracy 0.6206\n",
      "Epoch 204, CIFAR-10 Batch 1:  loss 0.648375 accuracy 0.6178\n",
      "Epoch 204, CIFAR-10 Batch 2:  loss 0.630837 accuracy 0.6182\n",
      "Epoch 204, CIFAR-10 Batch 3:  loss 0.600663 accuracy 0.6202\n",
      "Epoch 204, CIFAR-10 Batch 4:  loss 0.6128 accuracy 0.6208\n",
      "Epoch 204, CIFAR-10 Batch 5:  loss 0.593981 accuracy 0.626\n",
      "Epoch 205, CIFAR-10 Batch 1:  loss 0.648758 accuracy 0.6192\n",
      "Epoch 205, CIFAR-10 Batch 2:  loss 0.627862 accuracy 0.6182\n",
      "Epoch 205, CIFAR-10 Batch 3:  loss 0.596143 accuracy 0.6206\n",
      "Epoch 205, CIFAR-10 Batch 4:  loss 0.61305 accuracy 0.6206\n",
      "Epoch 205, CIFAR-10 Batch 5:  loss 0.590244 accuracy 0.6234\n",
      "Epoch 206, CIFAR-10 Batch 1:  loss 0.642289 accuracy 0.617\n",
      "Epoch 206, CIFAR-10 Batch 2:  loss 0.625772 accuracy 0.618\n",
      "Epoch 206, CIFAR-10 Batch 3:  loss 0.593139 accuracy 0.62\n",
      "Epoch 206, CIFAR-10 Batch 4:  loss 0.609291 accuracy 0.6214\n",
      "Epoch 206, CIFAR-10 Batch 5:  loss 0.588368 accuracy 0.624\n",
      "Epoch 207, CIFAR-10 Batch 1:  loss 0.644755 accuracy 0.6206\n",
      "Epoch 207, CIFAR-10 Batch 2:  loss 0.625301 accuracy 0.617\n",
      "Epoch 207, CIFAR-10 Batch 3:  loss 0.591045 accuracy 0.6216\n",
      "Epoch 207, CIFAR-10 Batch 4:  loss 0.606784 accuracy 0.6194\n",
      "Epoch 207, CIFAR-10 Batch 5:  loss 0.586893 accuracy 0.6236\n",
      "Epoch 208, CIFAR-10 Batch 1:  loss 0.637502 accuracy 0.6176\n",
      "Epoch 208, CIFAR-10 Batch 2:  loss 0.624527 accuracy 0.6174\n",
      "Epoch 208, CIFAR-10 Batch 3:  loss 0.590826 accuracy 0.6182\n",
      "Epoch 208, CIFAR-10 Batch 4:  loss 0.609597 accuracy 0.6186\n",
      "Epoch 208, CIFAR-10 Batch 5:  loss 0.58486 accuracy 0.6226\n",
      "Epoch 209, CIFAR-10 Batch 1:  loss 0.638824 accuracy 0.6198\n",
      "Epoch 209, CIFAR-10 Batch 2:  loss 0.621964 accuracy 0.616\n",
      "Epoch 209, CIFAR-10 Batch 3:  loss 0.592225 accuracy 0.6188\n",
      "Epoch 209, CIFAR-10 Batch 4:  loss 0.603381 accuracy 0.6198\n",
      "Epoch 209, CIFAR-10 Batch 5:  loss 0.58376 accuracy 0.6226\n",
      "Epoch 210, CIFAR-10 Batch 1:  loss 0.637965 accuracy 0.6162\n",
      "Epoch 210, CIFAR-10 Batch 2:  loss 0.618793 accuracy 0.6152\n",
      "Epoch 210, CIFAR-10 Batch 3:  loss 0.590043 accuracy 0.6196\n",
      "Epoch 210, CIFAR-10 Batch 4:  loss 0.601884 accuracy 0.6204\n",
      "Epoch 210, CIFAR-10 Batch 5:  loss 0.584259 accuracy 0.6208\n",
      "Epoch 211, CIFAR-10 Batch 1:  loss 0.638318 accuracy 0.6188\n",
      "Epoch 211, CIFAR-10 Batch 2:  loss 0.619013 accuracy 0.616\n",
      "Epoch 211, CIFAR-10 Batch 3:  loss 0.590165 accuracy 0.6192\n",
      "Epoch 211, CIFAR-10 Batch 4:  loss 0.599752 accuracy 0.6208\n",
      "Epoch 211, CIFAR-10 Batch 5:  loss 0.579806 accuracy 0.622\n",
      "Epoch 212, CIFAR-10 Batch 1:  loss 0.633326 accuracy 0.619\n",
      "Epoch 212, CIFAR-10 Batch 2:  loss 0.616237 accuracy 0.6148\n",
      "Epoch 212, CIFAR-10 Batch 3:  loss 0.58554 accuracy 0.6176\n",
      "Epoch 212, CIFAR-10 Batch 4:  loss 0.600648 accuracy 0.6206\n",
      "Epoch 212, CIFAR-10 Batch 5:  loss 0.57726 accuracy 0.6246\n",
      "Epoch 213, CIFAR-10 Batch 1:  loss 0.631211 accuracy 0.6206\n",
      "Epoch 213, CIFAR-10 Batch 2:  loss 0.615301 accuracy 0.615\n",
      "Epoch 213, CIFAR-10 Batch 3:  loss 0.584207 accuracy 0.6192\n",
      "Epoch 213, CIFAR-10 Batch 4:  loss 0.594225 accuracy 0.6174\n",
      "Epoch 213, CIFAR-10 Batch 5:  loss 0.575144 accuracy 0.625\n",
      "Epoch 214, CIFAR-10 Batch 1:  loss 0.626542 accuracy 0.6196\n",
      "Epoch 214, CIFAR-10 Batch 2:  loss 0.614747 accuracy 0.616\n",
      "Epoch 214, CIFAR-10 Batch 3:  loss 0.583812 accuracy 0.6188\n",
      "Epoch 214, CIFAR-10 Batch 4:  loss 0.594545 accuracy 0.6204\n",
      "Epoch 214, CIFAR-10 Batch 5:  loss 0.576494 accuracy 0.6242\n",
      "Epoch 215, CIFAR-10 Batch 1:  loss 0.625409 accuracy 0.6192\n",
      "Epoch 215, CIFAR-10 Batch 2:  loss 0.61682 accuracy 0.6178\n",
      "Epoch 215, CIFAR-10 Batch 3:  loss 0.588075 accuracy 0.6172\n",
      "Epoch 215, CIFAR-10 Batch 4:  loss 0.5958 accuracy 0.6162\n",
      "Epoch 215, CIFAR-10 Batch 5:  loss 0.572483 accuracy 0.6256\n",
      "Epoch 216, CIFAR-10 Batch 1:  loss 0.624257 accuracy 0.62\n",
      "Epoch 216, CIFAR-10 Batch 2:  loss 0.613951 accuracy 0.6138\n",
      "Epoch 216, CIFAR-10 Batch 3:  loss 0.591326 accuracy 0.6186\n",
      "Epoch 216, CIFAR-10 Batch 4:  loss 0.595226 accuracy 0.6144\n",
      "Epoch 216, CIFAR-10 Batch 5:  loss 0.575247 accuracy 0.6252\n",
      "Epoch 217, CIFAR-10 Batch 1:  loss 0.616507 accuracy 0.6214\n",
      "Epoch 217, CIFAR-10 Batch 2:  loss 0.613913 accuracy 0.6128\n",
      "Epoch 217, CIFAR-10 Batch 3:  loss 0.591623 accuracy 0.6164\n",
      "Epoch 217, CIFAR-10 Batch 4:  loss 0.592297 accuracy 0.612\n",
      "Epoch 217, CIFAR-10 Batch 5:  loss 0.582326 accuracy 0.6226\n",
      "Epoch 218, CIFAR-10 Batch 1:  loss 0.605496 accuracy 0.6236\n",
      "Epoch 218, CIFAR-10 Batch 2:  loss 0.607827 accuracy 0.6158\n",
      "Epoch 218, CIFAR-10 Batch 3:  loss 0.579279 accuracy 0.618\n",
      "Epoch 218, CIFAR-10 Batch 4:  loss 0.587307 accuracy 0.6196\n",
      "Epoch 218, CIFAR-10 Batch 5:  loss 0.587113 accuracy 0.6218\n",
      "Epoch 219, CIFAR-10 Batch 1:  loss 0.607366 accuracy 0.6272\n",
      "Epoch 219, CIFAR-10 Batch 2:  loss 0.602823 accuracy 0.619\n",
      "Epoch 219, CIFAR-10 Batch 3:  loss 0.568668 accuracy 0.6198\n",
      "Epoch 219, CIFAR-10 Batch 4:  loss 0.581258 accuracy 0.62\n",
      "Epoch 219, CIFAR-10 Batch 5:  loss 0.581913 accuracy 0.6226\n",
      "Epoch 220, CIFAR-10 Batch 1:  loss 0.605835 accuracy 0.624\n",
      "Epoch 220, CIFAR-10 Batch 2:  loss 0.597494 accuracy 0.6194\n",
      "Epoch 220, CIFAR-10 Batch 3:  loss 0.55775 accuracy 0.6198\n",
      "Epoch 220, CIFAR-10 Batch 4:  loss 0.577933 accuracy 0.6214\n",
      "Epoch 220, CIFAR-10 Batch 5:  loss 0.579857 accuracy 0.6208\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6217382917037377\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwGzuD6txAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f107144cef0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
