{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:35, 4.83MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 4:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1003, 1: 963, 2: 1041, 3: 976, 4: 1004, 5: 1021, 6: 1004, 7: 981, 8: 1024, 9: 983}\n",
      "First 20 Labels: [0, 6, 0, 2, 7, 2, 1, 2, 4, 1, 5, 6, 6, 3, 1, 3, 5, 5, 8, 1]\n",
      "\n",
      "Example of Image 9999:\n",
      "Image - Min Value: 0 Max Value: 255\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 8 Name: ship\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBD0AFCboAaAwQQ8Ahf0fXl+CSe72ciAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3d8fb5b38>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 4\n",
    "sample_id = 9999\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    a = 0\n",
    "    b = 1\n",
    "    grayscale_min = 0\n",
    "    grayscale_max = 255\n",
    "    return a + ( ( (x - grayscale_min)*(b - a) )/( grayscale_max - grayscale_min ) )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    result = np.zeros((len(x), 10))\n",
    "    result[np.arange(len(x)), x] = 1\n",
    "    return result\n",
    "    \n",
    "\"\"\" \n",
    "This is not as efficient:\n",
    "\n",
    "    y = np.zeros((len(x), 10))\n",
    "    for i in range(len(x)):\n",
    "        y[i,x[i]] = 1\n",
    "    return y\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # tf.placeholder(dtype, shape=None, name=None)\n",
    "    s = [None] + list(image_shape)\n",
    "    return tf.placeholder(tf.float32, shape=s, name='x')\n",
    "\"\"\"\n",
    "Notes:\n",
    "image_shape is a tuple(32,32,3).\n",
    "So currently in the following part,\n",
    "\n",
    "shape = [None, image_shape]\n",
    "you are actually doing this, and this is why you got the error.\n",
    "\n",
    "shape = [None, (32, 32, 3)]\n",
    "Instead, you want to change like following.\n",
    "\n",
    "shape = [None, 32, 32, 3]\n",
    "How can you fix this?\n",
    "\n",
    "Hint: [None] + list(tuple)\n",
    "Here are my suggestions and hints.\n",
    "Please use list concatenation( e.g. [1] + [2, 3, 4] will become [1, 2, 3, 4])\n",
    "Hint 1: image_shape is tuple of (32, 32, 3)\n",
    "Hint 2: Try [None] + list(tuple)\n",
    "\n",
    "I ended up using individual indecies: (None, tuple[0], tuple[1], tuple[2])\n",
    "\n",
    "You can also do it this way :\n",
    "tf.placeholder(tf.float32, shape = [None, *image_shape], name = \"x\")\n",
    "The * operator unpacks the tuple.\n",
    "\"\"\"\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # tf.placeholder(dtype, shape=None, name=None)\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,name=\"keep_prob\") # dropout (keep probability)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)\n",
    "    # TODO: Implement Function\n",
    "    # Create the weight and bias using conv_ksize, conv_num_outputs and the shape of x_tensor.\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0],    # height\n",
    "                                     conv_ksize[1],             # width\n",
    "                                     x_tensor.get_shape().as_list()[-1],   # input_depth\n",
    "                                     conv_num_outputs,], # output_depth\n",
    "                                     mean=0.0,\n",
    "                                     stddev=0.1,\n",
    "                                     dtype=tf.float32, \n",
    "                                     seed=None,\n",
    "                                     name=None))   \n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs)) # use number of outputs for the conv layer\n",
    "    \n",
    "    # conv_strides (batch, height, width, depth)\n",
    "    # set batch = 1 and depth = 1 (first and last positions should be 1s)\n",
    "    \n",
    "    # Apply a convolution to x_tensor using weight and conv_strides.\n",
    "    # We recommend you use same padding, but you're welcome to use any padding.\n",
    "    conv_layer0 = tf.nn.conv2d(x_tensor, \n",
    "                               weight, \n",
    "                               strides=[1, conv_strides[0], conv_strides[1], 1], \n",
    "                               padding='SAME')\n",
    "    \n",
    "    # Add bias\n",
    "    conv_layer1 = tf.nn.bias_add(conv_layer0, bias)\n",
    "    \n",
    "    # Apply activation function\n",
    "    # A non-linear function, the ReLU or rectified linear unit. \n",
    "    # The ReLU function is 0 for negative inputs and x for all inputs x>0. \n",
    "    conv_layer2 = tf.contrib.layers.batch_norm(conv_layer1, center=True, scale=True)\n",
    "    conv_layer2 = tf.nn.relu(conv_layer2)\n",
    "    \n",
    "    # Apply Max Pooling using pool_ksize and pool_strides.\n",
    "    conv_layer3 = tf.nn.max_pool(conv_layer2, \n",
    "                                 ksize=[1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                                 strides=[1,pool_strides[0],pool_strides[1],1], \n",
    "                                 padding='SAME')\n",
    "\n",
    "    return conv_layer3 \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10, 30, 6)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # tf.contrib.layers.flatten(*args, **kwargs)\n",
    "    batch_size = -1\n",
    "    width = x_tensor.get_shape().as_list()[1]\n",
    "    height= x_tensor.get_shape().as_list()[2]\n",
    "    depth = x_tensor.get_shape().as_list()[3]\n",
    "    \n",
    "    print(x_tensor.get_shape()[:])\n",
    "    image_flat_size = width*height*depth\n",
    "    return tf.reshape(x_tensor, [batch_size, image_flat_size])\n",
    "\"\"\"\n",
    "Flatten is an operation as name implies should contain single array as output combining all other dimensions. \n",
    "For example if input is [W,H,D] then it would become [W*H*D]. Since batch size indicate the current samples \n",
    "that are proceed, it should remain intact. I.e. for every sample, there should be feature vector.\n",
    "\n",
    "Now based on above explanation, we know that if input shape is [batch_size, W, H, D] then output should be \n",
    "[batch_size, W*H*D]. In order to achieve this you can use tf.reshape. Here [-1, W*H*D] means \n",
    "[first_dimension, second_dimension]. Having -1 in the expression means that code will automatically \n",
    "calculate the value based on the available dimension. To explain in depth, tf knows that dimension is \n",
    "[batch_size*W*H*D] and in that [W*H*D] is specified, so it calculates the value and kept it where -1 is. \n",
    "Finally [-1, W*H*D] is nothing but [batch_size, W*H*D]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weights = tf.Variable(tf.truncated_normal((x_tensor.get_shape().as_list()[1], num_outputs)))\n",
    "    biases = tf.Variable(tf.zeros(num_outputs))\n",
    "    layer = tf.add(tf.matmul(x_tensor, weights), biases)\n",
    "    layer1 = tf.nn.relu(layer)\n",
    "    \n",
    "    return layer1\n",
    "\n",
    "# alternate:\n",
    "# return = tf.layers.dense(inputs=x_tensor, unit= num_outputs, activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Output Layer - class prediction \n",
    "    weights = tf.Variable(tf.truncated_normal((x_tensor.get_shape().as_list()[1], num_outputs)))\n",
    "    biases = tf.Variable(tf.zeros(num_outputs))\n",
    "    out = tf.add(tf.matmul(x_tensor, weights), biases)\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1, 1, 64)\n",
      "(?, 1, 1, 64)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    \n",
    "    x_tensor = x\n",
    "    conv_num_outputs = 64 # 64\n",
    "    conv_ksize = (5, 5)\n",
    "    conv_strides = (2, 2)\n",
    "    pool_ksize = (4, 4) # (3, 3)\n",
    "    pool_strides = (2, 2)\n",
    "    num_outputs = 10\n",
    "    \n",
    "    \"\"\"\n",
    "    Tensorflow does the following for 'SAME' padding:\n",
    "    out_height = ceil( float(in_height) / float(strides[0]) )\n",
    "    out_width = ceil( float(in_width) / float(strides[1]) )\n",
    "    After the convolution: height & width = 16 = 32 / 2\n",
    "    After the pooling: height & width = 8 = 16 / 2\n",
    "    \n",
    "    It's not that Tensorflow is behaving differently, it's what Tensorflow has chosen to mean by 'SAME padding'.\n",
    "    If the lesson formula is\n",
    "    out = 1 + ( in - conv_ksize + 2P) / stride\n",
    "    then\n",
    "    16 = 1 + ( 32 - 5 + 2P) / 2\n",
    "    and so 2P = 3\n",
    "    i.e. Tensorflow has defined 'SAME padding' as 'make the output size equal to the input size / strides'. Given the output, input and conv_ksize it calculates the padding (P) required to satisfy the definition. Since the total padding needed is an odd number in this case it must have also have a rule on whether to pad more to the right/left, top/bottom\n",
    "    \"\"\"\n",
    "    \n",
    "    # Layer 1\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv2d_layer = conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    # Layer 2\n",
    "    conv2d_layer2 = conv2d_maxpool(conv2d_layer,conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    # Layer 3\n",
    "    conv2d_layer3 = conv2d_maxpool(conv2d_layer2,conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flatten_layer = flatten(conv2d_layer3)\n",
    "    #flat_layer_kp = tf.nn.dropout(flatten_layer, keep_prob)  # don't include this between layers as accuracy stays low.\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fully_connected1 = fully_conn(flatten_layer, 64)\n",
    "    fully_connected_kp = tf.nn.dropout(fully_connected1, tf.to_float(keep_prob))\n",
    "    fully_connected2 = fully_conn(fully_connected_kp, 32)\n",
    "    fully_connected3 = fully_conn(fully_connected2, 16)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    # TODO: return output\n",
    "    num_outputs = 10\n",
    "    return output(fully_connected_kp, num_outputs)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Run optimization \n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = session.run(cost, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})\n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.})\n",
    "    \n",
    "    print('loss', loss, 'accuracy', valid_acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBatch size is not about memory alone. You would have seen that in most the problem until now batch size is \\nconsidered as one| of the hyper parameters in tuning the network. So, that gives an inituation that changing \\nbatch size does affect the performance.\\n\\nGradient is updated after processing inputs of size batch. If you recollect, gradient will help you in finding \\nthe global minima. If the batch size is too large or it it occupies the complete training data, there is a \\nhigh probablity that you will end in the local minima. If you consider a batch size of one, then it is so noisely. \\nBut if you consider, batch size of 128 or 256, it is better than one but still noiser, when compared to complete \\ntraining data. But this noisiness help to get out local minima. So, pratically batch size are never taken as complete \\ntraining data both for memory and performance reason.\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 150\n",
    "batch_size = 256\n",
    "keep_probability = 0.5 \n",
    "\n",
    "\"\"\"\n",
    "Batch size is not about memory alone. You would have seen that in most the problem until now batch size is \n",
    "considered as one| of the hyper parameters in tuning the network. So, that gives an inituation that changing \n",
    "batch size does affect the performance.\n",
    "\n",
    "Gradient is updated after processing inputs of size batch. If you recollect, gradient will help you in finding \n",
    "the global minima. If the batch size is too large or it it occupies the complete training data, there is a \n",
    "high probablity that you will end in the local minima. If you consider a batch size of one, then it is so noisely. \n",
    "But if you consider, batch size of 128 or 256, it is better than one but still noiser, when compared to complete \n",
    "training data. But this noisiness help to get out local minima. So, pratically batch size are never taken as complete \n",
    "training data both for memory and performance reason.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss 11.8241 accuracy 0.2994\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss 5.34019 accuracy 0.3592\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss 3.27919 accuracy 0.3724\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss 2.46991 accuracy 0.3896\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss 1.62194 accuracy 0.3922\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss 1.37016 accuracy 0.39\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss 1.15366 accuracy 0.391\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss 1.0863 accuracy 0.3734\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss 1.14042 accuracy 0.3712\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss 1.02171 accuracy 0.3736\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss 1.0348 accuracy 0.3728\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss 0.938607 accuracy 0.3854\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss 0.921655 accuracy 0.3886\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss 0.920071 accuracy 0.3974\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss 0.877268 accuracy 0.4034\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss 0.800805 accuracy 0.4044\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss 0.799054 accuracy 0.4124\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss 0.743624 accuracy 0.4152\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss 0.738689 accuracy 0.4196\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss 0.67032 accuracy 0.4228\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss 0.673221 accuracy 0.4244\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss 0.677616 accuracy 0.4326\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss 0.66648 accuracy 0.4374\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss 0.638271 accuracy 0.4422\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss 0.603397 accuracy 0.4392\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss 0.599283 accuracy 0.448\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss 0.559148 accuracy 0.4502\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss 0.566517 accuracy 0.456\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss 0.541956 accuracy 0.454\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss 0.549831 accuracy 0.4532\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss 0.525429 accuracy 0.4602\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss 0.495335 accuracy 0.4594\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss 0.494272 accuracy 0.4624\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss 0.473912 accuracy 0.4608\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss 0.474709 accuracy 0.4688\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss 0.449512 accuracy 0.4688\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss 0.442353 accuracy 0.4682\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss 0.425982 accuracy 0.4706\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss 0.411817 accuracy 0.4758\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss 0.395497 accuracy 0.4782\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss 0.381748 accuracy 0.4746\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss 0.361897 accuracy 0.4792\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss 0.359907 accuracy 0.478\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss 0.33785 accuracy 0.4846\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss 0.361243 accuracy 0.4778\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss 0.362916 accuracy 0.4902\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss 0.382785 accuracy 0.4874\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss 0.354585 accuracy 0.4842\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss 0.302452 accuracy 0.4924\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss 0.323297 accuracy 0.4898\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss 0.312307 accuracy 0.4894\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss 0.298606 accuracy 0.4976\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss 0.29297 accuracy 0.4938\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss 0.292951 accuracy 0.5016\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss 0.285622 accuracy 0.4968\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss 0.244282 accuracy 0.4968\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss 0.24621 accuracy 0.502\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss 0.225091 accuracy 0.5076\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss 0.251988 accuracy 0.5016\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss 0.215536 accuracy 0.5052\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss 0.220362 accuracy 0.5094\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss 0.206316 accuracy 0.5082\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss 0.201545 accuracy 0.5174\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss 0.189344 accuracy 0.5142\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss 0.184957 accuracy 0.508\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss 0.160253 accuracy 0.5138\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss 0.171272 accuracy 0.5072\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss 0.149168 accuracy 0.5174\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss 0.163893 accuracy 0.5132\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss 0.147593 accuracy 0.5146\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss 0.143439 accuracy 0.5238\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss 0.142997 accuracy 0.52\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss 0.122777 accuracy 0.5186\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss 0.105423 accuracy 0.5224\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss 0.108874 accuracy 0.5172\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss 0.107582 accuracy 0.5216\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss 0.119292 accuracy 0.5166\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss 0.102517 accuracy 0.5258\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss 0.107481 accuracy 0.5266\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss 0.103747 accuracy 0.5214\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss 0.0892941 accuracy 0.5324\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss 0.0786625 accuracy 0.5296\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss 0.0883794 accuracy 0.5264\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss 0.070678 accuracy 0.534\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss 0.102795 accuracy 0.5306\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss 0.0938682 accuracy 0.522\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss 0.0697065 accuracy 0.5328\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss 0.0768153 accuracy 0.5376\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss 0.0735937 accuracy 0.5334\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss 0.0521742 accuracy 0.5374\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss 0.0781337 accuracy 0.5312\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss 0.0555141 accuracy 0.531\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss 0.0521917 accuracy 0.5378\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss 0.0440415 accuracy 0.5364\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss 0.0345185 accuracy 0.527\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss 0.0637292 accuracy 0.5344\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss 0.072082 accuracy 0.5408\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss 0.0647316 accuracy 0.535\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss 0.0511835 accuracy 0.536\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss 0.0486611 accuracy 0.5422\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss 0.048096 accuracy 0.5298\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss 0.0492892 accuracy 0.5364\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss 0.0487191 accuracy 0.5374\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss 0.0374915 accuracy 0.5472\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss 0.035245 accuracy 0.545\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss 0.0576595 accuracy 0.54\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss 0.0354245 accuracy 0.5452\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss 0.0314835 accuracy 0.545\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss 0.0286219 accuracy 0.5478\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss 0.0484702 accuracy 0.5392\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss 0.0319307 accuracy 0.5518\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss 0.0401118 accuracy 0.55\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss 0.0245836 accuracy 0.5492\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss 0.0352414 accuracy 0.5456\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss 0.0295023 accuracy 0.5476\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss 0.030215 accuracy 0.5496\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss 0.0175598 accuracy 0.549\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss 0.0258817 accuracy 0.551\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss 0.0255744 accuracy 0.5482\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss 0.0242665 accuracy 0.5484\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss 0.0278472 accuracy 0.5472\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss 0.027909 accuracy 0.5528\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss 0.0288986 accuracy 0.554\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss 0.05291 accuracy 0.5522\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss 0.0671734 accuracy 0.556\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss 0.0237034 accuracy 0.5566\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss 0.0240431 accuracy 0.5556\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss 0.0329702 accuracy 0.5536\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss 0.0407034 accuracy 0.5516\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss 0.0246627 accuracy 0.557\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss 0.0224615 accuracy 0.5522\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss 0.014599 accuracy 0.5462\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss 0.0220722 accuracy 0.557\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss 0.0237133 accuracy 0.5506\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss 0.0129312 accuracy 0.5472\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss 0.0257055 accuracy 0.5518\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss 0.0146186 accuracy 0.551\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss 0.0172861 accuracy 0.5532\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss 0.0135374 accuracy 0.5528\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss 0.0157448 accuracy 0.5594\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss 0.0111543 accuracy 0.556\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss 0.0132921 accuracy 0.5514\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss 0.01486 accuracy 0.557\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss 0.020862 accuracy 0.5628\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss 0.0182417 accuracy 0.5578\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss 0.0196016 accuracy 0.5554\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss 0.00658464 accuracy 0.5586\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss 0.0105473 accuracy 0.5562\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss 0.0171348 accuracy 0.5602\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss 0.0106529 accuracy 0.5554\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "        # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss 7.0346 accuracy 0.2788\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss 4.98612 accuracy 0.3032\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss 3.20511 accuracy 0.3186\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss 2.03871 accuracy 0.3002\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss 2.04562 accuracy 0.2892\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss 2.03321 accuracy 0.2824\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss 1.95424 accuracy 0.2768\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss 1.90549 accuracy 0.2818\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss 1.76052 accuracy 0.2876\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss 1.80332 accuracy 0.3118\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss 1.90665 accuracy 0.3128\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss 1.8006 accuracy 0.3222\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss 1.73746 accuracy 0.3184\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss 1.69628 accuracy 0.3304\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss 1.69654 accuracy 0.3464\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss 1.77251 accuracy 0.3434\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss 1.73772 accuracy 0.3526\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss 1.57355 accuracy 0.3542\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss 1.61726 accuracy 0.3562\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss 1.58532 accuracy 0.3656\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss 1.70315 accuracy 0.373\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss 1.65783 accuracy 0.3788\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss 1.41863 accuracy 0.3792\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss 1.5512 accuracy 0.3724\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss 1.45809 accuracy 0.3924\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss 1.53922 accuracy 0.3972\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss 1.58004 accuracy 0.4002\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss 1.24404 accuracy 0.4062\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss 1.47997 accuracy 0.4032\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss 1.41931 accuracy 0.4138\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss 1.46153 accuracy 0.4072\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss 1.44693 accuracy 0.4266\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss 1.23242 accuracy 0.4264\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss 1.40159 accuracy 0.4188\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss 1.35053 accuracy 0.4376\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss 1.39203 accuracy 0.4462\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss 1.31824 accuracy 0.451\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss 1.1417 accuracy 0.451\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss 1.28118 accuracy 0.4462\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss 1.33402 accuracy 0.4472\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss 1.28286 accuracy 0.4564\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss 1.28168 accuracy 0.4636\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss 1.05976 accuracy 0.473\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss 1.31523 accuracy 0.4632\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss 1.17377 accuracy 0.4764\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss 1.23459 accuracy 0.4756\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss 1.19651 accuracy 0.4862\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss 1.00959 accuracy 0.4848\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss 1.21023 accuracy 0.4802\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss 1.14308 accuracy 0.49\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss 1.21226 accuracy 0.4936\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss 1.13477 accuracy 0.502\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss 0.911376 accuracy 0.4944\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss 1.19861 accuracy 0.4996\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss 1.07052 accuracy 0.5036\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss 1.19419 accuracy 0.5072\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss 1.10577 accuracy 0.5028\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss 0.833387 accuracy 0.5176\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss 1.14531 accuracy 0.5138\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss 0.943844 accuracy 0.523\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss 1.06735 accuracy 0.5178\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss 1.0594 accuracy 0.5248\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss 0.79185 accuracy 0.521\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss 1.13314 accuracy 0.5256\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss 0.880316 accuracy 0.5324\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss 1.05129 accuracy 0.529\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss 1.03444 accuracy 0.5302\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss 0.760726 accuracy 0.5268\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss 1.0253 accuracy 0.5386\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss 0.878209 accuracy 0.5386\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss 0.975964 accuracy 0.5432\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss 1.00406 accuracy 0.539\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss 0.672371 accuracy 0.5382\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss 0.95683 accuracy 0.5324\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss 0.763862 accuracy 0.5484\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss 1.005 accuracy 0.5448\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss 0.888126 accuracy 0.5496\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss 0.672207 accuracy 0.556\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss 0.95308 accuracy 0.5472\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss 0.713226 accuracy 0.5562\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss 0.942331 accuracy 0.5572\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss 0.923775 accuracy 0.5506\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss 0.665041 accuracy 0.5572\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss 0.896578 accuracy 0.5544\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss 0.700488 accuracy 0.5588\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss 0.89975 accuracy 0.5604\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss 0.826456 accuracy 0.5544\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss 0.572414 accuracy 0.5642\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss 0.909896 accuracy 0.5596\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss 0.685422 accuracy 0.5648\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss 0.828941 accuracy 0.5744\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss 0.797674 accuracy 0.571\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss 0.58564 accuracy 0.5706\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss 0.857065 accuracy 0.5668\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss 0.696203 accuracy 0.565\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss 0.787617 accuracy 0.5728\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss 0.815578 accuracy 0.5726\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss 0.58193 accuracy 0.5824\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss 0.845634 accuracy 0.5858\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss 0.631087 accuracy 0.5808\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss 0.821285 accuracy 0.5888\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss 0.810168 accuracy 0.5874\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss 0.50996 accuracy 0.5878\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss 0.808644 accuracy 0.5818\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss 0.571291 accuracy 0.5882\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss 0.744258 accuracy 0.5998\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss 0.648451 accuracy 0.594\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss 0.481882 accuracy 0.5878\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss 0.779176 accuracy 0.5852\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss 0.56717 accuracy 0.5902\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss 0.761153 accuracy 0.606\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss 0.593274 accuracy 0.6002\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss 0.45034 accuracy 0.5972\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss 0.74415 accuracy 0.5966\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss 0.51739 accuracy 0.5956\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss 0.706751 accuracy 0.608\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss 0.595166 accuracy 0.6016\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss 0.483409 accuracy 0.6006\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss 0.748028 accuracy 0.5972\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss 0.498249 accuracy 0.597\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss 0.70944 accuracy 0.6106\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss 0.530245 accuracy 0.6038\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss 0.411594 accuracy 0.6034\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss 0.724305 accuracy 0.608\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss 0.485759 accuracy 0.6106\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss 0.675234 accuracy 0.6134\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss 0.556935 accuracy 0.614\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss 0.42397 accuracy 0.613\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss 0.666852 accuracy 0.5992\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss 0.464003 accuracy 0.6166\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss 0.632272 accuracy 0.6194\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss 0.521963 accuracy 0.612\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss 0.364737 accuracy 0.6184\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss 0.669902 accuracy 0.6094\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss 0.465305 accuracy 0.6164\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss 0.637276 accuracy 0.6208\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss 0.494878 accuracy 0.6174\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss 0.349322 accuracy 0.6198\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss 0.675215 accuracy 0.6168\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss 0.456152 accuracy 0.6202\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss 0.656141 accuracy 0.629\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss 0.462084 accuracy 0.6284\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss 0.306334 accuracy 0.6288\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss 0.592391 accuracy 0.6206\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss 0.393945 accuracy 0.6248\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss 0.607347 accuracy 0.6354\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss 0.45535 accuracy 0.6222\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss 0.296759 accuracy 0.6352\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss 0.597006 accuracy 0.631\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss 0.377281 accuracy 0.6336\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss 0.579639 accuracy 0.632\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss 0.437348 accuracy 0.6432\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss 0.275561 accuracy 0.6366\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss 0.610543 accuracy 0.6276\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss 0.362567 accuracy 0.6326\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss 0.578543 accuracy 0.636\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss 0.402332 accuracy 0.6444\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss 0.272588 accuracy 0.639\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss 0.538637 accuracy 0.6352\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss 0.343767 accuracy 0.6326\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss 0.541707 accuracy 0.6462\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss 0.385928 accuracy 0.6286\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss 0.251349 accuracy 0.643\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss 0.536824 accuracy 0.639\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss 0.333575 accuracy 0.642\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss 0.498725 accuracy 0.64\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss 0.34255 accuracy 0.6382\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss 0.257643 accuracy 0.6388\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss 0.453885 accuracy 0.6446\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss 0.261307 accuracy 0.6494\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss 0.464338 accuracy 0.6496\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss 0.33139 accuracy 0.6434\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss 0.244397 accuracy 0.6612\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss 0.469768 accuracy 0.6348\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss 0.223399 accuracy 0.644\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss 0.502869 accuracy 0.6524\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss 0.313771 accuracy 0.6414\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss 0.235919 accuracy 0.6538\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss 0.461032 accuracy 0.643\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss 0.231413 accuracy 0.6478\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss 0.516404 accuracy 0.6416\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss 0.295947 accuracy 0.6394\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss 0.201886 accuracy 0.6446\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss 0.408302 accuracy 0.6492\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss 0.245959 accuracy 0.6476\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss 0.482949 accuracy 0.6566\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss 0.292147 accuracy 0.6502\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss 0.218485 accuracy 0.6524\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss 0.435618 accuracy 0.6432\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss 0.21751 accuracy 0.6452\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss 0.417506 accuracy 0.6564\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss 0.253795 accuracy 0.6498\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss 0.197927 accuracy 0.6536\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss 0.370148 accuracy 0.6516\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss 0.170569 accuracy 0.6456\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss 0.423524 accuracy 0.654\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss 0.237892 accuracy 0.6538\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss 0.180317 accuracy 0.6532\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss 0.389768 accuracy 0.6544\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss 0.173701 accuracy 0.6562\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss 0.412044 accuracy 0.6574\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss 0.224045 accuracy 0.645\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss 0.18586 accuracy 0.6514\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss 0.338771 accuracy 0.654\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss 0.137899 accuracy 0.655\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss 0.430418 accuracy 0.6662\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss 0.208377 accuracy 0.6516\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss 0.192366 accuracy 0.6572\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss 0.319417 accuracy 0.6542\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss 0.150859 accuracy 0.6574\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss 0.405672 accuracy 0.6642\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss 0.198425 accuracy 0.6438\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss 0.170884 accuracy 0.6568\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss 0.3067 accuracy 0.6548\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss 0.127766 accuracy 0.6616\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss 0.376412 accuracy 0.656\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss 0.195908 accuracy 0.6556\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss 0.162122 accuracy 0.6622\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss 0.280011 accuracy 0.6668\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss 0.134801 accuracy 0.6638\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss 0.337289 accuracy 0.6678\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss 0.183046 accuracy 0.6566\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss 0.158844 accuracy 0.655\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss 0.248472 accuracy 0.6634\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss 0.165778 accuracy 0.6646\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss 0.281741 accuracy 0.6612\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss 0.190178 accuracy 0.655\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss 0.138769 accuracy 0.6592\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss 0.237751 accuracy 0.664\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss 0.141589 accuracy 0.6652\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss 0.330271 accuracy 0.667\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss 0.199801 accuracy 0.6638\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss 0.174961 accuracy 0.6664\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss 0.215253 accuracy 0.6632\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss 0.13341 accuracy 0.6624\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss 0.310766 accuracy 0.6662\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss 0.173483 accuracy 0.666\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss 0.140981 accuracy 0.6626\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss 0.228304 accuracy 0.6654\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss 0.126887 accuracy 0.665\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss 0.268569 accuracy 0.6732\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss 0.170308 accuracy 0.6614\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss 0.125035 accuracy 0.663\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss 0.225401 accuracy 0.6698\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss 0.142763 accuracy 0.6636\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss 0.234622 accuracy 0.6706\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss 0.163493 accuracy 0.6588\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss 0.133095 accuracy 0.6608\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss 0.209958 accuracy 0.6672\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss 0.102505 accuracy 0.6682\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss 0.275989 accuracy 0.6662\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss 0.140957 accuracy 0.6684\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss 0.121695 accuracy 0.668\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss 0.189653 accuracy 0.6618\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss 0.113536 accuracy 0.6694\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss 0.209885 accuracy 0.6708\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss 0.138802 accuracy 0.6582\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss 0.13987 accuracy 0.6596\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss 0.175341 accuracy 0.6658\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss 0.10016 accuracy 0.6624\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss 0.206702 accuracy 0.6742\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss 0.121565 accuracy 0.6558\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss 0.120425 accuracy 0.6608\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss 0.175653 accuracy 0.6632\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss 0.108719 accuracy 0.66\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss 0.217381 accuracy 0.6658\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss 0.132566 accuracy 0.6656\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss 0.12733 accuracy 0.6686\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss 0.168523 accuracy 0.6656\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss 0.112028 accuracy 0.6678\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss 0.212648 accuracy 0.6648\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss 0.13087 accuracy 0.6642\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss 0.114683 accuracy 0.6662\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss 0.158578 accuracy 0.661\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss 0.0814443 accuracy 0.6736\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss 0.213779 accuracy 0.668\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss 0.105381 accuracy 0.6678\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss 0.117839 accuracy 0.6646\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss 0.16384 accuracy 0.669\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss 0.0814235 accuracy 0.6684\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss 0.180651 accuracy 0.6654\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss 0.111787 accuracy 0.6648\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss 0.0794956 accuracy 0.6644\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss 0.160486 accuracy 0.6622\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss 0.0844085 accuracy 0.6706\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss 0.169858 accuracy 0.6724\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss 0.139982 accuracy 0.6682\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss 0.112509 accuracy 0.6672\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss 0.145435 accuracy 0.6674\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss 0.0943363 accuracy 0.6632\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss 0.180919 accuracy 0.6716\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss 0.0956437 accuracy 0.6642\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss 0.0900884 accuracy 0.661\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss 0.139539 accuracy 0.6708\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss 0.0916281 accuracy 0.6716\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss 0.166852 accuracy 0.669\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss 0.109907 accuracy 0.6676\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss 0.0792699 accuracy 0.6684\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss 0.138146 accuracy 0.6686\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss 0.0663094 accuracy 0.671\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss 0.145291 accuracy 0.6714\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss 0.110623 accuracy 0.6664\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss 0.0770074 accuracy 0.6672\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss 0.135245 accuracy 0.6722\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss 0.0782356 accuracy 0.6682\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss 0.140528 accuracy 0.6714\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss 0.100631 accuracy 0.6694\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss 0.0777747 accuracy 0.6672\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss 0.143815 accuracy 0.667\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss 0.0582269 accuracy 0.6674\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss 0.151293 accuracy 0.6696\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss 0.0915356 accuracy 0.6736\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss 0.0676341 accuracy 0.6684\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss 0.144752 accuracy 0.6556\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss 0.0725968 accuracy 0.6684\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss 0.148207 accuracy 0.668\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss 0.102276 accuracy 0.6648\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss 0.0649596 accuracy 0.6636\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss 0.134307 accuracy 0.6614\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss 0.0568585 accuracy 0.6732\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss 0.137922 accuracy 0.6666\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss 0.0796705 accuracy 0.6622\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss 0.0691639 accuracy 0.6596\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss 0.136645 accuracy 0.6696\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss 0.0517353 accuracy 0.6742\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss 0.14157 accuracy 0.674\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss 0.0970788 accuracy 0.674\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss 0.0661421 accuracy 0.666\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss 0.12118 accuracy 0.663\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss 0.0524195 accuracy 0.6676\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss 0.125463 accuracy 0.6716\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss 0.0903687 accuracy 0.6722\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss 0.0602445 accuracy 0.6642\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss 0.124041 accuracy 0.6608\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss 0.058353 accuracy 0.6682\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss 0.112137 accuracy 0.6756\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss 0.0881424 accuracy 0.6686\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss 0.0682666 accuracy 0.6622\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss 0.122615 accuracy 0.6664\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss 0.0550719 accuracy 0.6632\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss 0.136308 accuracy 0.6698\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss 0.0925475 accuracy 0.6588\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss 0.0650621 accuracy 0.6684\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss 0.119159 accuracy 0.664\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss 0.047988 accuracy 0.6668\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss 0.113435 accuracy 0.6684\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss 0.079978 accuracy 0.6704\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss 0.0644093 accuracy 0.6668\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss 0.105618 accuracy 0.6668\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss 0.0541884 accuracy 0.6652\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss 0.100943 accuracy 0.6654\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss 0.0613282 accuracy 0.6664\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss 0.0508862 accuracy 0.666\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss 0.108233 accuracy 0.6718\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss 0.0478347 accuracy 0.6684\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss 0.0848509 accuracy 0.6728\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss 0.0818535 accuracy 0.6674\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss 0.054489 accuracy 0.669\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss 0.105441 accuracy 0.6656\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss 0.0522902 accuracy 0.6692\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss 0.114383 accuracy 0.6676\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss 0.0684198 accuracy 0.666\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss 0.0516804 accuracy 0.6666\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss 0.100778 accuracy 0.6668\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss 0.0508532 accuracy 0.669\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss 0.121331 accuracy 0.665\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss 0.0660468 accuracy 0.6576\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss 0.0426341 accuracy 0.664\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss 0.103324 accuracy 0.6622\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss 0.0597949 accuracy 0.6688\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss 0.081785 accuracy 0.6702\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss 0.0850621 accuracy 0.669\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss 0.0419111 accuracy 0.6636\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss 0.103766 accuracy 0.6628\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss 0.0560132 accuracy 0.6668\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss 0.0902505 accuracy 0.6716\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss 0.0512679 accuracy 0.6666\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss 0.0449673 accuracy 0.6564\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss 0.102922 accuracy 0.6646\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss 0.0466593 accuracy 0.6704\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss 0.0743483 accuracy 0.6708\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss 0.0527492 accuracy 0.6594\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss 0.043025 accuracy 0.66\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss 0.103403 accuracy 0.6692\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss 0.0524151 accuracy 0.6686\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss 0.0854184 accuracy 0.6694\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss 0.0538885 accuracy 0.666\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss 0.0394143 accuracy 0.6718\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss 0.0948807 accuracy 0.6694\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss 0.0597212 accuracy 0.6712\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss 0.0831316 accuracy 0.6722\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss 0.0374078 accuracy 0.6684\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss 0.0424286 accuracy 0.6636\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss 0.0869422 accuracy 0.6654\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss 0.0291138 accuracy 0.6724\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss 0.081067 accuracy 0.668\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss 0.0391702 accuracy 0.6648\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss 0.0428127 accuracy 0.6586\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss 0.105913 accuracy 0.6678\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss 0.0414713 accuracy 0.6692\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss 0.0664951 accuracy 0.6654\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss 0.0442738 accuracy 0.668\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss 0.0348473 accuracy 0.6596\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss 0.106485 accuracy 0.6656\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss 0.0348635 accuracy 0.6674\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss 0.0740213 accuracy 0.6732\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss 0.0461064 accuracy 0.66\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss 0.0277833 accuracy 0.6604\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss 0.0936561 accuracy 0.6654\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss 0.0423888 accuracy 0.6666\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss 0.0727889 accuracy 0.667\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss 0.0452389 accuracy 0.662\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss 0.0233745 accuracy 0.6592\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss 0.0897706 accuracy 0.6632\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss 0.0326425 accuracy 0.6702\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss 0.0584722 accuracy 0.6756\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss 0.0383972 accuracy 0.661\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss 0.0318127 accuracy 0.6646\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss 0.0973087 accuracy 0.6652\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss 0.033404 accuracy 0.6656\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss 0.0617732 accuracy 0.6642\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss 0.0292156 accuracy 0.6552\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss 0.0297632 accuracy 0.6676\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss 0.103443 accuracy 0.6544\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss 0.0399609 accuracy 0.6692\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss 0.0772851 accuracy 0.6648\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss 0.0450198 accuracy 0.6588\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss 0.0216863 accuracy 0.664\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss 0.0823269 accuracy 0.6592\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss 0.0227734 accuracy 0.666\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss 0.0611543 accuracy 0.667\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss 0.0254225 accuracy 0.6596\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss 0.0202415 accuracy 0.6628\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss 0.0922304 accuracy 0.666\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss 0.0201349 accuracy 0.6642\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss 0.0486279 accuracy 0.6646\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss 0.0262948 accuracy 0.6588\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss 0.026847 accuracy 0.6592\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss 0.0827263 accuracy 0.6704\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss 0.0298307 accuracy 0.673\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss 0.0582866 accuracy 0.6732\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss 0.0197983 accuracy 0.6562\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss 0.0232531 accuracy 0.657\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss 0.0785069 accuracy 0.6596\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss 0.0356774 accuracy 0.6668\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss 0.0609776 accuracy 0.6652\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss 0.0274646 accuracy 0.6658\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss 0.017121 accuracy 0.6558\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss 0.0813714 accuracy 0.6614\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss 0.0442374 accuracy 0.662\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss 0.0514061 accuracy 0.6614\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss 0.0267459 accuracy 0.666\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss 0.0218468 accuracy 0.6644\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss 0.0841427 accuracy 0.6614\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss 0.0324265 accuracy 0.6608\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss 0.0454786 accuracy 0.6668\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss 0.023918 accuracy 0.6634\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss 0.0261616 accuracy 0.6684\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss 0.113058 accuracy 0.6566\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss 0.0232693 accuracy 0.6612\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss 0.0481545 accuracy 0.6642\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss 0.0270605 accuracy 0.6634\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss 0.0187879 accuracy 0.6652\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss 0.103642 accuracy 0.6654\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss 0.0295781 accuracy 0.6708\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss 0.0605682 accuracy 0.673\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss 0.0436273 accuracy 0.662\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss 0.016225 accuracy 0.661\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss 0.0738406 accuracy 0.6592\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss 0.0232236 accuracy 0.657\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss 0.0610282 accuracy 0.6608\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss 0.0268025 accuracy 0.6604\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss 0.0123635 accuracy 0.6736\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss 0.0735889 accuracy 0.6628\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss 0.0278078 accuracy 0.6586\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss 0.0453996 accuracy 0.6682\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss 0.0745289 accuracy 0.657\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss 0.0194522 accuracy 0.6652\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss 0.0709634 accuracy 0.6586\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss 0.0107778 accuracy 0.6632\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss 0.0559678 accuracy 0.6596\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss 0.0293948 accuracy 0.6632\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss 0.0122651 accuracy 0.668\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss 0.0705482 accuracy 0.6608\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss 0.027773 accuracy 0.6564\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss 0.0479578 accuracy 0.662\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss 0.0366322 accuracy 0.662\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss 0.0146319 accuracy 0.6626\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss 0.0911689 accuracy 0.6622\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss 0.0213816 accuracy 0.6614\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss 0.0467376 accuracy 0.6698\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss 0.0306879 accuracy 0.6582\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss 0.0241602 accuracy 0.6634\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss 0.0793848 accuracy 0.6592\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss 0.0167963 accuracy 0.6644\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss 0.0397385 accuracy 0.6664\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss 0.0368919 accuracy 0.6672\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss 0.0170836 accuracy 0.6698\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss 0.0668771 accuracy 0.6584\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss 0.0201881 accuracy 0.6566\n",
      "Epoch 101, CIFAR-10 Batch 1:  loss 0.0324093 accuracy 0.6658\n",
      "Epoch 101, CIFAR-10 Batch 2:  loss 0.0233492 accuracy 0.6664\n",
      "Epoch 101, CIFAR-10 Batch 3:  loss 0.0118528 accuracy 0.6626\n",
      "Epoch 101, CIFAR-10 Batch 4:  loss 0.0623814 accuracy 0.6594\n",
      "Epoch 101, CIFAR-10 Batch 5:  loss 0.0142019 accuracy 0.6526\n",
      "Epoch 102, CIFAR-10 Batch 1:  loss 0.0317135 accuracy 0.6654\n",
      "Epoch 102, CIFAR-10 Batch 2:  loss 0.0270911 accuracy 0.6632\n",
      "Epoch 102, CIFAR-10 Batch 3:  loss 0.0212683 accuracy 0.6636\n",
      "Epoch 102, CIFAR-10 Batch 4:  loss 0.0661785 accuracy 0.6592\n",
      "Epoch 102, CIFAR-10 Batch 5:  loss 0.00612477 accuracy 0.663\n",
      "Epoch 103, CIFAR-10 Batch 1:  loss 0.0539875 accuracy 0.6622\n",
      "Epoch 103, CIFAR-10 Batch 2:  loss 0.0365073 accuracy 0.6648\n",
      "Epoch 103, CIFAR-10 Batch 3:  loss 0.0132806 accuracy 0.6638\n",
      "Epoch 103, CIFAR-10 Batch 4:  loss 0.0604613 accuracy 0.6616\n",
      "Epoch 103, CIFAR-10 Batch 5:  loss 0.00946678 accuracy 0.6674\n",
      "Epoch 104, CIFAR-10 Batch 1:  loss 0.0406564 accuracy 0.665\n",
      "Epoch 104, CIFAR-10 Batch 2:  loss 0.0206892 accuracy 0.663\n",
      "Epoch 104, CIFAR-10 Batch 3:  loss 0.0136828 accuracy 0.6636\n",
      "Epoch 104, CIFAR-10 Batch 4:  loss 0.0579421 accuracy 0.6588\n",
      "Epoch 104, CIFAR-10 Batch 5:  loss 0.0161266 accuracy 0.6652\n",
      "Epoch 105, CIFAR-10 Batch 1:  loss 0.0318279 accuracy 0.6626\n",
      "Epoch 105, CIFAR-10 Batch 2:  loss 0.0176187 accuracy 0.6638\n",
      "Epoch 105, CIFAR-10 Batch 3:  loss 0.0115131 accuracy 0.6612\n",
      "Epoch 105, CIFAR-10 Batch 4:  loss 0.0663886 accuracy 0.6624\n",
      "Epoch 105, CIFAR-10 Batch 5:  loss 0.0123443 accuracy 0.6592\n",
      "Epoch 106, CIFAR-10 Batch 1:  loss 0.0400141 accuracy 0.6626\n",
      "Epoch 106, CIFAR-10 Batch 2:  loss 0.0165363 accuracy 0.6618\n",
      "Epoch 106, CIFAR-10 Batch 3:  loss 0.0112015 accuracy 0.6622\n",
      "Epoch 106, CIFAR-10 Batch 4:  loss 0.0691422 accuracy 0.657\n",
      "Epoch 106, CIFAR-10 Batch 5:  loss 0.0116526 accuracy 0.6656\n",
      "Epoch 107, CIFAR-10 Batch 1:  loss 0.0339296 accuracy 0.6646\n",
      "Epoch 107, CIFAR-10 Batch 2:  loss 0.0197483 accuracy 0.6632\n",
      "Epoch 107, CIFAR-10 Batch 3:  loss 0.0204521 accuracy 0.6624\n",
      "Epoch 107, CIFAR-10 Batch 4:  loss 0.0685764 accuracy 0.661\n",
      "Epoch 107, CIFAR-10 Batch 5:  loss 0.0173897 accuracy 0.6658\n",
      "Epoch 108, CIFAR-10 Batch 1:  loss 0.0274412 accuracy 0.6558\n",
      "Epoch 108, CIFAR-10 Batch 2:  loss 0.0173546 accuracy 0.6624\n",
      "Epoch 108, CIFAR-10 Batch 3:  loss 0.0124722 accuracy 0.6606\n",
      "Epoch 108, CIFAR-10 Batch 4:  loss 0.057683 accuracy 0.6602\n",
      "Epoch 108, CIFAR-10 Batch 5:  loss 0.0124299 accuracy 0.6664\n",
      "Epoch 109, CIFAR-10 Batch 1:  loss 0.0427899 accuracy 0.6594\n",
      "Epoch 109, CIFAR-10 Batch 2:  loss 0.0172624 accuracy 0.6664\n",
      "Epoch 109, CIFAR-10 Batch 3:  loss 0.0130779 accuracy 0.6668\n",
      "Epoch 109, CIFAR-10 Batch 4:  loss 0.0637253 accuracy 0.6604\n",
      "Epoch 109, CIFAR-10 Batch 5:  loss 0.00862016 accuracy 0.6576\n",
      "Epoch 110, CIFAR-10 Batch 1:  loss 0.0417373 accuracy 0.6582\n",
      "Epoch 110, CIFAR-10 Batch 2:  loss 0.0149651 accuracy 0.6596\n",
      "Epoch 110, CIFAR-10 Batch 3:  loss 0.0135529 accuracy 0.6596\n",
      "Epoch 110, CIFAR-10 Batch 4:  loss 0.0699196 accuracy 0.6584\n",
      "Epoch 110, CIFAR-10 Batch 5:  loss 0.0107549 accuracy 0.663\n",
      "Epoch 111, CIFAR-10 Batch 1:  loss 0.0351689 accuracy 0.6626\n",
      "Epoch 111, CIFAR-10 Batch 2:  loss 0.0175563 accuracy 0.6598\n",
      "Epoch 111, CIFAR-10 Batch 3:  loss 0.0134447 accuracy 0.6674\n",
      "Epoch 111, CIFAR-10 Batch 4:  loss 0.0559645 accuracy 0.6612\n",
      "Epoch 111, CIFAR-10 Batch 5:  loss 0.0039742 accuracy 0.66\n",
      "Epoch 112, CIFAR-10 Batch 1:  loss 0.0315521 accuracy 0.6638\n",
      "Epoch 112, CIFAR-10 Batch 2:  loss 0.0123973 accuracy 0.659\n",
      "Epoch 112, CIFAR-10 Batch 3:  loss 0.0102852 accuracy 0.6656\n",
      "Epoch 112, CIFAR-10 Batch 4:  loss 0.0429577 accuracy 0.6568\n",
      "Epoch 112, CIFAR-10 Batch 5:  loss 0.00687968 accuracy 0.664\n",
      "Epoch 113, CIFAR-10 Batch 1:  loss 0.0330692 accuracy 0.664\n",
      "Epoch 113, CIFAR-10 Batch 2:  loss 0.00757037 accuracy 0.6668\n",
      "Epoch 113, CIFAR-10 Batch 3:  loss 0.0131733 accuracy 0.6626\n",
      "Epoch 113, CIFAR-10 Batch 4:  loss 0.0519219 accuracy 0.6566\n",
      "Epoch 113, CIFAR-10 Batch 5:  loss 0.00919917 accuracy 0.6668\n",
      "Epoch 114, CIFAR-10 Batch 1:  loss 0.0425922 accuracy 0.6724\n",
      "Epoch 114, CIFAR-10 Batch 2:  loss 0.0279837 accuracy 0.6566\n",
      "Epoch 114, CIFAR-10 Batch 3:  loss 0.00722788 accuracy 0.6648\n",
      "Epoch 114, CIFAR-10 Batch 4:  loss 0.0552649 accuracy 0.6578\n",
      "Epoch 114, CIFAR-10 Batch 5:  loss 0.0103578 accuracy 0.6584\n",
      "Epoch 115, CIFAR-10 Batch 1:  loss 0.0248046 accuracy 0.6662\n",
      "Epoch 115, CIFAR-10 Batch 2:  loss 0.014358 accuracy 0.6652\n",
      "Epoch 115, CIFAR-10 Batch 3:  loss 0.00789761 accuracy 0.6628\n",
      "Epoch 115, CIFAR-10 Batch 4:  loss 0.0646014 accuracy 0.6544\n",
      "Epoch 115, CIFAR-10 Batch 5:  loss 0.00623931 accuracy 0.657\n",
      "Epoch 116, CIFAR-10 Batch 1:  loss 0.0219952 accuracy 0.6552\n",
      "Epoch 116, CIFAR-10 Batch 2:  loss 0.0177379 accuracy 0.6578\n",
      "Epoch 116, CIFAR-10 Batch 3:  loss 0.00571166 accuracy 0.664\n",
      "Epoch 116, CIFAR-10 Batch 4:  loss 0.0488225 accuracy 0.6608\n",
      "Epoch 116, CIFAR-10 Batch 5:  loss 0.0131333 accuracy 0.6624\n",
      "Epoch 117, CIFAR-10 Batch 1:  loss 0.0355268 accuracy 0.663\n",
      "Epoch 117, CIFAR-10 Batch 2:  loss 0.00830542 accuracy 0.6602\n",
      "Epoch 117, CIFAR-10 Batch 3:  loss 0.0153669 accuracy 0.6684\n",
      "Epoch 117, CIFAR-10 Batch 4:  loss 0.0519474 accuracy 0.6638\n",
      "Epoch 117, CIFAR-10 Batch 5:  loss 0.00676148 accuracy 0.6542\n",
      "Epoch 118, CIFAR-10 Batch 1:  loss 0.0316559 accuracy 0.6608\n",
      "Epoch 118, CIFAR-10 Batch 2:  loss 0.0100473 accuracy 0.6616\n",
      "Epoch 118, CIFAR-10 Batch 3:  loss 0.00925438 accuracy 0.6584\n",
      "Epoch 118, CIFAR-10 Batch 4:  loss 0.0774467 accuracy 0.6574\n",
      "Epoch 118, CIFAR-10 Batch 5:  loss 0.00776781 accuracy 0.6616\n",
      "Epoch 119, CIFAR-10 Batch 1:  loss 0.037133 accuracy 0.664\n",
      "Epoch 119, CIFAR-10 Batch 2:  loss 0.014181 accuracy 0.6606\n",
      "Epoch 119, CIFAR-10 Batch 3:  loss 0.00569605 accuracy 0.6666\n",
      "Epoch 119, CIFAR-10 Batch 4:  loss 0.0608206 accuracy 0.6602\n",
      "Epoch 119, CIFAR-10 Batch 5:  loss 0.0113956 accuracy 0.6548\n",
      "Epoch 120, CIFAR-10 Batch 1:  loss 0.0296028 accuracy 0.6612\n",
      "Epoch 120, CIFAR-10 Batch 2:  loss 0.0113243 accuracy 0.6582\n",
      "Epoch 120, CIFAR-10 Batch 3:  loss 0.00892717 accuracy 0.666\n",
      "Epoch 120, CIFAR-10 Batch 4:  loss 0.0582879 accuracy 0.662\n",
      "Epoch 120, CIFAR-10 Batch 5:  loss 0.0115881 accuracy 0.6636\n",
      "Epoch 121, CIFAR-10 Batch 1:  loss 0.0260872 accuracy 0.658\n",
      "Epoch 121, CIFAR-10 Batch 2:  loss 0.00963698 accuracy 0.6644\n",
      "Epoch 121, CIFAR-10 Batch 3:  loss 0.00961508 accuracy 0.659\n",
      "Epoch 121, CIFAR-10 Batch 4:  loss 0.0562534 accuracy 0.6544\n",
      "Epoch 121, CIFAR-10 Batch 5:  loss 0.0101297 accuracy 0.651\n",
      "Epoch 122, CIFAR-10 Batch 1:  loss 0.0232049 accuracy 0.6666\n",
      "Epoch 122, CIFAR-10 Batch 2:  loss 0.0102454 accuracy 0.668\n",
      "Epoch 122, CIFAR-10 Batch 3:  loss 0.00747761 accuracy 0.6664\n",
      "Epoch 122, CIFAR-10 Batch 4:  loss 0.0496376 accuracy 0.6512\n",
      "Epoch 122, CIFAR-10 Batch 5:  loss 0.00561106 accuracy 0.6654\n",
      "Epoch 123, CIFAR-10 Batch 1:  loss 0.0146366 accuracy 0.6606\n",
      "Epoch 123, CIFAR-10 Batch 2:  loss 0.0138882 accuracy 0.6602\n",
      "Epoch 123, CIFAR-10 Batch 3:  loss 0.011995 accuracy 0.6606\n",
      "Epoch 123, CIFAR-10 Batch 4:  loss 0.0610695 accuracy 0.662\n",
      "Epoch 123, CIFAR-10 Batch 5:  loss 0.00697259 accuracy 0.6626\n",
      "Epoch 124, CIFAR-10 Batch 1:  loss 0.0142001 accuracy 0.6734\n",
      "Epoch 124, CIFAR-10 Batch 2:  loss 0.0117006 accuracy 0.6618\n",
      "Epoch 124, CIFAR-10 Batch 3:  loss 0.0183289 accuracy 0.6626\n",
      "Epoch 124, CIFAR-10 Batch 4:  loss 0.0506057 accuracy 0.656\n",
      "Epoch 124, CIFAR-10 Batch 5:  loss 0.0281117 accuracy 0.6582\n",
      "Epoch 125, CIFAR-10 Batch 1:  loss 0.0127913 accuracy 0.6642\n",
      "Epoch 125, CIFAR-10 Batch 2:  loss 0.0109975 accuracy 0.6568\n",
      "Epoch 125, CIFAR-10 Batch 3:  loss 0.00451318 accuracy 0.658\n",
      "Epoch 125, CIFAR-10 Batch 4:  loss 0.0492019 accuracy 0.6582\n",
      "Epoch 125, CIFAR-10 Batch 5:  loss 0.0115437 accuracy 0.6598\n",
      "Epoch 126, CIFAR-10 Batch 1:  loss 0.00433645 accuracy 0.66\n",
      "Epoch 126, CIFAR-10 Batch 2:  loss 0.00713169 accuracy 0.657\n",
      "Epoch 126, CIFAR-10 Batch 3:  loss 0.00575725 accuracy 0.6616\n",
      "Epoch 126, CIFAR-10 Batch 4:  loss 0.0761192 accuracy 0.6584\n",
      "Epoch 126, CIFAR-10 Batch 5:  loss 0.00662455 accuracy 0.6598\n",
      "Epoch 127, CIFAR-10 Batch 1:  loss 0.0148428 accuracy 0.6616\n",
      "Epoch 127, CIFAR-10 Batch 2:  loss 0.018393 accuracy 0.6586\n",
      "Epoch 127, CIFAR-10 Batch 3:  loss 0.0133853 accuracy 0.6612\n",
      "Epoch 127, CIFAR-10 Batch 4:  loss 0.0545836 accuracy 0.6624\n",
      "Epoch 127, CIFAR-10 Batch 5:  loss 0.0073421 accuracy 0.6622\n",
      "Epoch 128, CIFAR-10 Batch 1:  loss 0.0237687 accuracy 0.6608\n",
      "Epoch 128, CIFAR-10 Batch 2:  loss 0.0113726 accuracy 0.658\n",
      "Epoch 128, CIFAR-10 Batch 3:  loss 0.00870476 accuracy 0.6614\n",
      "Epoch 128, CIFAR-10 Batch 4:  loss 0.047756 accuracy 0.6598\n",
      "Epoch 128, CIFAR-10 Batch 5:  loss 0.00646369 accuracy 0.6518\n",
      "Epoch 129, CIFAR-10 Batch 1:  loss 0.0118594 accuracy 0.6628\n",
      "Epoch 129, CIFAR-10 Batch 2:  loss 0.0176359 accuracy 0.656\n",
      "Epoch 129, CIFAR-10 Batch 3:  loss 0.00814847 accuracy 0.6578\n",
      "Epoch 129, CIFAR-10 Batch 4:  loss 0.0463587 accuracy 0.6538\n",
      "Epoch 129, CIFAR-10 Batch 5:  loss 0.00407832 accuracy 0.6634\n",
      "Epoch 130, CIFAR-10 Batch 1:  loss 0.0134483 accuracy 0.6666\n",
      "Epoch 130, CIFAR-10 Batch 2:  loss 0.0154955 accuracy 0.657\n",
      "Epoch 130, CIFAR-10 Batch 3:  loss 0.00742112 accuracy 0.6624\n",
      "Epoch 130, CIFAR-10 Batch 4:  loss 0.0510099 accuracy 0.6546\n",
      "Epoch 130, CIFAR-10 Batch 5:  loss 0.00451886 accuracy 0.6614\n",
      "Epoch 131, CIFAR-10 Batch 1:  loss 0.0123298 accuracy 0.6672\n",
      "Epoch 131, CIFAR-10 Batch 2:  loss 0.0182918 accuracy 0.6606\n",
      "Epoch 131, CIFAR-10 Batch 3:  loss 0.0032956 accuracy 0.6602\n",
      "Epoch 131, CIFAR-10 Batch 4:  loss 0.0751647 accuracy 0.6632\n",
      "Epoch 131, CIFAR-10 Batch 5:  loss 0.00888659 accuracy 0.6602\n",
      "Epoch 132, CIFAR-10 Batch 1:  loss 0.00710658 accuracy 0.6602\n",
      "Epoch 132, CIFAR-10 Batch 2:  loss 0.0123124 accuracy 0.6564\n",
      "Epoch 132, CIFAR-10 Batch 3:  loss 0.00459067 accuracy 0.6634\n",
      "Epoch 132, CIFAR-10 Batch 4:  loss 0.0489003 accuracy 0.6512\n",
      "Epoch 132, CIFAR-10 Batch 5:  loss 0.00205796 accuracy 0.6618\n",
      "Epoch 133, CIFAR-10 Batch 1:  loss 0.00810597 accuracy 0.6684\n",
      "Epoch 133, CIFAR-10 Batch 2:  loss 0.0682354 accuracy 0.6586\n",
      "Epoch 133, CIFAR-10 Batch 3:  loss 0.00481045 accuracy 0.666\n",
      "Epoch 133, CIFAR-10 Batch 4:  loss 0.0541998 accuracy 0.6602\n",
      "Epoch 133, CIFAR-10 Batch 5:  loss 0.0044308 accuracy 0.6614\n",
      "Epoch 134, CIFAR-10 Batch 1:  loss 0.0189919 accuracy 0.6598\n",
      "Epoch 134, CIFAR-10 Batch 2:  loss 0.0142862 accuracy 0.6544\n",
      "Epoch 134, CIFAR-10 Batch 3:  loss 0.00390752 accuracy 0.6644\n",
      "Epoch 134, CIFAR-10 Batch 4:  loss 0.0474423 accuracy 0.659\n",
      "Epoch 134, CIFAR-10 Batch 5:  loss 0.00543831 accuracy 0.6674\n",
      "Epoch 135, CIFAR-10 Batch 1:  loss 0.00465617 accuracy 0.6722\n",
      "Epoch 135, CIFAR-10 Batch 2:  loss 0.0126984 accuracy 0.6572\n",
      "Epoch 135, CIFAR-10 Batch 3:  loss 0.0051525 accuracy 0.6622\n",
      "Epoch 135, CIFAR-10 Batch 4:  loss 0.046537 accuracy 0.6602\n",
      "Epoch 135, CIFAR-10 Batch 5:  loss 0.00275969 accuracy 0.6614\n",
      "Epoch 136, CIFAR-10 Batch 1:  loss 0.0115293 accuracy 0.6718\n",
      "Epoch 136, CIFAR-10 Batch 2:  loss 0.0103173 accuracy 0.6566\n",
      "Epoch 136, CIFAR-10 Batch 3:  loss 0.00323892 accuracy 0.6602\n",
      "Epoch 136, CIFAR-10 Batch 4:  loss 0.0403361 accuracy 0.6568\n",
      "Epoch 136, CIFAR-10 Batch 5:  loss 0.00235648 accuracy 0.6606\n",
      "Epoch 137, CIFAR-10 Batch 1:  loss 0.00684696 accuracy 0.6682\n",
      "Epoch 137, CIFAR-10 Batch 2:  loss 0.0267013 accuracy 0.6586\n",
      "Epoch 137, CIFAR-10 Batch 3:  loss 0.00403104 accuracy 0.6634\n",
      "Epoch 137, CIFAR-10 Batch 4:  loss 0.0499539 accuracy 0.656\n",
      "Epoch 137, CIFAR-10 Batch 5:  loss 0.00242332 accuracy 0.673\n",
      "Epoch 138, CIFAR-10 Batch 1:  loss 0.00440388 accuracy 0.6692\n",
      "Epoch 138, CIFAR-10 Batch 2:  loss 0.0106851 accuracy 0.66\n",
      "Epoch 138, CIFAR-10 Batch 3:  loss 0.00324966 accuracy 0.6666\n",
      "Epoch 138, CIFAR-10 Batch 4:  loss 0.0447035 accuracy 0.6642\n",
      "Epoch 138, CIFAR-10 Batch 5:  loss 0.00222132 accuracy 0.6654\n",
      "Epoch 139, CIFAR-10 Batch 1:  loss 0.00502413 accuracy 0.6688\n",
      "Epoch 139, CIFAR-10 Batch 2:  loss 0.00656586 accuracy 0.6594\n",
      "Epoch 139, CIFAR-10 Batch 3:  loss 0.00656515 accuracy 0.6642\n",
      "Epoch 139, CIFAR-10 Batch 4:  loss 0.0446326 accuracy 0.664\n",
      "Epoch 139, CIFAR-10 Batch 5:  loss 0.00219793 accuracy 0.6554\n",
      "Epoch 140, CIFAR-10 Batch 1:  loss 0.00845287 accuracy 0.6668\n",
      "Epoch 140, CIFAR-10 Batch 2:  loss 0.0108245 accuracy 0.6612\n",
      "Epoch 140, CIFAR-10 Batch 3:  loss 0.00601221 accuracy 0.663\n",
      "Epoch 140, CIFAR-10 Batch 4:  loss 0.0472684 accuracy 0.6576\n",
      "Epoch 140, CIFAR-10 Batch 5:  loss 0.00141311 accuracy 0.664\n",
      "Epoch 141, CIFAR-10 Batch 1:  loss 0.00435966 accuracy 0.6648\n",
      "Epoch 141, CIFAR-10 Batch 2:  loss 0.00949666 accuracy 0.6612\n",
      "Epoch 141, CIFAR-10 Batch 3:  loss 0.00509745 accuracy 0.6582\n",
      "Epoch 141, CIFAR-10 Batch 4:  loss 0.0512715 accuracy 0.6588\n",
      "Epoch 141, CIFAR-10 Batch 5:  loss 0.00332698 accuracy 0.6568\n",
      "Epoch 142, CIFAR-10 Batch 1:  loss 0.00901966 accuracy 0.6716\n",
      "Epoch 142, CIFAR-10 Batch 2:  loss 0.00566866 accuracy 0.662\n",
      "Epoch 142, CIFAR-10 Batch 3:  loss 0.00672142 accuracy 0.6636\n",
      "Epoch 142, CIFAR-10 Batch 4:  loss 0.0527245 accuracy 0.6572\n",
      "Epoch 142, CIFAR-10 Batch 5:  loss 0.00434366 accuracy 0.6566\n",
      "Epoch 143, CIFAR-10 Batch 1:  loss 0.00465789 accuracy 0.6654\n",
      "Epoch 143, CIFAR-10 Batch 2:  loss 0.00640937 accuracy 0.6538\n",
      "Epoch 143, CIFAR-10 Batch 3:  loss 0.00532842 accuracy 0.6586\n",
      "Epoch 143, CIFAR-10 Batch 4:  loss 0.0437078 accuracy 0.6562\n",
      "Epoch 143, CIFAR-10 Batch 5:  loss 0.00245697 accuracy 0.659\n",
      "Epoch 144, CIFAR-10 Batch 1:  loss 0.00514838 accuracy 0.6688\n",
      "Epoch 144, CIFAR-10 Batch 2:  loss 0.0052457 accuracy 0.663\n",
      "Epoch 144, CIFAR-10 Batch 3:  loss 0.00911369 accuracy 0.663\n",
      "Epoch 144, CIFAR-10 Batch 4:  loss 0.0472585 accuracy 0.6596\n",
      "Epoch 144, CIFAR-10 Batch 5:  loss 0.00345976 accuracy 0.6576\n",
      "Epoch 145, CIFAR-10 Batch 1:  loss 0.00734431 accuracy 0.6662\n",
      "Epoch 145, CIFAR-10 Batch 2:  loss 0.00475863 accuracy 0.6554\n",
      "Epoch 145, CIFAR-10 Batch 3:  loss 0.000822876 accuracy 0.6616\n",
      "Epoch 145, CIFAR-10 Batch 4:  loss 0.0460025 accuracy 0.6658\n",
      "Epoch 145, CIFAR-10 Batch 5:  loss 0.00517236 accuracy 0.6528\n",
      "Epoch 146, CIFAR-10 Batch 1:  loss 0.00520139 accuracy 0.6642\n",
      "Epoch 146, CIFAR-10 Batch 2:  loss 0.00466915 accuracy 0.6608\n",
      "Epoch 146, CIFAR-10 Batch 3:  loss 0.0102244 accuracy 0.6602\n",
      "Epoch 146, CIFAR-10 Batch 4:  loss 0.0454399 accuracy 0.6518\n",
      "Epoch 146, CIFAR-10 Batch 5:  loss 0.00177684 accuracy 0.6614\n",
      "Epoch 147, CIFAR-10 Batch 1:  loss 0.00996216 accuracy 0.659\n",
      "Epoch 147, CIFAR-10 Batch 2:  loss 0.00626809 accuracy 0.6614\n",
      "Epoch 147, CIFAR-10 Batch 3:  loss 0.00241891 accuracy 0.6596\n",
      "Epoch 147, CIFAR-10 Batch 4:  loss 0.0439113 accuracy 0.6586\n",
      "Epoch 147, CIFAR-10 Batch 5:  loss 0.00222711 accuracy 0.657\n",
      "Epoch 148, CIFAR-10 Batch 1:  loss 0.00716517 accuracy 0.6646\n",
      "Epoch 148, CIFAR-10 Batch 2:  loss 0.00389007 accuracy 0.655\n",
      "Epoch 148, CIFAR-10 Batch 3:  loss 0.0029876 accuracy 0.6666\n",
      "Epoch 148, CIFAR-10 Batch 4:  loss 0.0437707 accuracy 0.6546\n",
      "Epoch 148, CIFAR-10 Batch 5:  loss 0.00352906 accuracy 0.6656\n",
      "Epoch 149, CIFAR-10 Batch 1:  loss 0.00975446 accuracy 0.6596\n",
      "Epoch 149, CIFAR-10 Batch 2:  loss 0.0102368 accuracy 0.6644\n",
      "Epoch 149, CIFAR-10 Batch 3:  loss 0.00296593 accuracy 0.6634\n",
      "Epoch 149, CIFAR-10 Batch 4:  loss 0.046631 accuracy 0.6618\n",
      "Epoch 149, CIFAR-10 Batch 5:  loss 0.00124425 accuracy 0.6564\n",
      "Epoch 150, CIFAR-10 Batch 1:  loss 0.00603763 accuracy 0.6646\n",
      "Epoch 150, CIFAR-10 Batch 2:  loss 0.00427064 accuracy 0.6598\n",
      "Epoch 150, CIFAR-10 Batch 3:  loss 0.00335092 accuracy 0.6574\n",
      "Epoch 150, CIFAR-10 Batch 4:  loss 0.0318765 accuracy 0.6522\n",
      "Epoch 150, CIFAR-10 Batch 5:  loss 0.00296412 accuracy 0.657\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.66484375\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAIABJREFUeJzs3XecZFWZ//HP07knB9IwA44kGQQEhiAgMKisImvYVWEN\nu6Kra0RF19XVdYV1DT8jKxjWVcQMimlXQTEAEkRgSA6ZgSHMDMMME3umU1U9vz/OuXVv36muru6u\nTjXf9+tVr+q6595zT1VXeOrUc84xd0dERERERKBpohsgIiIiIjJZKDgWEREREYkUHIuIiIiIRAqO\nRUREREQiBcciIiIiIpGCYxERERGRSMGxiIiIiEik4FhEREREJFJwLCIiIiISKTgWEREREYkUHIuI\niIiIRAqORUREREQiBcciIiIiIpGCYxERERGRSMHxBDOzZ5jZ35rZ283sX83sQ2Z2jpm92syONrMZ\nE93GwZhZk5m93MwuNbOHzGyrmXnm8vOJbqPIZGNmi3Ovk/Pqse9kZWbLcvfh7Iluk4hINS0T3YBd\nkZnNA94OvAV4xhC7l8zsHuA64FfA7929Z4ybOKR4Hy4HTp3otsj4M7NLgDcMsVsB2AxsAG4jPId/\n6O5bxrZ1IiIiI6ee43FmZn8N3AP8J0MHxhD+R4cSgulfAq8au9YNy3cYRmCs3qNdUguwG3Aw8Frg\nq8BqMzvPzPTFfArJvXYvmej2iIiMJX1AjSMzOxP4AdCcK9oK/AV4EugF5gL7AkuYhF9gzOy5wBmZ\nTY8C5wO3Atsy23eMZ7tkSpgOfAw42cxOd/feiW6QiIhIloLjcWJm+xN6W7OB8QrgI8AV7l6ocMwM\n4BTg1cDfALPGoam1+Nvc7Ze7+50T0hKZLD5ASLPJagH2BJ4HvIPwhS9xKqEn+U3j0joREZEaKTge\nP58A2jO3fwe8zN27BzvA3bsIeca/MrNzgDcTepcn2tLM36sUGAuwwd1XVdj+EHCDmX0J+D7hS17i\nbDP7krvfMR4NnIriY2oT3Y7RcPdrmOL3QUR2LZPuJ/tGZGadwMsym/qBN1QLjPPcfZu7f9Hdf1f3\nBg7fHpm/10xYK2TKiM/11wEPZDYb8LaJaZGIiEhlCo7Hx1FAZ+b2je4+lYPK7PRy/RPWCplSYoD8\nxdzmF0xEW0RERAajtIrxsVfu9urxPLmZzQJOAhYC8wmD5tYBf3b3x0ZSZR2bVxdmth8h3WMR0Aas\nAq5296eGOG4RISd2H8L9WhuPe2IUbVkIPBvYD5gTN28EHgP+tItPZfb73O39zazZ3YvDqcTMDgUO\nARYQBvmtcvcf1HBcO3ACYaaYPYAi4bVwl7vfNZw2DFL/gcCxwN5AD/AEcLO7j+trvkK7DgKOAHYn\nPCd3EJ7rK4B73L00gc0bkpntAzyXkMM+k/B6WgNc5+6b63yu/QgdGvsQxoisA25w94dHUeezCI//\nXoTOhQLQBTwOPAjc5+4+yqaLSL24uy5jfAH+DvDM5cpxOu/RwJVAX+782ctdhGm2rEo9y6ocP9jl\nmnjsqpEem2vDJdl9MttPAa4GShXq6QO+AsyoUN8hwBWDHFcCfgIsrPFxbort+Cqwcoj7ViTkm59a\nY93fzh3/9WH8/z+VO/aX1f7Pw3xuXZKr++waj+us8JjsUWG/7PPmmsz2NxICunwdm4c476HAj4Ht\nVf43jwPvBVpH8HicCPx5kHoLhLEDS+O+i3Pl51Wpt+Z9Kxw7B/gPwpeyas/J9cDFwDFD/I9rutTw\n/lHTcyUeeyZwR5Xz9QO/BZ47jDqvyRy/KrP9OMKXt0rvCQ7cBBw/jPO0Au8n5N0P9bhtJrznnFaP\n16cuuugyusuEN2BXuADPz70RbgPmjOH5DPhMlTf5SpdrgLmD1Jf/cKupvnjsqpEem2vDgA/quO3d\nNd7HW8gEyITZNnbUcNwqYN8aHu83jeA+OvB5oHmIuqcD9+aO+7sa2nRa7rF5Aphfx+fYJbk2nV3j\ncR0VHofdK+yXfd5cQxjM+qMqj2XF4JjwxeWzhC8ltf5f7qTGL0bxHB+u8XnYR8i7Xpzbfl6Vumve\nN3fc3wCbhvl8vGOI/3FNlxreP4Z8rhBm5vndMM99AdBUQ93XZI5ZFbedQ/VOhOz/8MwazrE7YeGb\n4T5+P6/Xa1QXXXQZ+UVpFeNjOeHDOZnGbQbwHTN7rYcZKertf4B/zG3rI/R8rCH0KB1NWKAhcQrw\nRzM72d03jUGb6irOGf1f8aYTepdWEr4YHAHsn9n9aOBC4I1mdipwGWlK0X3x0keYV/qwzHHPIPTc\nDrXYST53vxu4m/Cz9VZCb+m+wOGElI/E+wg9Xx8arGJ3325mZxF6JTvi5q+b2a3u/lClY8xsL+C7\npOkvReC17v70EPdjPCzK3XZCEDeUCwhTGibH3E4aQO8HPDN/gJk1E/7Xr8wV7SC8JtcSXpP7A88h\nfbwOB240s2PdfV21RpnZewkz0WQVCf+vxwkpAEcS0j9aCQFn/rVZV7FNX2Dn9KcnCb8UbQCmEf4X\nhzFwFp0JZ2YzgWsJr+OsTcDN8XoBIc0i2/b3EN7TXj/M870O+FJm0wpCb28v4bmxlPSxbAUuMbPb\n3f3BQeoz4KeE/3vWOsJ89hsIX6Zmx/oPQCmOIpPLREfnu8qF8JN2vpdgDWFBhMOo38/db8ido0QI\nLObk9mshfEhvye3/wwp1dhB6sJLLE5n9b8qVJZe94rGL4u18ask/D3Jc+dhcGy7JHZ/0iv0K2L/C\n/mcSgtTs43B8fMwduBE4osJxy4Cnc+d6yRCPeTLF3qfiOSr2XhG+lHyQgT/tl4Djavi/vi3XpluB\ntgr7NRF+Zs7u+9ExeD7n/x9n13jcP+WOe2iQ/VZl9tmW+fu7wKIK+y+usO0TuXOtI6RlVHrc9mfn\n1+gVQ9yXw9i5t/EH+edv/J+cCTwV99mYO+a8KudYXOu+cf8XsXMv+bWEPOud3mMIweVLCT/pL8+V\n7Ub6mszWdzmDv3Yr/R+WDee5Anwrt/9W4K3k0l0IweXn2bnX/q1D1H9NZt8u0veJnwEHVNh/CeHX\nhOw5LqtS/xm5fR8kDDyt+B5P+HXo5cClwI/r/VrVRRddhn+Z8AbsKhdCz1RP7k0ze3maEOh9lPCT\n+PQRnGMGO/+Ueu4QxxzHznmYVfPeGCQfdIhjhvUBWeH4Syo8Zt+nys+ohCW3KwXUvwPaqxz317V+\nEMb996pWX4X9j889F6rWnznusly7/qvCPh/J7fOHao/RKJ7P+f/HkP9PwpesfIpIxRxqKqfjfHoY\n7TuOgUHi/VT40pU7pomdc7xPr7L/1bl9vzxE/c9m58C4bsExoTd4XW7/i2r9/wN7VinL1nnJMJ8r\nNb/2CYNjs/vuAE4cov535Y7pYpAUsbj/NRX+BxdRfdzFngx8b+0d7ByEsQfJfv3AM4fxWHUM57HV\nRRddxuaiqdzGiYeFMv6eEBRVMg94CWEAzVXAJjO7zszeGmebqMUbSGdHAPi1u+enzsq368/Av+c2\nv6fG802kNYQeomqj7L9J6BlPJKP0/96rLFvs7r8kBFOJZdUa4u5PVquvwv5/Ar6c2fSKOIvCUN5C\nSB1JvNvMXp7cMLPnEZbxTqwHXjfEYzQuzKyD0Ot7cK7ov2us4g5C4F+rD5GmuxSAV7h71QV04uP0\nVgbOJvPeSvua2SEMfF48AJw7RP13A/9StdWj8xYGzkF+NXBOrf9/HyKFZJzk33vOd/cbqh3g7hcR\nev0T0xle6soKQieCVznHOkLQm2gjpHVUkl0J8g53f6TWhrj7YJ8PIjKOFByPI3f/MeHnzetr2L2V\n0IvyNeBhM3tHzGWr5nW52x+rsWlfIgRSiZeY2bwaj50oX/ch8rXdvQ/If7Be6u5ra6j/D5m/94h5\nvPX0i8zfbeycX7kTd99KSE/py2z+lpntG/9fPyTNa3fgH2q8r/Wwm5ktzl0OMLMTzOxfgHuAV+WO\n+b67L6+x/i96jdO9xan0sovu/MDd763l2BicfD2z6VQzm1Zh13xe62fi820oFxPSksbCW3K3qwZ8\nk42ZTQdekdm0iZASVot/y90eTt7xF929lvnar8jdfk4Nx+w+jHaIyCSh4Hicufvt7n4ScDKhZ7Pq\nPLzRfEJP46Vm1lZph9jzeFRm08PufnONbeonTHNVro7Be0Umi6tq3G9l7vZvazwuP9ht2B9yFsw0\ns73zgSM7D5bK96hW5O63EvKWE3MJQfG3GTjY7bPu/uvhtnkUPgs8krs8SPhy8v/YecDcDewczFXz\ny6F3KVvGwPe2nwzjWIA/Zv5uBY6psM/xmb+Tqf+GFHtxLx9me4ZkZrsT0jYSt/jUW9b9GAYOTPtZ\nrb/IxPt6T2bTYXFgXy1qfZ3cl7s92HtC9lenZ5jZO2usX0QmCY2QnSDufh1wHZR/oj2BMKvCMYRe\nxEpfXM4kjHSu9GZ7KANHbv95mE26CXhH5vZSdu4pmUzyH1SD2Zq7fX/FvYY+bsjUljg7wgsJsyoc\nQwh4K36ZqWBujfvh7heY2TLCIB4Iz52smxheCsJ46ibMMvLvNfbWATzm7huHcY4Tc7c3xS8ktWrO\n3d6PMKgtK/tF9EEf3kIUtwxj31odl7t93RicY6wtzd0eyXvYIfHvJsL76FCPw1avfbXS/OI9g70n\nXMrAFJuLzOwVhIGGV/oUmA1IZFen4HgScPd7CL0e3wAwszmEnxfPJUwrlfUOM7u4ws/R+V6MitMM\nVZEPGif7z4G1rjJXqNNxrdV2NrPjCfmzh1Xbr4pa88oTbyTk4e6b274ZeI2759s/EYqEx/tpwtRr\n1xFSHIYT6MLAlJ9a5KeL+2PFvWo3IMUo/kqT/X/lf50YSsUp+EYpn/ZTUxrJJDMR72E1r1bp7v25\nzLaK7wnufrOZfYWBnQ0vjJeSmf2FkFr3R8KA5lp+PRSRcaS0iknI3Te7+yWEno//qLDLORW2zcnd\nzvd8DiX/IVFzT+ZEGMUgs7oPTjOzFxMGP400MIZhvhZj79MnKxS9391XjaIdI/VGd7fcpcXd57v7\nQe5+lrtfNILAGMLsA8NR73z5Gbnb+dfGaF9r9TA/d7uuSyqPk4l4DxurwarvIvx6syO3vYmQq/xO\nwuwza83sajN7VQ1jSkRknCg4nsQ8+BjhTTTrhbUcPszT6Y15BOJAuO8xMKVlFfBx4HTgWYQP/Y5s\n4EiFRSuGed75hGn/8l5vZrv667pqL/8IDPXamIyvtSkzEK+Kyfi41iS+d3+SkJLzQeBP7PxrFITP\n4GWEMR/XmtmCcWukiAxKaRVTw4XAWZnbC82s0927M9vyPUWzh3mO/M/6yourzTsY2Gt3KfCGGmYu\nqHWw0E5iD9O3gYUVik8ljNyv9IvDriLbO10AOuucZpJ/bYz2tVYP+R75fC/sVNBw72FxCrjPAJ8x\nsxnAscBJhNfpiQz8DD4J+HVcmbHmqSFFpP529R6mqaLSqPP8T4b5vMwDhnmOg4aoTyo7I/P3FuDN\nNU7pNZqp4c7NnfdmBs568u9mdtIo6p/qsvP1tjDKXvq8GLhkf/Lff7B9BzHc12Yt8nM4LxmDc4y1\nhn4Pc/cud/+Du5/v7ssIS2D/G2GQauJw4E0T0T4RSSk4nhoq5cXl8/FWMHD+2/zo9aHkp26rdf7Z\nWjXCz7yVZD/Ar3f37TUeN6Kp8szsaODTmU2bCLNj/APpY9wM/CCmXuyKbsrdfsEYnOO2zN8HxkG0\ntao0Ndxo3cTA19hU/HKUf88ZzXtYiTBgddJy9w3u/gl2ntLwpRPRHhFJKTieGp6Vu92VXwAj9mZl\nP1z2N7P81EgVmVkLIcAqV8fwp1EaSv5nwlqnOJvssj/91jSAKKZFvGa4J4orJV7GwJzaN7n7Y+7+\nG8Jcw4lFhKmjdkW/y90+ewzO8afM303AK2s5KOaDv3rIHYfJ3dcDd2c2HWtmoxkgmpd9/Y7Va/cW\nBubl/s1g87rnxfuaned5hbtvq2fjxtBlDFw5dfEEtUNEIgXH48DM9jSzPUdRRf5ntmsG2e8Hudv5\nZaEH8y4GLjt7pbs/XeOxtcqPJK/3inMTJZsnmf9ZdzB/z8h+9v46YYBP4kJ3/3nm9kcY2Gv6UjOb\nCkuB15W7PwT8PrPpODPLrx45Wt/P3f4XM6tlIOCbqJwrXg9fz93+Qh1nQMi+fsfktRt/dcmuHDmP\nynO6V/Lx3O3v1aVR4yDmw2dntaglLUtExpCC4/GxhLAE9KfNbI8h984ws1cCb89tzs9ekfg2Az/E\nXmZm7xhk36T+Y9j5g+VLw2ljjR4Gsos+PH8MzjER/pL5e6mZnVJtZzM7ljDAcljM7J8YOCjzduAD\n2X3ih+xrGBiwf8bMsgtW7CrOy93+HzM7bTgVmNkCM3tJpTJ3v5uBC4McBHxxiPoOIQzOGivfZGC+\n9QuBC2oNkIf4Ap+dQ/iYOLhsLOTfez4e36MGZWZvJ10QB2A74bGYEGb29rhiYa37n87A6QdrXahI\nRMaIguPxM40wpc8TZvYzM3tltTdQM1tiZl8HfsTAFbtuY+ceYgDiz4jvy22+0Mw+a2YDRn6bWYuZ\nvZGwnHL2g+5H8Sf6uoppH9nlrE8xs2+Y2QvM7MDc8spTqVc5vxTwT8zsZfmdzKzTzM4l9GjOIqx0\nWBMzOxS4ILOpCzir0oj2OMdxNoexDbhsGEvpNgR3v56B80B3EmYC+IqZHTjYcWY2x8zONLPLCFPy\n/UOV05zDwC987zSz7+efv2bWZGavJvziM5cxmoPY3XcQ2psdo/Bu4PdxkZqdmFm7mf21mV1O9RUx\nswupzAB+ZWZ/E9+n8kujj+Y+/BH4bmbTdOC3ZvaP+Z55M5tlZp8BLspV84ERzqddLx8EHovPhVcM\n9tqL78H/QFj+PWvK9HqLNCpN5Tb+Wgmr370CwMweAh4jBEslwofnIcA+FY59Anh1tQUw3P1iMzsZ\neEPc1AT8M3COmf0JWEuY5ukYYLfc4feycy91PV3IwKV9/zFe8q4lzP05FVxMmD0iCbjmA78ws0cJ\nX2R6CD9DH0f4ggRhdPrbCXObVmVm0wi/FHRmNr/N3QddPczdLzezrwFvi5sOAL4KvL7G+9QoPkpY\nQTC5302Ex/3t8f9zD2FAYyvhNXEgw8j3dPe/mNkHgS9kNr8WOMvMbgIeJwSSSwkzE0DIqT2XMcoH\nd/erzOyfgc+Tzvt7KnCjma0F7iKsWNhJyEs/nHSO7kqz4iS+Abwf6Ii3T46XSkabyvEuwkIZyeqg\ns+P5/5+Z3Uz4crEXcHymPYlL3f2rozx/PXQQnguvBdzMHgAeIZ1ebgFwJDtPV/dzd/+/cWuliFSk\n4Hh8bCQEv/lgFELgUsuURb8D3lLj6mdvjOd8L+kHVTvVA87rgZePZY+Lu19mZscRgoOG4O69saf4\nD6QBEMAz4iWvizAg674aT3Eh4ctS4lvuns93reRcwheRZFDW68zs9+6+ywzSi18i/97M7gT+k4EL\ntQz2/8mrOleuu38xfoH5OOlrrZmBXwITBcKXwdEuZ11VbNNqQkCZ7bVcwMDn6HDqXGVmZxOC+s4h\ndh8Vd98a05N+SgjsE/MJC+sM5suEnvLJxgiDqvMDq/MuI+3UEJEJpLSKceDudxF6Op5P6GW6FSjW\ncGgP4QPipe5+Wq3LAsfVmd5HmNroKiqvzJS4m/CGfPJ4/BQZ23Uc4YPsFkIv1pQegOLu9wFHEX4O\nHeyx7gK+Axzu7r+upV4zew0DB2PeR+Wlwyu1qYeQo5wd6HOhmR1cy/GNxN0/RxjIeAE7zwdcyf2E\nLyXHu/uQv6TE6bhOZmDaUFaJ8Do80d2/U1OjR8ndf0SY3/lzDMxDrmQdYTBf1cDM3S8jjJ84n5Ai\nspaBc/TWjbtvJkzB91pCb/dgioRUpRPd/V2jWFa+nl5OeIxuYuj3thKh/We4+99p8Q+RycHcG3X6\n2ckt9jYdFC97kPbwbCX0+t4N3FOPlb1ivvHJhFHy8wiB2jrgz7UG3FKbOLfwyYSf5zsIj/Nq4LqY\nEyoTLA6MO5zwS84cwpfQzcBK4G53f6rK4UPVfSDhS+mCWO9q4GZ3f3y07R5Fm4yQpvBsYHdCqkdX\nbNvdwL0+yT8IzGxfwuO6J+G9ciOwhvC6mvCV8AZjZh3AoYRfB/ciPPb9hIHTDwG3TXB+tIhUoOBY\nRERERCRSWoWIiIiISKTgWEREREQkUnAsIiIiIhIpOBYRERERiRQci4iIiIhECo5FRERERCIFxyIi\nIiIikYJjEREREZFIwbGIiIiISKTgWEREREQkUnAsIiIiIhIpOBYRERERiRQci4iIiIhECo5FRERE\nRCIFxyIiIiIikYJjEREREZFIwbGIiIiISKTgWEREREQkUnAsIiIiIhIpOBYRERERiRQci4iIiIhE\nCo5FRERERCIFxyIiIiIikYLjUTIzj5fFE90WERERERkdBcciIiIiIpGCYxERERGRSMGxiIiIiEik\n4FhEREREJFJwPAQzazKzc8zsTjPrNrP1ZvZ/ZnZ8DcceaWbfM7PHzazXzDaY2W/M7JVDHNdsZu81\ns7sy5/ylmZ0YyzUIUERERGQMmLtPdBsmLTNrAS4HXh43FYAuYE78+yzgJ7Hsme6+KnPsPwFfJf0C\nshmYCTTH298Dznb3Yu6crcAvgNMHOeffxTbtdE4RERERGR31HFf3QUJgXAI+AMx297nAfsDvgIsr\nHWRmJ5AGxpcD+8Tj5gAfARx4PfCvFQ7/N0JgXATeC8yKxy4Gfg18o073TURERERy1HM8CDObDqwB\nZgHnu/t5ufJ24DbgkLip3ItrZr8Hng/cAJxSoXf4k4TAuAtY6O5b4/YZwJPAdOAj7v7J3HGtwC3A\nc/LnFBEREZHRU8/x4P6KEBj3Al/MF7p7L/C5/HYzmwecGm9+Kh8YR/8P6AFmAC/JbH8RITDuAb5U\n4Zz9wBeGdS9EREREpGYKjgd3VLy+w923DLLPtRW2HQkYIXWiUjmxvuW58yTHJufsGuSc1w3aYhER\nEREZFQXHg9s9Xq+pss/qKsdtqRLgAjyR2x9gt3i9tspx1dojIiIiIqOg4HjstI/gGKthHyWJi4iI\niIwRBceDWx+v966yT6Wy5LhOM9u9QnliUW7/7N8LhnlOEREREakDBceDuy1eH2FmswbZ55QK224n\n7d09tUI5ZjYbWJo7T3Jscs4Zg5zzpEG2i4iIiMgoKTge3G+ArYT0iPfkC82sDXh/fru7bwSujjc/\naGaVHuMPAh2EqdyuyGy/Ctgey95Z4ZwtwLnDuhciIiIiUjMFx4Nw9x3AZ+LNj5nZ+8ysEyAu2/wz\nYJ9BDv8oYeGQo4BLzWxRPG6GmX0Y+FDc79PJHMfxnNtIp437z7hsdXLOfQkLijyzPvdQRERERPK0\nCEgVo1w++q3AVwhfQJywfPQs0uWjvw+8ocICIW3A/xHmWQboj+ecG/8+C/hpLNvb3avNbCEiIiIi\nw6Ce4yrcvQC8Eng3cBchIC4CvyKsfPfTKsf+N3AM8APC1GwzgC3Ab4FXu/vrKy0Q4u59wBmElI0V\nhB7oIiFgPpk0ZQNCwC0iIiIidaKe4ynGzF4A/A541N0XT3BzRERERBqKeo6nng/E699OaCtERERE\nGpCC40nGzJrN7HIze3Gc8i3Z/mwzuxx4ESH3+EsT1kgRERGRBqW0ikkmDgLsz2zaCrQA0+LtEvB2\nd//6eLdNREREpNEpOJ5kzMyAtxF6iA8D9gBagSeBPwIXuPttg9cgIiIiIiOl4FhEREREJFLOsYiI\niIhIpOBYRERERCRScCwiIiIiEik4FhERERGJWia6ASIijcjMHgFmAasmuCkiIlPVYmCruz9zPE/a\nsMHxY/df7QBNXkg3NoWO8r7eXgBKpVK5qFQKs3aYxV1b0k715ubmeHhT3Md2Og7SbYnW1tbwRzHu\n0dRcLnMLdTU1t5a3tbV2AtDf3xfKLJ1JpFgKUx87oc1tbR3lspa29lgW6my2tM6mJh9Qp2XL4v2a\nOf+AnRsvIqM1q7Ozc96SJUvmTXRDRESmonvvvZfu7u5xP2/DBsctMfAr9qfraXgMhpMgt1hMA+dC\nIQafcWq7Fk8fmuYY1BYLIcptbU3LLAlyKwTOSV3FYjyvp8F4MQa5VupL6/LQBqM5njcNZK2pLR4X\n9i8lETdQLBbjucPtkqX3qzxTXyxsaUkD9Kamhv33SwMzs1UA7r54YlsypFVLliyZt3z58oluh4jI\nlLR06VJuu+22VeN9XuUci4iIiIhE6joUERkjK1ZvYfGHfjXRzRARqdmqT58x0U2YcA0bHBdiqkF/\nX295W5KbWyqFtINSMU25wGNqQpKGUErzfUuFsH85ZSLNjiinKVjTzp3w5TyZWFdLS/pwJ9U3ZdIw\n+gth4/auLgAee/SJctnmzdsAaJ8e6jjk2UvKZfPmhfxjL4X7UMikXCRpH8TzFUgfDzykaHTu1HIR\nERGRXZPSKkRk0rHgXWZ2t5n1mNlqM7vIzGYPsn+7mX3IzO4ysx1mttXMrjOzM6vU/x4zuydfv5mt\nSvKaRURk19OwPcfJIDVPO4DTAWtx8F0Tmd7hOFguGURHZkBeU5yJwuJ1qZjpmW1OB7gB9Pf1kddS\nnvki7SVubQm9vVu3paMwb7k1DNy5864VANx6y23lsi1btgKwYO89AHjhC5eVy0477RQA9lm0IG7J\n3umB33/6+tOe4+z9F5lkLgDeDawFvg70Ay8HjgPagPILzczagN8ApwD3AV8GpgGvAi4zsyPc/cO5\n+r8MvB1YE+vvA14GHAu0xvOJiMguqGGDYxGZmszsBEJgvBI41t03xu0fAa4GFgCPZg55PyEwvhJ4\nmXuYv9HMzgduBv7VzH7p7jfG7ScRAuMHgOPcfXPc/mHgd8DeufqHau9g01EcXGsdIiIyeTRscJzM\nTVzK9Jz2x2ndkindst3KVu7VjfMdZ+pKeqGT6dqamnaeFjjJ9/VMnel+YVspM5UbpVDXjTfeXt70\n9W9+F4An120AoKtrW7msN87NvG79pnD95Ppy2coH7wfg9a97FQAHLXlWer88nKct5lsX+3rS+1VK\ne8BFJpEPRlA5AAAgAElEQVQ3xutPJIExgLv3mNm/EgLkrDcRXmTvSwLjuP9TZvZx4BvAm4EbY9Eb\nMvVvzuzfF+u/vq73RkREppSGDY5FZMo6Kl5fW6HsOqAcAJvZTOAAYLW731dh/z/E6yMz25K/KwXB\nN2Xrr4W7L620PfYoH1WpTEREJi8NyBORySYZdLcuX+DuReDpCvuuHaSuZPucEdYvIiK7mIbtOW7v\nnA6kK98BeCmkJnhMc+jvSzuIkmna0qnPsqkTNmCTZZaWTlIT0gGAO6dqFGI6RVNm8N7qteFz+Ypf\np78QP746fI53ds4AoLUtTXvo7glt74lT023clKZc3HDdLQDsNjd8/u+9YEG5bOacEAcUirFdmSWp\n9c1IJqkt8XpP4OFsgZk1A/OB1bl99xqkrgW5/QC2DqN+ERHZxTRscCwiU9ZthHSEU8gFr8BJZN63\n3H2bma0E9jOzA939wdz+p2bqTNxOSK14XoX6n0sd3xcPXTib5ZpQX0RkSmnY4DgZkGdNaW9tU/zb\niuFuFyztOU56fJNBd1hm8FxTKPPYq2zWWi4q9Ic6CnEBj9bW9CFtbo7nib3XxWLaV7v8trsAuOPO\nv5S3zZo1F4AzXvJSAH7729+Vy9ZvCoP0prWGHvH22LsMsK17OwB3rggpl+ueerJcNnP2NCCdYm7A\nYiWlzH0UmTwuIQyg+4iZ/SIzW0UH8KkK+18MfAL4rJm9MqZGYGa7AR/N7JP4DmEQX1L/lrh/G/DJ\nMbg/IiIyhTRscCwiU5O732BmFwLnACvM7HLSeY43sXN+8eeA02P5nWZ2BWGe41cDewCfcffrM/Vf\na2ZfB/4JuNvMfhLrfykh/WINA9bBFBGRXYnSTkVkMnoPITjeArwVeA1hoY8XklkABMIUbMBpwEfi\npnMI07U9CLzW3T9Yof63A+8DuoC3Aa8lzHF8GjCLNC9ZRER2MQ3bc5ykSTRnBsH19sR5jisMnsv+\nDVDKpFVYee7j8F2imFk7yz3U39YaUi1a29rKZU1xkF9zcyh7/IkN5bJrrg5Trq7fUJ7GlYULnwFA\nS0tbPC7990xvC+kRzzsxrIa3ZMkh5bLLfvg9AO5/KKxbcMutaXrlon3COKXW2L6kvQBeVOeYTE4e\nXpAXxUve4gr79xBSImpKi3D3EvDFeCkzswOBGcC9w2uxiIg0CvUci8gux8z2Mhu4trqZTSMsWw3w\ns/FvlYiITAYN23Nc8p1XpUumW+uPq80V+3cekFeMg9RaWzvLZa2toSe3PW5rtrR3uBSncrO4Gl52\n9bxkVbv+/lD3LbesKJfd9ZcHY5vSHutNm8JsU1dccUVsU9pFvffCvcP+hdDmjZke5/7+0Obt27sA\nuP32e8plzzvxOAD22mu30M5sD3lpWGsdiDSS9wKvMbNrCDnMewEvABYRlqH+8cQ1TUREJlLDBsci\nIlX8FngO8FfAPMKqeA8AXwIu8HyelYiI7DIaNjhOeomTvF+A1pZwd4uWLM6R+fyLf7fFnOGZs+eV\ni5piznDS09zf31MuS3qmPQ5ub8r8UtvfH3p+1z0VxvZc98c/l8s2bgzTrzW1pP+CZHGSrVs3AfDC\n007JtD204YY/3gDAg/fcXy7r29Ed2hd7sR9+aE257KEHQh7y7ruFaeJaM49HqZRJnhbZhbj774Hf\nT3Q7RERk8lHOsYiIiIhIpOBYRERERCRq2LQKjykGLS3panalYkiZaGoO06S2taffDTqnhanSps0I\nK9A1tU8rl3V3h7SFvu4dYUNM2QCwljgQL04ZV8wMcmtpCdu6e0IaxoaNW8plvXHFuta2tH3JWL7t\n27cB0JxZ3a+jowOAbdtCHeZp29vawr9xR3dIy3j88afKZTfeuByAJUv2A2CP3eaXy8hM6yYiIiIi\n6jkWERERESlr2J7jZIq1Ads83N22jtArnCyMAdDUGsr6kkF6Pb3lsvKUb7HHuJgZkGdNbfE69MI2\nk/bGlgpxsF4pdAlnl9zoK4U6mktpG/p6wrnb20Mv8R23pVO/JT3TzS2hrK8vbV/Se+2E62JmoOFD\nD68CYM2a0Ju8x267pcc1N+y/X0RERGRE1HMsIiIiIhI1bNdhqbyYR6Z3uKMdgP7YQ1sopvnBxUKY\n1syawveFlpZ0yrNkmrckh5hSS+a4cN3XH3KI21s70jbEsm3bQq7yxk2bM1XGXuhi2tPcGo/t7JwB\nwLp1T5fLkpzjjvZZAHR1rUvrIpwoWcAk03J2bA891KvXrAXg2YccWC5ratZ3IxEREZEsRUciIiIi\nIpGCYxERERGRqGHTKpK0hVIpuyJcSDvojYPZmprSspaW3EORGdCXTAtXLITrAevKekzH8DilW+br\nRinWv/KhVQBs3LipXNYcB9hZZkW9pqaQAtLfl6SEtGXaEPbriavueWYatpKHtIrk3hQKfeWynjiN\n3Lonw4C8/v60rL05rV9ERERE1HMsInViZovNzM3skolui4iIyEg1bM9x0r+b9BYDFAqhh7UpDqxr\nyXTzFuM0bUnvq7HzcV5KpmZLz5J0/CZTrSU9uwBbt4Ve2/vvXwnAjh3d5bJk4Y7mCv+CpM3ZBUyS\nNvT0hDpaW7M9x0kdrbEt2fsV6tqw4enYhu3lso72Bv73i4iIiIyAoiMRkTGyYvUWFn/oVxPdDFZ9\n+oyJboKIyJShtAoRERERkahhe47LaRKWDrprjukGzS1hvuNiJgUimcs4GW3XlykrxRSLtjhArq8v\nLdu4MQzue+qpJwFYvTadm3jdUxsBePDBR8P5CplV+2JWRHYe5tbm8HchnrtYTM/T2xtSNArFkFbR\n0ZEOprOmuPJf23QAZk6fVi5raw33tSmu4FcopHWaDRhaKFI3ZrYY+DTwQmAGsAI4z91/mduvHTgX\neC1wAFAA7gQudPcfVajzEeDbwCeBjwOnArsBz3f3a8xsP+BDwPOBhUA3sBq4AfiIuz+dq/M1wD8B\nRwCdsf7vA591915ERGSX07DBsYhMmGcANwMPA98F5gFnAb8wsxe6+9UAZtYG/AY4BbgP+DIwDXgV\ncJmZHeHuH65Q//7An4EHCIFsJ7DVzBYAtwCzgCuAnwAdwDOBvwcuAsrBsZl9E3gT8ATwU2Az8FxC\n0P0CMzvN3dOVggZhZssHKTp4qGNFRGTyadzguBRXwetPe2s7p3XGojhYL9OT29wcHopi7BUuZVbP\na2kPvbQ9/eG45bc/UC770w23A7BxUxcAnnlIN20OU7dt2RZ6e9vb0t7evsLA3miAefPmALB9e6ir\ntycdPJd8Rnd2tMT7kpnmLXZ3J33k7XElQIB99l0EwAEHHhDuS+Z8ZmmvtUgdLSP0Ep+fbDCzHwC/\nBj4AXB03v58QGF8JvCwJRM3sfEJw/a9m9kt3vzFX//OAT+UDZzM7hxCIv9fd/ytXNh3SUbZmdjYh\nMP4Z8Dp3786UnQd8DHgnMKAeERFpfMo5FpF6exT4z+wGd/8N8BhwbGbzmwiJTO/L9tC6+1OE3luA\nN1eofx1wfoXtie78Bnffng2AgfcQUjjelNtOPPfTwOuqnCNb99JKF0JvuIiITDEN23NcTqfN5Bw3\nxdzc5BtBwdLe4WJc6KMQr1syucDbukLq4VW//xMA//urq8tljz+6GoA9d18IwNy588tl6zesT84M\nwJzZc8pl2+OUaoX+tA19hXCeI496DgDTOtMe4KefCot4bNmyGYD2TM7x5thD3d0dPuP7+9NUyQUL\n9wLgoIMPAmDGzNnlsiYtAiJj4w5PVuEZ6HHgeAAzm0nIMV7t7pWCyD/E6yMrlN05SD7w/xJykb9s\nZi8ipGzcANzj7uUEezObBjwH2AC8NzsuIaMXWFKpQEREGlvDBsciMmE2D7K9QPrdNPmWtnaQfZPt\ncyqUPVnpAHd/1MyOBc4DXgz8bSx63Mw+5+5firfnErKQdiekT4iIiJQprUJEJsKWeL3XIOULcvtl\nDTrNirvf6+5nAfOBowkzVzQB/2Vm/5ir83Z3t2qXYd0jERFpCA3bc+xxGbuWpswqc3GKtLb2ZHqz\n9LtBIVn2Ln4c7uhNpzy79trbAPjt724GYP3GTemJYh1dO8Iguv7MQL5kRbwmC9OomaXns6Z4omL6\nOb9+fUjDuO/++wE4/NBDymUnn3wyAD09OwDYtHFjuezuu/8CwPa2cJ65c2eVyxYt2jNeh7SPzmkd\naduzS/2JjCN332ZmK4H9zOxAd38wt8up8fq2EdZfAJYDy83sRuCPwCuAb7p7l5ndDTzbzOa5+8Zq\ndY3GoQtns1wLcIiITCnqORaRiXIx4evoZ82svB66me0GfDSzT03M7Fgz27NCUbJtR2bbF4A24GIz\n2yl1w8zmmtlRtZ5bREQaR8P2HBf6+8IfmcE2yZRnrXFKtVIp03PqA6+fWL2+XHT9jXcB8NSG0BNs\nzZ3lsqKHxTk2xYFy2UU9OjpCL21fHCCXHXxXKBQGnDbrySdDSuWG9evK27pi/c85/FAA1qxZUy7r\n7QttOO2vlgFw1JGHlcsWLgq/Ws+cFXqTMx3peKGvwtlFxs3ngNOBlwN3mtkVhHmOXw3sAXzG3a8f\nRn2vBd5pZtcCDwGbCHMiv5QwwO6CZEd3v9jMlgLvAFaaWTKbxjzCvMgnA98C3jaqeygiIlNOwwbH\nIjK5uXufmZ0GvI8Q2J5DukLee939h8Os8odAO3ACcBRhcZDVwKXA5919Re787zSzKwkB8AsJg/82\nEoLkzwLfG+FdExGRKaxhg+O+ntCbWiymvcPtMd22Keb7ZnuO++OyysW4OMc99z1SLlv56BMAbNoa\np0NtTXuji6XwdyEu9dzUnOmpjt3CyVRR2eWgd9oJaGlpju0L2S5treVfmrn9jjsAeOLx0K7Zs+aW\ny3q7w6/FixbtAcCxxxxeLmtqiZkz5WaldSbLTovUg7uvIvNMq1C+rMK2HsL0a5+sQ/1/JqycV7O4\nnPUvh9xRRER2Gco5FhERERGJFByLiIiIiEQN+7t6kjqRXfyqWAyLdnkppDIUCukiXv19IeVh+/aQ\njvHQQ0+Uy7Z0bQ3Hx+FzHW3p4PZSb9ifliRlIk2TSNI2mmIj2trSFel6e8MgvVJmOjUvhGNbWsK/\nxTIpEMngvra2MKJunzg1G8DKh8M0cuueDKv19fRsLZfNmBmndUsmA7Ds96GG/feLiIiIjIh6jkVE\nREREoobtOmxvDQt99GWnTyuF3t3NT4ee1iefTBfz2LQ5LJq1efM2AB5dtbpcVoqD9KbPCL2wHdNn\nlsv6doTjvC/sU+rPTM7WGrfFm9aUGQzXHL6XJNOwxR0AaG4N18kgQQCLi3UV+sL0a6VSOg1bT3e4\nP8tvuR2A449/brnsOYfvFpoSz11qTnuvadYCYCIiIiJZ6jkWEREREYkUHIuIiIiIRA2bVlGKKRSe\nGSDX2xO2rXl0AwAPPLiqXLa9O6QpdMfrteueLJcl6Q0z46A4Jx1E19wW0jcohLKmUprGkQzOa26O\nKQ2ldACgxbmMW1rSJeuSeZCT+Y1bW9I0jLZYB/G6r5Cep7sn1LvuyZAS8uCKB8tlBz/r2eHcM6eH\n85FpQ8X1+URERER2Xeo5FhERERGJGrbnuBina+vPLEq3cVNYSW7Vo2sBeOiRzHRtW0NZb2/oke3J\nDKxraQu9wq2xl7inP+05bmkOZR3T4/7FHeWypjgIrj0e392TDr4reXjoZ8+ene4fe6Q72kJZX186\n6K4lrnTX2h56mp/a8HRa1hEGCs6cvQCAW2+9r1zW+oyHADjheUcBsGdr2uNcKmZ7uUVEREREPcci\nIiIiIlHD9hw/9uh6ANZkpmvbsjX03G7cGhbg2Lol7Zlduzbs198fcnLnz927XFZKvkM0h17bjtb0\nYeuOZXOnTwNgXuy9BeiN08j194Ve5cceS3uq2+NiHvPmzitv64zbpneE6dY2bV5fLps5M/Q+H354\nyCG+/4GHy2WbNoWp3Cwu9PHwljRXuf/x0L5Zfwm9yc17dpTLZu+2GyIiIiKSUs+xiIiIiEik4FhE\nREREJGrYtIpNm8PAuJ7edEReMq1bZ0dIX5g/N13prj0OdOvpKQy4BujpC3Vs6wp1tran3yn232df\nAJYeHq4X7pUOsOvaEY67+Za7AXj8scfLZX09IaWja1tXuq01pDy0t4a2HPKs/ctlhxwc6j/i8EMA\nmNWRtqEjTgu3rTucr3f23HLZlg0bAbj0WzcAcM20dDDh6S9ZBsAZ+x6NyK7MzK4BTnF3LRspIrKL\nU8+xiMgYWbF6y0Q3QUREhqlhe473XbwXAHsX057SYin83dcXeoW370gHz+2Ii3/s2BGut29Pe5y3\nbA29u2vXro23t5bLjjvqoHB99IEAtDalx23bFv5+7LFw3AH7P6NctnZd6NHtzvZQ94Se6ZnTw5Rx\nz1y8T7nsyEPDedosDCY8/uhnp2WHhd7klatDnXesTxf6eGT5FQBsXBfuw2PNaftmTb8RgDPOfCsi\nIiIiop5jEZlizOxYM7vMzFabWa+ZrTWzq8zszMw+Z5vZT8zsYTPrNrOtZnaDmb0+V9diM3PglHjb\nM5drxveeiYjIZNCwPced7SG/OJneDKAYO1SLbWHb7Dj9WhCWV3ZCymF2fYy+mHO8Y0eY3m1bV5on\nPHt2yFu2UuhxLmVWZJ4+PUzJdsqJhwJw5BEHlMsefnQdAL/81dXlbY/GbRs2hfbd/cBj5bLDloRe\n59136wx1t6X/ullzw3l2XzAfgGduSXuObymFXu4HV22J9y89bv783RGZSszsLcBXgSLwv8CDwB7A\n0cA7gB/FXb8K3AP8EVgLzAdeAnzXzJ7l7h+N+20GzgfOBp4R/06sGsO7IiIik1TDBsci0ljM7BDg\nK8BW4CR3vztXvihz81B3X5krbwOuBD5kZl9z99Xuvhk4z8yWAc9w9/NG0K7lgxQdPNy6RERk4imt\nQkSmircTvtB/PB8YA7j7E5m/V1Yo7wO+HOt4wRi2U0REprCG7Tku9obV8JJBeACl+HdybbbzrE1N\ncVq01qb0e0NzW9i/ozWkL8yclk4BV4i5Gtu2hBX2MlkVtMUV72ZND1O07b7brHLZ4jjYriNO2wbw\n3e//MtTVEwbkPb56e7lsxX0hxWLZSWEgXktL2vbWuGKfezj7Pru1lcv2OOMEALZ0hXZ296Qt3H12\n+073X2QSe268vnKoHc1sX+CDhCB4X6Azt8vCejXK3ZcO0oblwFH1Oo+IiIyPhg2ORaThzInXq6vt\nZGb7ATcDc4HrgKuALYQ85cXAGwB9MxQRkYoaNjgulUJPqWd6jpO/S7G3N9tznPxtTRaPzyweEntk\nC/2FeL1zGUldma7j3jgtXKG7G4CWtnRwYOe0MADwec89tLztyafDoLmrfh9+MW5tTQcMliz0MFtz\nqKMtU1dbW+iZLsZRhB4HBwJMaw/tmhcXPOnonJ62vZA+NiJTwOZ4vRC4r8p+7yMMwHuju1+SLTCz\n1xCCYxERkYqUcywiU8VN8fr0IfZLpoX5SYWyUwY5pghg2elt6uDQhbOH3klERCYVBcciMlV8FSgA\nH40zVwyQma1iVbxelit/EfDmQep+Ol7vO+pWiojIlNawaRWtLaEDqL+QHSIXUycIZYVCOpmxNTfF\nbSFlor8/TU1IUjQK5bSFTJ2ezKcc6m7KDuSzpni+mJbR21su6+oPq+EV+tNtRx+5OJy7N7Rhzuz5\n5bLjjgmdYdOntQ+4f9lzNjfHgXj9aRtKHtseV9/b3tddLmtpadh/vzQgd7/HzN4BfA243cx+QZjn\neD5hnuNtwKmE6d7eCPzYzH5CyFE+FHgxYR7ksypU/3vg1cBPzewKoBt41N2/O7b3SkREJhtFRyIy\nZbj7/5jZCuCfCT3DrwA2AHcB34j73GVmpwL/SVj4owW4E/hbQt5ypeD4G4RFQP4O+Jd4zLXAaILj\nxffeey9Ll1aczEJERIZw7733QhhIPa6sPKBMRETqxsx6gWZCYC4yGSUL1VQb4CoykZ4DFN19XGcY\nUs+xiMjYWAGDz4MsMtGS1R31HJXJqsoKpGNKA/JERERERCIFxyIiIiIikYJjEREREZFIwbGIiIiI\nSKTgWEREREQk0lRuIiIiIiKReo5FRERERCIFxyIiIiIikYJjEREREZFIwbGIiIiISKTgWEREREQk\nUnAsIiIiIhIpOBYRERERiRQci4iIiIhECo5FRGpgZovM7GIzW2NmvWa2yswuMLO5w6xnXjxuVaxn\nTax30Vi1XXYN9XiOmtk1ZuZVLh1jeR+kcZnZq8zsQjO7zsy2xufT90ZYV13ejwfTUo9KREQamZnt\nD9wI7AH8ArgPOBZ4D/BiMzvR3Z+uoZ75sZ6DgD8AlwIHA28EzjCz49394bG5F9LI6vUczTh/kO2F\nUTVUdmX/BjwH6AKeILz3DdsYPNd3ouBYRGRoXyG8Eb/b3S9MNprZF4BzgU8Ab6uhnk8SAuMvuvv7\nMvW8G/iveJ4X17Hdsuuo13MUAHc/r94NlF3euYSg+CHgFODqEdZT1+d6JebuozleRKShmdl+wEpg\nFbC/u5cyZTOBtYABe7j79ir1TAfWAyVggbtvy5Q1xXMsjudQ77HUrF7P0bj/NcAp7m5j1mDZ5ZnZ\nMkJw/H13f/0wjqvbc70a5RyLiFT3/Hh9VfaNGCAGuDcA04DnDlHP8UAncEM2MI71lICr4s1TR91i\n2dXU6zlaZmZnmdmHzOx9Zna6mbXXr7kiI1b353olCo5FRKp7Vrx+YJDyB+P1QeNUj0jeWDy3LgU+\nBXweuAJ4zMxeNbLmidTNuLyPKjgWEaludrzeMkh5sn3OONUjklfP59YvgJcCiwi/dBxMCJLnAJeZ\n2emjaKfIaI3L+6gG5ImIjE6SmznaARz1qkckr+bnlrt/MbfpfuDDZrYGuJAwqPTK+jZPpG7q8j6q\nnmMRkeqSnojZg5TPyu031vWI5I3Hc+sbhGncjogDn0Qmwri8jyo4FhGp7v54PVgO24HxerAcuHrX\nI5I35s8td+8BkoGk00daj8gojcv7qIJjEZHqkrk4/ypOuVYWe9BOBLqBm4ao56a434n5nrdY71/l\nzidSq3o9RwdlZs8C5hIC5A0jrUdklMb8uQ4KjkVEqnL3lYRp1hYD78wVn0/oRftOdk5NMzvYzAas\n/uTuXcB34/7n5ep5V6z/N5rjWIarXs9RM9vPzBbm6zez3YBvxZuXurtWyZMxZWat8Tm6f3b7SJ7r\nIzq/FgEREamuwnKl9wLHEeYkfgA4IbtcqZk5QH4hhQrLR98MLAFeDjwV61k51vdHGk89nqNmdjYh\nt/hawkILG4F9gZcQcjxvBU5z981jf4+k0ZjZK4BXxJt7AS8CHgaui9s2uPs/x30XA48Aj7r74lw9\nw3quj6itCo5FRIZmZvsA/0FY3nk+YSWmnwPnu/vG3L4Vg+NYNg/4GOFDYgHwNGH0/7+7+xNjeR+k\nsY32OWpmhwHvB5YCexMGN20D7gZ+BPy3u/eN/T2RRmRm5xHe+wZTDoSrBcexvObn+ojaquBYRERE\nRCRQzrGIiIiISKTgWEREREQkUnA8CDNbZWZuZsuGedx58bhLxqZlYGbL4jlWjdU5RERERHZFCo5F\nRERERCIFx/W3gbCCy9qJboiIiIiIDE/LRDeg0bj7RcBFE90OERERERk+9RyLiIiIiEQKjmtgZvua\n2TfM7HEz6zGzR8zsc2Y2u8K+gw7Ii9vdzBab2RIz+3ass9/Mfp7bd3Y8xyPxnI+b2f+Y2aIxvKsi\nIiIiuzQFx0M7gLBk5j8CcwAnrOn9fuBWM1swgjpPinX+A2FJzgHr1Mc6b43nWBzPOQd4M3AbMGCt\ncRERERGpDwXHQ/scsAU4yd1nAtMJy75uIATO3x5BnV8BbgEOc/dZwDRCIJz4dqx7A/ByYHo898nA\nVuDzI7srIiIiIlKNguOhtQOnu/v1AO5ecvdfAGfG8tPM7HnDrPOpWOeKWKe7+0oAMzsJOC3ud6a7\n/6+7l+J+1xHWEe8Y1T0SERERkYoUHA/tR+7+UH6ju18N3BhvvmqYdV7k7t2DlCV13RTPkT/vQ8Bl\nwzyfiIiIiNRAwfHQrqlSdm28PmqYdf6pSllS17VV9qlWJiIiIiIjpOB4aKtrKNt9mHWur1KW1LWm\nhvOKiIiISB0pOB4dG+FxxQk6r4iIiIhUoeB4aHtXKUumcavWEzxcSV21nFdERERE6kjB8dBOqaHs\ntjqeL6nr5BrOKyIiIiJ1pOB4aGeZ2X75jWZ2MnBivPnjOp4vqev4eI78efcDzqrj+UREREQkUnA8\ntD7gSjM7AcDMmszspcDlsfy37n5DvU4W51P+bbx5uZn9tZk1xXOfCPwa6K3X+UREREQkpeB4aP8M\nzAVuMLNtQBfwv4RZJR4C3jAG53xDrHt34P+Arnju6wnLSL+/yrEiIiIiMkIKjof2EHA0cDFhGelm\nYBVhCeej3X1tvU8Y6zwG+ALwaDznFuCbhHmQV9b7nCIiIiIC5u4T3QYRERERkUlBPcciIiIiIpGC\nYxERERGRSMGxiIiIiEik4FhEREREJFJwLCIiIiISKTgWEREREYkUHIuIiIiIRAqORUREREQiBcci\nIiIiIlHLRDdARKQRmdkjwCzCcvMiIjJ8i4Gt7v7M8TxpwwbHt/zqPgfoL+wob2tqKQLQ0RHvdktz\nuay7uxeA7Vu6AChkOtVXb3oagJWrHwdgw+anymUbNz8JwJan1wOwYM4e5bJTTzgNgAMPODRsKBbL\nZW2xCdPndJa39VMAwAoGQKul7esv9gHQUwrt3Ni9rVz21LZNAGzr7gagc9qMctn0WbNCmzdsDPt0\ndZXLZs2cCcA7X/MKQ0TqbVZnZ+e8JUuWzJvohoiITEX33nsv3TG2GU8NGxwnwWeTpXextbUj/OEl\nALZv6SmXbe8KQfTmzVsA6Lc0kG2KQWtyXSgWymXNraH+5vZWANZnAucHV90LwIIFiwDYY878cllH\na/COh+YAACAASURBVNjfimkQ3hLb2tQctpX6+8tlpRgwt7dPA2CGpcdt7Qv3Y1tPuC5mgvBCIbS1\nozME4bNmzymXzcn8LSJ1t2rJkiXzli9fPtHtEBGZkpYuXcptt922arzPq5xjEZk0zGyxmbmZXVLj\n/mfH/c+uYxuWxTrPq1edIiIydSg4FhERERGJGjatwswBaG9tK2/r6AhpFaVSSDvoLqRpC03FkN/b\n0hzSHbp6tqdlHSGlYd7ckMu7dlOaOtHcEuqfFvN3C83pcU9uWBP2fyrkKi/aa8+0La3tAPT0pika\nTU3Nse3hdqEvLSsWYqpE/DpTKpbKZaVSuK9eCtvc07KenpCjHHdh5ow0H3nWjOmITHE/A24C1k50\nQypZsXoLiz/0q4luhojIhFj16TMmugkj0rDBsYg0PnffAmyZ6HaIiEjjaNjguNgfelqtNZ3xwT10\nn5rtPBtEMhiurbMjlrWXyzZvDzM9zJwdel07p6VlW3bEWSOaw7aOma3lsu44GO7R1Y8AsGjBXuWy\nvXfbO7SptPNEEcXYo93bnQ4YxEJvcLEnlO3oS3uo+3p74/0K3cpNzdlZOMJAw2Ix3PemPXYvl7W3\npfuJTDZmdjDwaeBkoB24HfgPd78qs8/ZwLeAN7r7JZntq+KfhwPnAX8LLAQ+4e7nxX32BD4J/DVh\nyrX7gS8Cj47ZnRIRkUmvYYNjEZnSngn8CVgB/DewADgLuNLMXuvul9VQRxvwB2AecBWwFXgEwMzm\nAzcC+wHXx8sC4Gtx35qZ2WDTURw8nHpERGRyaNzguBh6ZEuk05r1EXpRW1pDWVtbZjxid+hZ7Y69\nsG2ZfNyup0M6o3notU3mHAbYHudHbmqOuc0tac9xU2uo87H1qwHwW64vlx1+0LMB2HuvheVtrW2h\n17p/RzhPdiq3tjhVnMdc6kJ/2quc5BhPnz5tQD0AXdvD/IB9MWe5lGm7ey8ik9TJwOfc/QPJBjO7\niBAwf83MrnT3rUPUsQC4BzjF3bfnyj5FCIwvcPdzK5xDRER2UZqtQkQmoy3Af2Q3uPutwPeBOcDf\n1FjP+/OBsZm1Aq8DthFSLiqdo2buvrTSBbhvOPWIiMjkoOBYRCaj29x9W4Xt18TrI2uoowe4q8L2\ng4FpwB1xQN9g5xARkV1Qw6ZVNFlMb/A0jaBYCH83NcUBeW3ZFIiwbcNTmwFoJh10V4gD3Z54dBUA\n9628v1zmTWGQXkccyNe9PT3f7Jkh1aK3L3zGb9u8uVzW1xPSHY4g3b+1Ofw7tm0IHV0L5qdTv82Z\nsyDch2Tlv+2eHheXwW6ZFtIqiqW0LFnNz8or6qXTvPX2VIo9RCaFdYNsfzJez66hjqc8GYU7UHLs\nUOcQEZFdkHqORWQy2nOQ7cmUL7VM31YpMM4eO9Q5RERkF9SwPcctLXFAnafTlRXjtGnu8bo9nUZt\nm4XBeg+sDrM4behKP3s3b1oPwJbNYfGPlY88WC5rbQ89x3Pmhc/ZQiHtmd22NfQcz5geeqELPekg\nuv7+lQBMm5b+C7ZtjPU/8BgAB+xzQLns5BNOAmDW/HkA7Ni+o1zWPCP0gLe0hu86vZky4oDEGXHx\nj+nTOsslpaIG5MmkdZSZzayQWrEsXt8+irrvA3YAR5jZ7AqpFct2PmRkDl04m+VTdBJ8EZFdlXqO\nRWQymg38e3aDmR1NGEi3hbAy3oi4ez9h0N1McgPyMucQEZFdVMP2HIvIlPZH4M1mdhxwA+k8x03A\nW2uYxm0oHwZeALw3BsTJPMdnAVcALxtl/SIiMkU1bHDc3BpSGrw/neeYppCCmMwVvHpTOh7n+jtu\nBmD5ijsAePD/s3fncZZdZb3/P885NVfPYzrpdCpzQgIkBEMgYBLUhIgIV+EHKMigP0RQJr1XRklE\nhntVBrmAKGJkMqCoUQSJQhImEUlIINCZOql0eh5r6hrPOc/941ln792nq6qru6u6uk99369XOFV7\n7b322tWHqlVPPc9aj2zK2kZHopCutRR9ued9DvTvT6+RelHNsyqopfSN7u6o/6kX3AFsSzveTYzm\nRXrDaTybHom0ivvuywvtN/dGEeC60zcAsPz0fH3ksy99fDwzaX3jwvi6U6HgmlUrAVi6eHHWdmBA\naRVywnoEeDWxQ96riR3y7iJ2yPvqsXbu7nvM7Epih7znAE8mdsj7LaAXTY5FRBaspp0ci8jJx917\ngeKe6s89zPk3ATdNcrxnBvfaAbxyiuZD93UXEZEFoXknxykyW2rJ06otPe5wJQrW7vnJxqztG//5\nXQAe3bIFgLZyJWtbviKireVyRGTLbYUiuqHoq74DnRfSuPv74y+/BwYiqmyFyLHXamlMeZGeVWN5\nt2pHFBEOVfO2nzz6EwB2DMZufWdU82K99Rf0ALB02XIAutvzHfK6OqIwce3KVTF2y3/mj40pciwi\nIiJSpII8EREREZGkaSPHtdoEAG2FyHHNUkR2KCK02/bszdq27Ypl1DpT1HbNyjw3d1lXHCulr9bA\nSL5UWiktlXZgNDbzmKjk+b5Ll0QEdzxFaEut+aYj4+m0lu58aTVqcZ/WFPm18Tx63dIRx8ppKbaa\nFTYBKcUzdqQ86/a2PHLc2h7nd6Xr+/p2Z219hU1JRERERESRYxERERGRjCbHIiIiIiJJ06ZVuEdK\nghcfsRy/Cwylneoe2fJY1lRN55+xLgrXLjh7XdZ29hmnAmDlKGbbtXdf1rZ1exTI7e+LjbyGC0Vu\nwwcOANA/EOkLo5W8GG5RZ+xYt3TZknzMtTTmwUjRsDwLg+4lkeaxaElct+GM87O2pctimba2tkir\nqBWK7lpaIlVjZCRSSfbs2ZO19ffPZAdeERERkYVDkWMRERERkaRpI8f1grxKLS9cGxuL5dP29kck\nd8/evDitpRRtp6+LKOwVl1+ata1ZE8e6uyNqa6Vy1tb7aGzYsa8vbdhleVs9YvzgQw8A8MjWHVlb\nNW0Q0t6an99WjlBxiYgAW6GvZStWAHDKmrUArFvfk7XVi+6qKWDshRVaKx7PNTA4BMD+QhGelnIT\nEREROZgixyIiIiIiSdNGjivVyNut5xIDjNRi/bR9fbHl8+Dg/qytvSUizGf1xLbMG04/NWtbvno1\nAMuWrkjntuXXtXUBsHd/RGQ7uxZlbeMTMYa1p5wCQMcP7snadqal48rk+023EGO1tP3zWLoeYMnS\nuOdZ5/UAsHh5nqs8Wo0oeW0scqlb2vLxedroa0/9mYcP5GMvaxMwERERkSJFjkVEREREEk2ORURE\nRESSpk2rKNUiXaFSmciOVVNahdd3z8vr3ejqjKK2ng2RTrH+1PVZ28o18XFbe6RM1Cp5qkZXVyzh\ntr8vlkWbmMiL3EoWv3t0d8cybGesz/v0lDLRanlaRUdaum3z9l4AhobyHfx274i0j82LIp2io5yn\nTpRa2wFY2havraX8n3VwOFItdu6JtIrRkfzr0dJdWCtORERERBQ5FpHZYWY9ZuZmdtN8j0VERORo\nNW3k2DwirdVi5Njj4+HhWHbNa3nB2/JlERVengrdli5dlrV1dSwFoFzuAKBWzqO9pIK3SiUixuOp\n72iK0PRYKoJb1JFHe884LZZkq4wOZ8dWr1wOwIYzI8L8cO8jWdv27RFFvvfu7wHw47vvyvs6JzYE\nufjSJwFw6ulnZG0Dw/GM/f1xfckLS9vljy8iIiIiKHIsIiIiIpLR5FhEREREJGnatIqRSuQMVArr\nHNfTIcY95ROU8xSDzs4oZhtPuQZ9+/Od5LoXRQpEe3ukSQz092dtBwb2AXlx3/DYSNZWbomCt0Wd\n8WX2ifzL3dUWaRsTY+3Zscdf9HgA1q0/BwAr57+7fP22rwDw7//xLwBs2741a7vr+5F+8aMf/Wfc\nb+nKrG3DWecBsHbdhniGju6sbWI8/9qIzCYz6wHeB/wssAi4F7jB3b/UcF478EbgV4BzgApwD/Bh\nd//CJH0+AvwN8B7gXcA1wCrgme5+u5mdBbwZeCZwGjACbAW+DbzN3fc29Pli4FXAJUBn6v+zwB+7\nu7aQFBFZgJp2ciwi8+YM4HvAw8CngRXAC4FbzOxn3f02ADNrA74KXAXcB3wE6AKeD3zezC5x97dO\n0v/ZwH8BDxAT2U5gwMzWAf8NLAG+DHwR6ADOBF4K/F8gmxyb2V8BrwS2AP8A9AFXEJPunzGzn3N3\n/QYpIrLANO3keDz9TCu357vAuUWkeMxjebPBA/kOeaVy7H7X1h5Fd/v27cvarLQFgO6uiCZv39Kb\ntQ327QagUo0+qRYKAKtp6bi0811b4as9Op52tSucf/8D90fbRGu6bz72Hdu3p48i+n3FFU/O2tad\ncmq6X4Svt+7In2vbzhjzYHtEjBedmkeO60WEIrPsaiJKfGP9gJl9Dvg34H8Ct6XDv0tMjL8C/GJ9\nImpmNxKT67eY2Zfc/TsN/T8deG/jxNnMfoeYiL/B3T/U0NYN+XaUZvZyYmL8j8CvuvtIoe0G4J3A\na4GD+pmMmd05RdMFh7tWREROPMo5FpHZ9ijwR8UD7v5VYDNweeHwKwEH3lSM0Lr7LiJ6C/Abk/S/\nE7hxkuN1I40H3P1AcQIMvJ5I4Xhlw3HSvfcCvzrNPUREpEk1beS4szsiwF6uZsf29kc0+LGtkaN7\nYDjPHV6yOCKqo6MRTd25c3fWNl6JiGw5RXKH+vZkbS2tEY3u6oj7mee/b0xU4t6Wcpsn2vKl3Oq/\nl5TbOrIj5XL8c2zbvRmASjVfMu6scy8E4KeuuBKAVatWZW3d6d6Dg7EsXO9jO7K2LWkJuD37h9Jd\n83/ysfFRRObA3e5eneT4Y8BTAcxsMZFjvNXd75vk3K+n10snabtninzgfyZykT9iZtcRKRvfBn7i\nnq9haGZdwBOBPcAbzGySrhgDLpysoZG7XzbZ8RRRftJM+hARkRNH006ORWTe9E1xvEL+16ql6XX7\nFOfWjy+bpG3HJMdw90fN7HLgBuBZwC+lpsfM7E/c/c/S58uJBcpXE+kTIiIiGaVViMh8qP/Z5pQp\n2tc1nFfkkxyLBveN7v5CYCXwZGLlihLwITP79YY+f+DuNt1/R/REIiLSFJo2ctzd3QVA/1C+ctMP\n74ld5e6++78BaGvLfzew9OED9z8QbS15CsT6M+IvuCtXRNHekuVLs7YlS+M+1bR0XPtIXmDnKcWi\nNVXi1ch/1tbSz91ya36fNWtjybhSW/36/PyWcle6MPqqVgs/t2up+LAtduI7q31tPr7VgwDc/9DD\nAIwVduSrDBySmilyXLj7oJltAs4ys3Pd/cGGU65Jr3dxFFIO853AnWb2HeAbwPOAv3L3ITP7MXCR\nma1w933T9SUiIguLIsciMl8+SaQ3/LFZ2msdMLNVwDsK58yImV1uZmsnaaofGy4cez/QBnzSzA5J\n3TCz5WamfGERkQWoaSPH9QfbtX1bduyu7/8XAP37o6DurAtOz9oqaUm1vXsj0rxkUR4d7uuLwNLq\nNVEEt2zliqxteDgis7t27QJgaCgvchsfi4K6UjmWZutatChr61ocP487uvJ/gn2DqYapFL+zeC0v\nyDswHAWCB4bi5/v4eB6hHhuJe9Yj1B2L8uXa9h0YAKDcHW1Lu/N5QHs2HRGZF38CXA88F7jHzL5M\nrHP8AmAN8H/c/VtH0N+vAK81szuAh4D9xJrIzyEK7D5YP9HdP2lmlwGvATaZWX01jRXEusg/Dfw1\n8OpjekIRETnpNO3kWERObO4+bmY/B7yJmNj+DvkOeW9w9789wi7/FmgHnkasEtFJ7I53M/Cn7n5v\nw/1fa2ZfISbAP0sU/+0jJsl/DHzmKB9NREROYk07OR5PS7Jt25JHjrdujiXSujoikrtsyeKsrZRK\nfErlCKe2tee5wKUUyW1tidddO/IC+4c2Rark9u1RQF+t5bnAnZ1LAFi77jQAlrbnEd2qx336BvJI\n8849W9LYI4f4wIEDWVt/2qZ6f39EkHfvzp+rrSXykc/oORsAb81DwuX2eNbuFE1e0tGVta3uzp9f\n5Fi5ey8wZRGbu189ybFRYvm198xC//9F7Jw3Y2k76y8d9kQREVkwlHMsIiIiIpJociwiIiIikjRt\nWkV1opZe8426RkaimK21M3IoOtvyxx8dimXNOls7AehenBfPnXbqegAmRiMF4oGNP8ratmyN9Ibh\nlMZRLiwBt6Qnit+WpwI+LyzP2t8XhXKDB/KNvqq1tJPeSIxl86O9WVvvo7Gr38h4XNfanvfVc9o5\nAKxOS83tHx7K2sbH4vk9sito7cxTLlYsWYKIiIiI5BQ5FhERERFJmjdynDbJaGtrz44tSkVpQ0S0\ndmw0L4ab8CiCW7ZsOQBdXXnh2sRELJu25dHHANi2bVuhLa4rleJLuXx5vszbmjVrABjojw25Rscq\nWVulFhHcSh7YZmQ0NhLZuyf6//5deW3RgbRc2xlnxvJzi5d2ZG0trREWbm2JPtsLxYTDKUI9NhxR\n84lCm7flS8WJiIiIiCLHIiIiIiIZTY5FRERERJKmTasYToVutULaQltbpBTUUpHawOBA1rZqWaRA\ndHZGQV5lIt+B7pHeKIbr3x+752H57xStKW2jqyOuO/2MM7K28ZRyUb+PlVqztpG0lnGhXpCh4VjX\n+Ic/+j4Aj217NGt7/BOfDMAFFz8hnm80L7qrpmLAwaFI3xir5ukbE+ORjjHukWYy1pqPYaiWp1iI\niIiIiCLHIiIiIiKZpo0c96ciuPHxPALc1RnLsw3vqxfk5cuoLU5Lt9Wjy4NDeWR2LBXK1Tfnam3L\ni+EsFeKtSsV3wyN5nwNDEQlubYnzJybGs7a+gUEAKoWauB27Ype9H997DwBLl6/M2h530eMAWLc+\nlpXbu39f1jaYrusbiMj2gfH8PpUUJe9eHOOcKBQhjtSGEREREZGcIsciIiIiIknTRo5LadpfXMpt\nyZJYZs22RnR4bCxP+G1vj+jueIq67tu7N2traYk+6kultbTkX7bFS5cCUG6Nc3bs2l24Lu5TGY9N\nPfr68hzn0fEUvS5Ekzf++Idx7EBEdE8598Ks7cyUy7x8dUSoK7U8Il490Bd9jkakui9FzQFaSzGG\n0qLFccDzUHVLS74hiIiIiIgociwiIiIiktHkWEREREQkadq0iq7uSCdYvWpVduycsy8A4JFdvQCM\njPdlbYNpubVKNdIOuju7s7a2VIDXmpZBW7Q43z2vVI4v4b79+wGwUv77Rn33vJHBSKs4UCjyG69G\nOsX2HduzY1sf2wzAyiWxS9/Zp5+ZtS3rXgLAku649/6OfBm2gda452DaBW+osETdkq5Ip7C0pl1r\na55KsWjZIkRONmbWC+DuPfM7EhERaUaKHIuIiIiIJE0bOS63xrJrLR35I7Z1RDS4uzsK84b35RHW\ngcGI6tY39Whpy69rbWtNr/VorWVt9SK7ibSbR7klj+iOpWXTxlKRX7WwI8noSER5H9u8OTvWkqLO\nZ5zWA8D6U9bnY08R6vGRiEJPjI1kbdUUhR5Om4hMjOXLyVlHRIe9FlHs9rZ8E5Cly5YiIiIiIjlF\njkVEREREkqaNHJdaYt5vbXmObak95QqnpdgK+29kWz3XiOiuk0d568dGxyISPFHYWroyHtdVU65y\nrTp+SFvauZnxwnXbt8fGHf379mfH1q1eC8DjL7oUgDM2nJWPoRpjGNwXS8wN9edR77GUazyaNh0p\nVTxra61vWe1xrK2wfXR32ipb5ERjZga8Fvgt4GxgL/CPwNumOL8deCPwK8A5QAW4B/iwu39hiv5f\nB/wmcFZD//eAcppFRBaqpp0ci8hJ7YPE5HU78BfABPBc4ClAG5D9FmpmbcBXgauA+4CPAF3A84HP\nm9kl7v7Whv4/Qky8t6X+x4FfBC4HWtP9RERkAdLkWEROKGb2NGJivAm43N33peNvA24D1gGPFi75\nXWJi/BXgF929ks6/Efge8BYz+5K7fycdfwYxMX4AeIq796XjbwX+Azi1of/DjffOKZoumGkfIiJy\n4mjayXGpFLkM5cLSZaVypFrUd7grLrtGlvoQxWyVaiF1ohrpEdnZedZC9kk59VVfvg1gIhXijaeU\niN27893zHtv8GACt5TzNYf1ppwOwak3sgtfRnS8nVy/mGxoaBGA47aIHMJZ226uk3fbaWvJdATtS\ngWG5HF+Hzo6OrK2YYiFyAnlFen13fWIM4O6jZvYWYoJc9Eri/4hvqk+M0/m7zOxdwCeA3wC+k5pe\nVui/r3D+eOr/W7P6NCIiclJp2smxiJy0npRe75ik7ZtEPjEAZraYyDHe6u73TXL+19PrpYVj9Y8n\nmwR/t9j/TLj7ZZMdTxHlJ03WJiIiJ66mnRy7R4FcW2FJtolKRFb7+qMIzgtLq5VSBHhooB+AxYWo\nbXtrFK511peFs7yUryNtxuGp6m5i/MAh99u5cxcA99+X/+z2SvRx3jnnZ8fOOyf+Crts5cq4b2ce\n5SVt9FG/c7Waj70eoa5U4tjirnw5uXqUPIscd+V9FjcEETmB1NcY3NnY4O5VM9s7ybnbG89tOL7s\nKPsXEZEFRku5iciJpj+9rm1sMLMysHKSc0+Zoq91DecB1Jd6mUn/IiKywGhyLCInmrvS61WTtD2D\nwl+83H2QKNw7zczOneT8axr6BPhBen36JOdfQRP/RU1ERA6vaX8ItKSUgRbLUwfqaQRjab3ilpb8\nd4O21vhSjI7GTnn14jaAWiWK8+qpGu3t+frAlir5xuprGheq9Srpum1btwAwNDCYtW1YvwGAx1/0\nxOzYky79KQCWrlkFwLKVK/K+PK1zPBp97Nu/J2vL1ljOUknytIqOVIDXmdY0Xrx4cdbW3pEX7omc\nQG4iCujeZma3FFar6ADeO8n5nwTeDfyxmf2ye/yfxcxWAe8onFP3KaKIr95/fzq/DXjPHDyPiIic\nRJp2ciwiJyd3/7aZfRj4HeBeM/t78nWO93NofvGfANen9nvM7MvEOscvANYA/8fdv1Xo/w4z+wvg\nVcCPzeyLqf/nEOkX2zh4j6Cj1bNx40Yuu2zSej0RETmMjRs3AvQc7/uaux/+LBGR46iwQ95rOXgH\nu7cyyQ52Kar8JmKHvLPJd8j7iLv/7ST9l4DXEzvkndnQ/xZgk7tfcozPMAaU6+MVmQf1tbYnW8lF\n5Hg41vdgDzDg7mfOznBmRpNjEZEk5S0/ANzs7i8+xr7uhKmXehOZa3oPynw7Wd+DKsgTkQXHzE5J\n0ePisS5i22qIKLKIiCxAyjkWkYXoDcCLzex2Iof5FOBngPXENtR/N39DExGR+aTJsYgsRP8OPBG4\nFlhB5Cg/APwZ8EFXvpmIyIKlybGILDju/jXga/M9DhEROfEo51hEREREJNFqFSIiIiIiiSLHIiIi\nIiKJJsciIiIiIokmxyIiIiIiiSbHIiIiIiKJJsciIiIiIokmxyIiIiIiiSbHIiIiIiKJJsciIiIi\nIokmxyIiM2Bm683sk2a2zczGzKzXzD5oZsuPsJ8V6bre1M+21O/6uRq7NIfZeA+a2e1m5tP81zGX\nzyAnLzN7vpl92My+aWYD6f3ymaPsa1a+n86VlvkegIjIic7Mzga+A6wBbgHuAy4HXg88y8yudPe9\nM+hnZernPODrwM3ABcArgGeb2VPd/eG5eQo5mc3We7DgximOV45poNLM3g48ERgCthDfu47YHLyX\nZ50mxyIih/dR4hv569z9w/WDZvZ+4I3Au4FXz6Cf9xAT4w+4+5sK/bwO+FC6z7NmcdzSPGbrPQiA\nu98w2wOUpvdGYlL8EHAVcNtR9jOr7+W5YO4+n/cXETmhmdlZwCagFzjb3WuFtsXAdsCANe5+YJp+\nuoHdQA1Y5+6DhbZSukdPuoeix5KZrfdgOv924Cp3tzkbsDQ9M7uamBx/1t1fcgTXzdp7eS4p51hE\nZHrPTK+3Fr+RA6QJ7reBLuCKw/TzVKAT+HZxYpz6qQG3pk+vOeYRS7OZrfdgxsxeaGZvNrM3mdn1\nZtY+e8MVmdKsv5fngibHIiLTOz+9PjBF+4Pp9bzj1I8sPHPx3rkZeC/wp8CXgc1m9vyjG57IjJ0U\n3wc1ORYRmd7S9No/RXv9+LLj1I8sPLP53rkFeA6wnvhLxgXEJHkZ8Hkzu/4YxilyOCfF90EV5ImI\nHJt67uaxFnDMVj+y8Mz4vePuH2g4dD/wVjPbBnyYKBr9yuwOT2TGTojvg4oci4hMrx7JWDpF+5KG\n8+a6H1l4jsd75xPEMm6XpMIokblwUnwf1ORYRGR696fXqXLgzk2vU+XQzXY/svDM+XvH3UeBeqFo\n99H2I3IYJ8X3QU2ORUSmV1/L89q05FomRdiuBEaA7x6mn++m865sjMylfq9tuJ9I3Wy9B6dkZucD\ny4kJ8p6j7UfkMOb8vTwbNDkWEZmGu28illnrAV7b0HwjEWX7VHFNTjO7wMwO2j3K3YeAT6fzb2jo\n57dT/1/VGsfSaLbeg2Z2lpmd1ti/ma0C/jp9erO7a5c8OSZm1preg2cXjx/Ne3k+aBMQEZHDmGS7\n043AU4g1iR8Anlbc7tTMHKBxo4VJto/+HnAh8FxgV+pn01w/j5x8ZuM9aGYvJ3KL7yA2YtgHbAB+\nnsgB/T7wc+7eN/dPJCcbM3se8Lz06SnAdcDDwDfTsT3u/nvp3B7gEeBRd+9p6OeI3svzQZNjEZEZ\nMLPTgT8ktndeSezk9E/Aje6+r+HcSSfHqW0F8E7ih8w6YC+xOsAfuPuWuXwGObkd63vQzB4P/C5w\nGXAqUfw0CPwY+ALwcXcfn/snkZORmd1AfO+aSjYRnm5ynNpn/F6eD5oci4iIiIgkyjkWEREREUk0\nORYRERERSTQ5FhERERFJNDk+Rmb2cjNzM7v9KK7tSdcq8VtERETkBKDJsYiIiIhI0jLfA1jgJsi3\nUhQRERGReabJ8Txy963ABYc9UURERESOC6VViIiIiIgkmhxPwszazOz1ZvYdM+szswkz22lmBu+j\nWQAAIABJREFU95jZR8zsqdNc+xwzuy1dN2Rm3zWzF09x7pQFeWZ2U2q7wcw6zOxGM7vPzEbMbJeZ\n/a2ZnTebzy0iIiKy0CmtooGZtQC3AlelQw70E9sbrgGekD7+z0mufQexHWKN2JKzm9gv/HNmttbd\nP3gUQ2oHbgOuAMaBUWA18CLgF83senf/xlH0KyIiIiINFDk+1K8QE+Nh4KVAl7svJyapZwC/Ddwz\nyXVPJPYcfwew0t2XAacAf5/a32tmK45iPL9FTMhfBixy96XApcBdQBfwBTNbfhT9ioiIiEgDTY4P\ndUV6/ZS7f8bdRwHcverum939I+7+3kmuWwa8093/yN370jU7iQn2bqAD+IWjGM9S4FXu/il3n0j9\n3g1cB+wF1gKvPYp+RURERKSBJseHGkiv647wulHgkLSJNLn+avr04qMYz6PA5ybpdw/w8fTp84+i\nXxERERFpoMnxob6SXp9rZv9sZr9kZitncN1P3P3AFG1b0+vRpD/c4e5T7aB3R3q92MzajqJvERER\nESnQ5LiBu98B/AFQAZ4DfBHYY2YbzexPzOzcKS4dnKbb0fTaehRD2jqDtjJHN/EWERERkQJNjifh\n7u8CzgPeQqREDBCbdfwu8BMz+7V5HF6RzfcARERERJqJJsdTcPdH3P197v4sYAVwDfANYvm7j5rZ\nmuM0lFOnaavnRVeB/cdhLCIiIiJNTZPjGUgrVdxOrDYxQaxf/OTjdPurZtB2r7uPH4/BiIiIiDQz\nTY4bHKawbZyI0kKse3w89Ey2w15aM/lV6dO/O05jEREREWlqmhwf6lNm9tdmdp2ZLa4fNLMe4G+I\n9YpHgG8ep/H0A39pZi9Ju/dhZk8gcqFXA7uAjx6nsYiIiIg0NW0ffagO4IXAywE3s36gjdiNDiJy\n/JtpneHj4WPA1cCngU+Y2RiwJLUNAy9wd+Ubi4iIiMwCRY4P9WbgfwH/BjxMTIzLwCbgr4Enufun\nj+N4xohiwD8kNgRpI3bcuzmN5RvHcSwiIiIiTc2m3l9C5pOZ3QS8DLjR3W+Y39GIiIiILAyKHIuI\niIiIJJoci4iIiIgkmhyLiIiIiCSaHIuIiIiIJCrIExERERFJFDkWEREREUk0ORYRERERSTQ5FhER\nERFJNDkWEREREUla5nsAIiLNyMweAZYAvfM8FBGRk1UPMODuZx7Pmzbt5Pjmt5/tACXLH7GlHB+3\ntJcBqFHN2rwWq3YY8VpcxKO+oke2skfJsrZqJY61luPYrRtXZW27du4C4AVXx31bC3H6cr1/r+X3\noXbQfaqVfHxWv6fXL8uvo2HBkclWIKlWq1Mee8kfb7NDGkXkWC3p7OxcceGFF66Y74GIiJyMNm7c\nyMjIyHG/b9NOjkWkuZjZ7cBV7j7jX+bMzIE73P3quRrXNHovvPDCFXfeeec83FpE5OR32WWXcddd\nd/Ue7/s27eS4rS1+fpZL+SPWo8jlFIUtRlhrpRStrdUjrHlbKZ1fsgj91qPMAC0t9XO6AOhZlv+G\nc/bKNgC6WiPKWyP/mV6u/3wv9IWVD3qG4gzA7OAxF6cHxfE0Xlk/vxhobnwuEREREQlNOzkWEQEu\nBIbn6+b3bu2n583/Ol+3FxGZV73ve/Z8D+GoaHIsIk3L3e+b7zGIiMjJpWknx62p+K5UzlMVLHtt\nKLArtJXSR26FFIhypFOU6qkNVshRsOijpa0bgKdc3JE1jQ3uBKDqMZZaqVhE52l8+T9BOfVfq7eV\nChV8aai1lPZRLK+rj6eeXlE7KM2i3pel5zw05UJkvpnZLwKvBx4HrAD2Ag8Cn3f3jzac2wL8L+AV\nwAZgF/A54B3uPt5w7iE5x2Z2A/BO4BrgDOANwAXAIPAl4K3uvmPWH1JERE4KWudYROaVmb0KuIWY\nGP8L8KfAl4FOYgLc6HPA7wDfBD4GjBCT5Y8f4a3fCPw5cA/wQeD+dL/vmNnqI34QERFpCk0bOa5X\nrB1UcpaivNkqapNcdlC0tn5Z6iWPyBYizinaW6+lO6iOvhQHW1PkuVZorNYr5ApLuVUbqubKhah3\n/Zb16LUVItv1aLKnosJigLq+XFtWVFh8PlNBnpwQfhMYB57o7ruKDWa2apLzzwYucvd96Zy3ERPc\nXzOztxxB1Pd64Cnu/oPC/T5ARJLfB/z6TDoxs6mWo7hghuMQEZETiCLHInIiqAATjQfdfc8k5/5+\nfWKczjkAfJb4fvbkI7jnp4sT4+QGoB/4FTNrP4K+RESkSTRt5DjLLy5GRxsCpQcFTg9ZDW2SqKof\nGjn2hlzgWrWwqUeKCtcDzsVb1MdVPL9SqaTr4sxi5Lhcj/hOsnlIFjnOItvF33lSHnL9/EI+cs0n\nWd9N5Pj7LJFK8WMz+zxwB/Btd989xfnfn+TYY+l1+RHc947GA+7eb2Z3A1cRK13cfbhO3P2yyY6n\niPKTjmA8IiJyAlDkWETmlbu/H3gZsBl4HfCPwE4zu83MDokEu3vfJN1U0mt5krap7JzieD0tY+kR\n9CUiIk1Ck2MRmXfu/il3vwJYCTwb+Cvgp4GvmtmaObrt2imOn5Je++foviIicgJr2rSKaiWlLxZ2\ngasnFNgkO93VUy48SzUoLqNWOvj6UiE4VY22bGe9iXwlqWo95yKlTljhdvlOd3lfpZY4VqlOHHw9\nYPXx1JeTKzxXffm5Wr3g8OCqwPRaf65CSkjpSIJsInMvRYW/DHzZ4v+orwSeAXxxDm53FfCp4gEz\nWwpcAowCG4/1BheftpQ7T9JF8EVEFipFjkVkXpnZs9LaxY3qEeO52uHupWZ2acOxG4h0ir9197E5\nuq+IiJzAmjZyvPaMqIMpt7bmB1PUtSUVunmhIC0rkKsXz3mx6C47GH0WivVK6Wd6uT3uUy7lBe61\nymi99/jfcv67SL3LcmFpNcs2F0lRaC9ElUutB53vxc086h9mm4fkj1xLS7mll2I9Hl49aL8Ekfly\nMzBqZt8Ceon/wzwD+CngTuA/5ui+XwG+bWZfALYDT0//9QJvnqN7iojICU6RYxGZb28G/pNY2eE1\nxEYcrcDvA9e4+yFLvM2SD6T7XUK+S95NwNMa11sWEZGFo2kjxxc84WoASoW82nrEuJxFjovLmtWL\n3eubc1Boi9f6ttOlQuS4/tfgepC3pdRRuHD84PsUI86l0iHH6hHjWoroeu3QnODsssKvNVVPS7ml\n3OaWQsF+/d59E3Gf9sLW16UJRY5l/rn7nxM71R3uvKunabuJmNg2Hp92p5uprhMRkYVLkWMRERER\nkUSTYxERERGRpGnTKsptbcDBKRAtLfG4JQ7NTajU4uP62aVS4UtT3/2uXvBWLuY0RFs9KbJWuF85\nFdGR0h6sWHxXH0OpWBQY53u21NwkD1ZPEymkfZRqI6n/VPhXy8cwkTqppPu1F/7IXCzOExERERFF\njkVkgXH3G9zd3P32+R6LiIiceJo2clytRoGdteSPWKnFsdYUFfZC+LWWYsbldI4XCtdqWYi1nD6v\nZG1U0vkpolvz0aypnGqBrC2K9GrlvFCuHsAtU9zoI/o6KGqd5EV96fkqeTFdOf2OUyqnTUS8ML7U\n1lVK51fzwv8q09YqiYiIiCw4ihyLiIiIiCRNGzn2SsrzLUSOa7WIBnvaZrlaLST1pjzfWjpWPxfy\n/OD68mu1QrJufUOQbGm22qGJwp7OKcZps01HKnmU11NEOuu+uLV0yieuR8Srxeh1OeUqp3/OWmGz\nMWuJPlqqEb2uVkaytoprKTcRERGRIkWORUREREQSTY5FRERERJKmTauoVVJ6RCUvQKunMlS9nh6R\npyZYOS3z1tqRzi30lXasy4r2Cr9TZCkQaYu84jJvlZSiURkdjuvqS7sBtC1K96nmYyi1pU5T+kc1\nT3uopeXZ6ika5bauvK96kV61nr6RD76WCgW9OpKeLx9DuTTZWnEiIiIiC5cixyIiIiIiSdNGjq0e\nWW1pyw/W0nJrKbBaK41lTaW0AUe92K5WLFxLUeiW9hRxJo/2VutLrKVCOfe8rVZNfdWXgCvs6jFW\niWOttUJke3wo7lOPBNfy311aWuOfyrxeVJhHjqu1WD6uNpH690L0On3cmiLIo4WNP6opoi0iIiIi\nQZFjEREREZGkeSPHaWm2iVJxq+fI4W1Jy6fVavmGHVVLucbp89pE3lZLm3JMjEeU1wrR4frGIPVd\nnYtLs5VS7nCpJY2lsATc/hS1XdqZ5wB3EB+P1zejLizlVt+WulwPe4/l96mkjT3KLRFNLm7uUa3G\nfarpum3785EPHYjXtYiIiIgIKHIsIg3M7HYz88Ofecz36TEzN7Ob5vpeIiIiM6XJsYiIiIhI0rRp\nFRNjUdyGLc6OlauRDjFRXyqtJS9q87QUG54K39rztprXC+siNaGzlBfR+UTqsxq/ZxjtWVtLa0qr\nSDvYWbkja+tq6QTg0d0D2bHvfvfHAFx31YUAnLGqO2sbb9jdLxsv+fJsJav/rpO31Qv4arUY57LF\neVv7ovxrI1Lwa0DXYc+Sw7p3az89b/7Xg471vu/Z8zQaERGZiaadHIvI0XH3zfM9BhERkfnStJPj\naoqUlgpR1GqqmpsoRXR3orDJxlglIrI798br5q3bs7bND28CoL0zgmkrF+UFbxtOPwWAU9YuB6C1\nJb9fdwoUW4oqHxg7kLUdmIjo80OP5ffZeSCODY9EHwMH8qK7vpF4nm17Y4m5Re35GC44K+49sj+q\n7UqlPHptadm51lQUuLgrH19nTZuALBRm9nLgOcClwDpgAvgR8DF3/0zDubcDV7m7FY5dDdwG3Ah8\nGXgn8FRgOXCmu/eaWW86/YnAu4H/AawEHgb+HPiwux82l9nMzgNeCfwscAawBNgBfBX4Q3ff0nB+\ncWz/lO59JdAG/DfwFnf/ziT3aQFeRUTKH0d8P7wf+Cvgo+6u/4OIiCxATTs5FpGDfAz4CfANYDsx\naf154NNmdr67v2OG/TwVeAvwLeCTwCpgvNDeBvwHsAy4OX3+y8CHgPOB187gHr8EvJqY8H4n9X8R\n8BvAc8zsye6+dZLrngz8L+A/gU8AG9K9v2Zml7j7/fUTzawV+BfgOmJC/DlgFLgG+DDwFOClMxgr\nZnbnFE0XzOR6ERE5sTTt5LialkWzwiYbVYsNQWrltOTZeN42kn6837elD4BPf+7fsraHHnoIgO6O\nJQAsX56nYz7uonMBeNVLfgGArlIhUj0WUdu77n4YgC/delvWtmX7XgAOjOdLxq1cERHge777QwAW\n5SnKeNrUZHd/DPTs9Suztje9/BoAVi1KkfHi5h4pR3l8NKLQ1pJ3OjGeb3QiTe9id99UPGBmbcBX\ngDeb2Z9PMeFsdC3wanf/+BTt64hI8cXuPpbu804igvsaM/u8u3/jMPf4NPCB+vWF8V6bxvt24Lcm\nue7ZwCvc/abCNb9JRK1fD7ymcO7biInx/wXe4Gn3HjMrA38BvNLM/t7dbznMWEVEpMlotQqRBaBx\nYpyOjQMfIX5J/pkZdnX3NBPjurcUJ7buvg94V/r0FTMY69bGiXE6fivwY2JSO5lvFyfGySeBCnB5\n/YDFIui/TaRqvNEL21qmj38XcOBXDzfWdM1lk/0H3DeT60VE5MTStJFjEcmZ2Qbg94lJ8Aags+GU\n02bY1fcO014hUiEa3Z5eLz3cDczMiInpy4n85eUUl2A5OI2j6PuNB9x9wsx2pj7qziPSSh4E3m5m\njZcBjAAXHm6sIiLSfJp2clyv+qmO5WkLLWkzuraWSDvoWLwkb2uPn72nronUi2Xd+c51o8ODAIyN\nRDDLy/l1D26KlIv2lkhbaG/Nf253d0Sf2/dECsXuwbasrW3pOgDGh/MivUXLTwXg7HPPAWBw786s\nrStV951aih/kKxbnqR17h6Pf1ctizOXWwj9rKjqcqMb4itVQ1Tnf5kFOBGZ2FjGpXQ58E7gV6Aeq\nQA/wMiisQTi9HYdp31OMxE5y3dIZ3OP9wBuI3OivAluJySrEhPmMKa7rm+J4hYMn1/WcpHOJwsKp\nLJrBWEVEpMk07eRYRDJvIiaEr2hMOzCzFxOT45k63K9Uq8ysPMkE+ZT02j/dxWa2BngdcC/wNHcf\nnGS8x6o+hn9091+ahf5ERKSJNO3kuDoRgaZydx6oGk8LM02Mxs/3ttWnZ22l7ki/bunfDcDewTwI\nNTYSG4p4Kb5cXQfyCHDlQNynZXH8VXp5677CdftTXxFx/o1fe17WtuqUiBxX2vMo9GBf3POpl/8U\nAFsezdNEN6xfC8DQcPTVaUNZ2/CBGPPwRCwL19map5LXUqS5vS2CYJVCEZ51KTC2QJyTXr84SdtV\ns3yvFuBpRIS66Or0+oPDXH8WUQtx6yQT4/Wp/VjdR0SZrzCzVnefONwFR+vi05Zypzb9EBE5qagg\nT6T59abXq4sHzew6Ynm02fZeM8vSNMxsBbHCBMBfH+ba3vT69LRyRL2PRcBfMgu/0Lt7hViubR3w\nZ2bWmH+Nma0zs8cd671EROTk07SRYxHJfJRYJeLvzOyLRA7vxcCzgC8AL5zFe20n8pfvNbN/BlqB\n5xMT0Y8ebhk3d99hZjcDLwLuNrNbiTzlnyPWIb4buGQWxvkuotjv1cTayV8nvi5riFzkK4nl3n4y\nC/cSEZGTSNNOjusF6APDeRrBg49FUdqesT0APP26bHUndg/HX1Z3p2D6WKF+xy2OWUqrmFh+Sta2\n7NJIgWhZ3QNAf3+eavnwvii2G/AITK1fuThrW74qiufXnXluduyxx2Ljrwcf+BEApXKe3vlIb6yV\nPDwa/Y8M5HVRnS3xsEva4xl6Vuf/rKVKFB+2lOIZqpVCoX+paf/5pcDdf2hm1wB/RGz80QLcQ2y2\n0cfsTo7HiZ3t3kNMcFcR6x6/j4jWzsSvp2teSGwashv4Z+APmDw15IilVSyeB7yEKPL7BaIAbzfw\nCPAO4LOzcS8RETm5aHYksgCk7ZOfOUWzNZx79STX39543jT36icmtdPuhufuvZP16e7DRNT2bZNc\ndsRjc/eeKY47seHIp6cbp4iILCxNOzkulePRvnX7xuzYpz8bO9S1t0fbzgf3ZG1bByLCPFCLqr2x\nvryo/tzzIvVwpBLFfZXzz8zaRp5wGQAP7BoA4Pu9+d4Fl54Vy6RefGH0/a4P5HsntLenpdlOXZcd\na+uIcT3ycESJK7X8eQb2R3GfpzlA/+BA1lZLu9+9+HlPA+BVL7s2a6tUI2o9NhZRZatVsrb610hE\nREREggryRERERESSpg0d7h2MSOk3duVLsu1YnpZgG4+NQT5/y+eztrEDEabdkyLHtbE8N/e0tOxa\nbTg28xj90e6sbWgkPv7Gnl4A7h3OV4Vqs8hH7hqIZdc2btqctY2MRC7w8B3fyo45ce/u7u64X2GX\njr6+iByXS5ELXW7Jc6IXL45c5gd6Y9OQe+/fnn8hyilCvW4VACva2wptM933QURERGRhaNrJsYgc\nX1Pl9oqIiJxMlFYhIiIiIpI0beR492ikGmx+cFd2bGIklkErL10BgFfzorb2lfF7wtIVsWPd2Hi+\nJNvOtMPdeCnSI6oDo1nbIz+MtIotP/wvAIzWrO2xO6IAsO1AFPcN7ssLAGtpeTgv5UX27e1dAJxy\nWuy2N9CfFwX2DcbHi5dFUWC5nP9eMzoa4/n2D6L4cNv+/Lla2yKt4vqnXwrAS55/ddZmrYfsfSAi\nIiKyoClyLCIiIiKSNG3keMPaiBz3dHRkxx7ZGcV5yybShhhDQ1nbov441rU7osSV9vz3hom09Nv4\n2ihqG1+f32eiFgV4o9VYIs225X0O7XkMgP5U9+ZL82K4ylAs71YdzZd+q45HEeCWR3qjz8IGJl6J\n/kcHY2OR4qKuB8ZScV9/RIy3bNmZ95le9/dHMeE1z8w3F1uzNL+3iIiIiChyLCIiIiKSadrI8aKW\nyMNddery7Nj+wYjqDtQjxpbvstHeErnC3a0R5u1qy6O8raVYUm1J2g66mO87ntpqLfGlXLR6bdZW\nXhfnbfWIAI95vjTbmEckuHPpkuxY/76IbI8Px9jLE4Xxpch0NS01V6rmbV2tK+oPEdd15nnPLW3x\nPP2LY3m4Hz3yUNZ21flnISIiIiI5RY5FRERERBJNjkVEREREkqZNq2hfvAaAF/yPn8+OndlzOgA/\nua8XgDt/9GDWtnnLNgB2pKK2scIOefXit46OSFdYsmhx1taV0hY6LM5f1JVf194ZqRmrtka6RIk8\nFcKWxrJtPWfm1X0/2T0YY6hEGV3V8l3wSh2x7Nr6czYAsOWRx7K2wf5I96ASBXZtQ/lSc10exXqd\n47Gk3eb1P8jauDgvzhMRERERRY5FZAEysx4zczO7ab7HIiIiJ5amjRyPpyXSKiP50mpPfvy5AFx8\nQUSQn/2cn87aBgYj2nrvjx8A4CcPbM3aejdtBmDrti0AbN+Vb+aBp2hwOaK87bt3ZE2L2yPau3jR\nIgDWn3pa1rZ8SRQKPvbDvECurS8K91aPp1h1Yb222nhEhxdVIwJ82khe3Dc6Gv+MrelQi+cXruiO\nQrx156wE4MJLr8zayi1N+88vJwAz6wEeAf7G3V8+r4MRERGZIUWORURERESSpg0dWi1tf1HOt0je\ntGUfAIu6It932Yp8mbe21sgVftLjI4f4qZc/OWsbSZtxbHr4EQB2F7Zn3vLY9mjr7QVg69Y8qty3\nP6LWe/tji+mte/IodkdXfbm1PA+5jYg+l9LvLKVyHh2u1WIpt307IjLd3dWetZXaIlJs5RRBbs1z\nlZeeFseu/+WrAXj61Xnk2Cr5JiMiIiIiosixiMwBM7uBSKkAeFnK763/93Izuzp9fIOZXW5m/2pm\n+9KxntSHm9ntU/R/U/HchrbLzezzZrbVzMbMbLuZ3Wpm/98Mxl0ysz9Lff+DmXUc7hoREWkuTRs5\nFpF5dTuwDHg9cA/wT4W2u1MbwFOBtwDfAj4JrALGOUpm9v8DHyN2Tv9n4EFgDfBk4DXAF6a5tgP4\nDPDLwEeA17l7barzRUSkOTXt5LhtySoANnQuyo51L4ul0oZHo/iuK+0aBzAxFgV8Xe2xY125Nd9l\nbm8pUiU2bJgA4HHn5ykNbT/9U/FBOa7fu2df1nbfA7E83MMP9gJw7wOPZm1bt8fybgP9w9mxgUp9\nTlD/eZynVeTVeX7QZ0X1PwO0FY51t8TXYe3KU+LzrhVZW6UyhMhccPfbzayXmBzf7e43FNvN7Or0\n4bXAq93948d6TzN7HPBRYAB4hrv/uKF9/aQXRtsK4BbgSuDN7v6/j+C+d07RdMFM+xARkRNH006O\nReSkcPdsTIyT3yK+p72rcWIM4O5bJrvIzM4A/g04G3ipu392lsYjIiInoaadHJdSGHVkbH92zFqi\nUG1JWlqtkjbNAKhORHHa2EhElSvjeWx2bDwirNWoiWPTQ/kyb9VKnH/qmtUAnHnWGVnbUy+/PM4Z\niTHs6u/L2jY/1g/Ag5vyaPK998WmJJt6o8ivry+P7A4OpqXpJiJ6XatOZG1e8/TM8dBtpTzi3HP+\nRQCcfv4TACh3L83aRvvzwkKRefK9WezrivT6lSO45nzgP4Fu4Hp3/9qR3tTdL5vseIooP+lI+xMR\nkfmlgjwRmU87Dn/KjNXzmLdOe9bBzgPWAQ8Dd83iWERE5CTVtJHj2njk8i7uypdyK0/EEm4jwxF1\nrYxXsraOzsg1ro5Fvu/g/vxndj0fuVaJ61aszLeP3rMnIscP9j4MwN7+PNq75OG9AFhLXHfeeRdm\nbevXx6Ycp556anbsRb/8izG+tIFJf18e9R4ejWjw2HjkJY9bXidUKkUOtFm8tnXmBfZnnxP3PHXD\n6en59ubXtakQX+adH6Ztqu9RyyY5Vv/TzGnAfTO8/78A9wPvAb5mZte6+57DXCMiIk1MkWMRmStp\nsXHK0541tf3A6Y0HzawMXDLJ+d9Nr9cfyU3c/b3AG4FLgdvMbO0RjlNERJqIJsciMlf2E9HfDUd5\n/feADWZ2bcPxtwNnTHL+x4AK8I60csVBplutwt0/SBT0XQTcYWanTnWuiIg0t6ZNqxgbj7SD/UOF\norYDsZRbuTPSDw4M9WdtlYH4i2xra6RhjIzkKRcDg3Hd2FikUCxakv9Fd9nyNQDULJZ+61ySF7zV\nPH73aLFI59izJy+A27UjCuf3D+RFeisWxzJra1ZHcV9LWx5wW7w4igjXLo57e2Ett0WdMeZSOaVJ\nlPN/1tWr4md8Oe0UOFHN+yyTL1cnMtvcfcjM/gt4hpl9FniAfP3hmfgT4DrgFjP7PLAPeBpwJrGO\n8tUN9/uJmb0G+HPgB2Z2C7HO8UpineNB4JppxvvnZjYK/BXwDTN7prtvnuFYRUSkSTTt5FhETggv\nBT4APAt4MbFE9xag93AXuvvXzOx5wB8ALwIOAP8OvBC4cYpr/tLM7gV+j5g8Pw/YA/wQ+MQM7nmT\nmY0BnyKfID98uOum0LNx40Yuu2zSxSxEROQwNm7cCNBzvO9r7tPVw4iIyNFIk+wysUOgyHyqb0gz\n00JVkbl0JO/HHmDA3c+cu+EcSpFjEZG5cS9MvQ6yyPFS38VR70U5EZwM70cV5ImIiIiIJJoci4iI\niIgkmhyLiIiIiCSaHIuIiIiIJJoci4iIiIgkWspNRERERCRR5FhEREREJNHkWEREREQk0eRYRERE\nRCTR5FhEREREJNHkWEREREQk0eRYRERERCTR5FhEREREJNHkWEREREQk0eRYRGQGzGy9mX3SzLaZ\n2ZiZ9ZrZB81s+RH2syJd15v62Zb6XT9XY5fmMxvvRzO73cx8mv865vIZpDmY2fPN7MNm9k0zG0jv\nnc8cZV+z8n32WLUcz5uJiJyMzOxs4DvAGuAW4D7gcuD1wLPM7Ep33zuDflamfs4Dvg7cDFwAvAJ4\ntpk91d0fnpunkGYxW+/HghunOF45poHKQvF24InAELCF+J52xObgfX3UNDkWETm8jxJcyAVRAAAg\nAElEQVTfsF/n7h+uHzSz9wNvBN4NvHoG/byHmBh/wN3fVOjndcCH0n2eNYvjluY0W+9HANz9htke\noCwobyQmxQ8BVwG3HWU/s/q+Phbm7sfjPiIiJyUzOwvYBPQCZ7t7rdC2GNgOGLDG3Q9M0083sBuo\nAevcfbDQVkr36En3UPRYJjVb78d0/u3AVe5uczZgWVDM7GpicvxZd3/JEVw3a+/r2aCcYxGR6T0z\nvd5a/IYNkCa43wa6gCsO089TgU7g28WJceqnBtyaPr3mmEcszWy23o8ZM3uhmb3ZzN5kZtebWfvs\nDVdkRmb9fX0sNDkWEZne+en1gSnaH0yv5x2nfmRhm4v30c3Ae4E/Bb4MbDaz5x/d8ESOygn1/VGT\nYxGR6S1Nr/1TtNePLztO/cjCNpvvo1uA5wDrib9qXEBMkpcBnzez649hnCJH4oT6/qiCPBGRY1PP\n1zzWAo7Z6kcWthm/j9z9Aw2H7gfeambbgA8TBaRfmd3hiRyV4/r9UZFjEZHp1SMWS6doX9Jw3lz3\nIwvb8XgffYJYxu2SVAwlMtdOqO+PmhyLiEzv/vQ6Va7buel1qly52e5HFrY5fx+5+yhQLxrtPtp+\nRI7ACfX9UZNjEZHp1dfsvDYtuZZJUbUrgRHgu4fp57vpvCsbo3Gp32sb7icymdl6P07JzM4HlhMT\n5D1H24/IEZjz9/WR0ORYRGQa7r6JWGatB3htQ/ONRGTtU8W1N83sAjM7aJcodx8CPp3Ov6Ghn99O\n/X9VaxzLdGbr/WhmZ5nZaY39m9kq4K/Tpze7u3bJk1ljZq3p/Xh28fjRvK/ndJzaBEREZHqTbGu6\nEXgKsSbxA8DTituampkDNG6uMMn20d8DLgSeC+xK/Wya6+eRk9tsvB/N7OVEbvEdxOYL+4ANwM8T\neZ/fB37O3fvm/onkZGZmzwOelz49BbgOeBj4Zjq2x91/L53bAzwCPOruPQ39HNH7ei5pciwiMgNm\ndjrwh8T2ziuJHZv+CbjR3fc1nDvp5Di1rQDeSfwwWQfsJVYE+AN33zKXzyDN41jfj2b2eOB3gcuA\nU4mCp0Hgx8AXgI+7+/jcP4mc7MzsBuJ72lSyifB0k+PUPuP39VzS5FhEREREJFHOsYiIiIhIosmx\niIiIiEiiyfE0zGyxmb3fzDaZ2biZuZn1zve4RERERGRuaPvo6f0D8LPp4wGimnf3/A1HREREROaS\nCvKmYGYXAfcCE8BPu/txWXhaREREROaP0iqmdlF6/aEmxiIiIiILgybHU+tMr0PzOgoREREROW40\nOW5gZjekBdNvSoeuSoV49f+urp9jZjeZWcnMftvMvmdmfen4JQ19XmpmnzGzx8xszMz2mNlXzeyX\nDzOWspm9wcx+aGYjZrbbzL5kZlem9vqYeubgSyEiIiKy4Kgg71BDwE4icryEyDku7spS3DHIiKK9\n5wJVYnehg5jZq4CPkf8i0gcsA64FrjWzzwAvd/dqw3WtxPaJ16dDFeLf69nAdWb2oqN/RBERERGZ\njCLHDdz9T9z9FOD16dB33P2Uwn/fKZz+S8QWh68Blrj7cmAtsac4ZvY08onx3wOnp3OWAW8DHHgJ\n8JZJhvJ2YmJcBd5Q6L8H+DfgE7P31CIiIiICmhwfq0XA69z9Y+4+DODuu9x9ILW/i/gafxt4kbtv\nSecMuft7gPel837fzJbUOzWzRcSe9wB/4O4fcveRdO2jxKT80Tl+NhEREZEFR5PjY7MX+ORkDWa2\nArgmffrexrSJ5H8Do8Qk++cLx68DulPbnzVe5O4TwPuPftgiIiIiMhlNjo/N9929MkXbpUROsgN3\nTHaCu/cDd6ZPn9RwLcDd7j7VahnfPMKxioiIiMhhaHJ8bKbbLW91eu2fZoILsKXhfIBV6XX7NNdt\nO8zYREREROQIaXJ8bCZLlWjUfhT92gzO0daGIiIiIrNMk+O5U48qd5rZ6mnOW99wfvHjddNcd+rR\nDkxEREREJqfJ8dz5AXl095rJTjCzpcBl6dO7Gq4FuCStXDGZZxzzCEVERETkIJoczxF33wfclj79\nfTOb7Gv9+0AHsfHIlwvHbwUOpLbXNl5kZi3AG2d1wCIiIiKiyfEcewdQI1aiuNnM1kOsY2xmbwXe\nnM57X2FtZNx9EPhA+vSPzOx3zKwzXbuB2FDkzOP0DCIiIiILhibHcyjtpvcaYoL8AmCzme0jtpB+\nN1F491nyzUCK3kVEkFuItY7707WPEmsiv7Jw7thcPYOIiIjIQqLJ8Rxz948DPwV8jliabRHQD/w7\n8AJ3f8lkG4S4+zjwbGKnvHuJCXYV+Bfgp8lTNiAm2yIiIiJyjMxdK4KdjMzsZ4D/AB519555Ho6I\niIhIU1Dk+OT1P9Prv8/rKERERESaiCbHJygzK5vZ35vZs9KSb/XjF5nZ3wPXARNEPrKIiIiIzAKl\nVZyg0nJtE4VDA0RxXlf6vAb8lrv/xfEem4iIiEiz0uT4BGVmBryaiBA/HlgDtAI7gG8AH3T3u6bu\nQURERESOlCbHIiIiIiKJco5FRERERBJNjkVEREREEk2ORUREREQSTY5FRERERBJNjkVEREREkpb5\nHoCISDMys0eAJUDvPA9FRORk1QMMuPuZx/OmTTs5/sDHv+sAg1XLjh0YXwnAzs0jAJy2uiNre8Kl\n8aUotcbSdh2L8uuqtRoA+/bEObffsjdrGxqMvlb3rABgxWn5dctXRmC+qxzX7Xl0Z9ZmpTIAlUr+\nT9DaEfe56InR1/LF3Vnbpo3DANx99w4AJryctS1dEc/V3z8EwOmntWZtiztiPK1tVQBOOS3bbI8f\n3R3PccObL8kHLSKzZUlnZ+eKCy+8cMV8D0RE5GS0ceNGRkZGjvt9m3ZyXLNVAFRG83Wcy6VBAM67\nYDkA/fvySa63xgTz0e0xCa1V876WLlkCwO5dFQC6VnRlbQOj0ce69XH96g35l3TH1mjbvbct+uls\nz9oq1fjHbrG8r9pYXDvYF+czUSn0Fc9RGeuM+6ztzMd3Wrxa22oAtj80mrUdWBzPc/bjY1L8o7v3\nZ239+2uInGjM7HXEBjhnAh3AG939g/M7qqPSe+GFF664884753scIiInpcsuu4y77rqr93jft2kn\nxyJy8jGzFwEf4v+1d+dBdl7lnce/z73dfXtfpdYutSQvko1jbBOMcQAzDNtABhKYYQYyg6GSChkI\nW0gVASaYJCxFMsQpGIrMAGEJA0wNEGZYBlIBB2PigG2WsS3bsmztS+/7crtvn/njOfc9N51uqSW1\npNbt36eq63a/z7nnPVe61X366eecAz8F7gRmgHsv6aBERGRN0eRYRFaTl5YfQwjHL+lIVsCDx0bo\neec3L/UwRETO28EPveRSD+GiqdrJcVOHlzDs29+bXZuc9DKCPdd6SUKplEoa9j/sj6dOesnF+Oh4\nFsvlvRyjhNf51qeKBnbv2QxAS6vXF9fVpDrmMOcN8zl/bGhKZRWHDj3p12rTf8HUpNdyzBQHAdi5\nszWLzZZiWcW8tx/oTaUT+Ubvf7jfxzk1mmqOG1u8/fETswA8/HCqe965czMiq8xmgGqYGIuIyOVJ\nW7mJyCVnZneYWQCeG78O5Y+Kr+8ys41m9kkzO2ZmJTO7vaKPTWb2X83soJkVzazPzL5qZjctcc82\nM7vTzI6a2bSZPWJmbzezXfF+n7kIL11ERFaZqs0c9w+OAFBXSDs+jIx6ZvbBh48C0NnckcUmBz0z\nO+HJV+ZDWshXF3euaIiP19yQUse1uSIAw4OemT16oJj6HPFMcY15xvrIyeEsdiImcNub0n9Bc8wA\nTwyO+vO70iYS6zd7XyeHvP/R/nSf1n5fwDc37jtZtGxsyWKN6zz7PDfvi/s6utJrnp1Nr1HkErsr\nPt4O7ADet0ibTrz+eBz4KjAPnAIws53AD/HM8/eALwLbgH8DvMTMXhFC+Ea5IzOrj+1uxOubvwC0\nAe8GnnU2AzezpVbc7TmbfkREZHWo2smxiFw+Qgh3AXeZ2W3AjhDCHYs0uw74PPD6EMLcgtgn8Inx\ne0II7y9fNLOPAz8APmtmO0II5Xqp38cnxl8CXh1CKGeo3w88sFKvS0RELj9VOzke7fVMa10uZVGv\nuS5mh2f6AShNpK3MCvlYMxy3PqvJpYqTdeu8jjjU+M/jukKK1eY9Mz024tu2TYykMUzF/uvq/Hnz\nIdUjN+S93nd2JtUHT+d9fDbrtdBHD6Wf/1uu8HY7r/At2fbPTGSx+ZgVvumXfU+3/vH+LDZw0rPl\nAU+Jb9iaxoAyx3J5KQLvWDgxNrOtwAuAw8CHK2MhhB+Z2ReB3wB+HfhcDL0Wzzz/QXliHNsfMbM7\ngT9Z7qBCCEuVbdyPT8BFROQyoppjEblcHAwh9C5y/Yb4eHcIYXaR+Pcq25lZK7AbOBZCOLhI+x+e\n70BFROTypcmxiFwuTi5xvXzs44kl4uXr7fGxvA3MqUXanu66iIisAVVbVjE0UD5eeTK71t7p1zob\nvbRgYq7ihDjz8gOLi+esLsVaun3B23zwUo0jh9NRhpNj/hfZmTEve5ibTferb/B/3nh6NKXZlNTK\nxb/k5vPpWjDvI1/nC/OGRlNfhSG/Z8d6/31m4/a0KHCobziO2csqCrn2LBamvPyitctf37r16djp\n5sZmRC4jS9UBlYuZNi4R37Sg3Wh83LBE+6Wui4jIGlC1k2MRWTN+Gh9/xcxqFlms99z4+ABACGHU\nzJ4AesysZ5HSil9ZqYE9ZUsb96+hjfNFRKpB1U6Oi1OeWW3rSAvX8vOeeBo64VnY/r4UIx7GMTTu\nC9c6NqUDO5rjlmoT4zMA5PIpNj3jmdiAZ5cLTSkTnM979rmroxOAmYmUCc7Pega4UEh9nRrwDHDR\nk7yULP2MH530MdfNeNa7qS1ljsdjdvihhw4B0FpI27Xt3OFj3rLNx9C+OWWVDx/WX4/l8hdCOGpm\nfws8H3gr8GflmJndDLwaGAK+VvG0zwF3AB80s8rdKrbFPkREZI2q2smxiKwpbwDuAf7UzF4A3Efa\n53geeF0IYayi/YeBlwP/DrjazL6L1y7/W3zrt5fH54mIyBqjBXkictkLITwBPA3f7/hq4B3Ai4H/\nC9waQvj6gvZTeLnFR/Fa5bfFrz8AfDA2G0VERNacqs0cFxq81KC9aya71hhPoBsZ8MepmZQYqs15\n6UR9vZdHrOvanMVOHve+Tp44DMDWramkobHRyxRCnZdXrO9uy2LFSb/Wd9ITVvmKU/fCvMeKM2lx\nX45YopHzdm1t6XS/XK33MTXtpRbtzen0vCuv8ntOxrKPjR1p0d2uLesBePKA/5wfGUzPK79WkdUi\nhHDbEtdtsesL2hwDfucs7jUMvDl+ZMzst+Kn+5bbl4iIVA9ljkVkTTKzzYtc2wb8Z2AO+MY/e5KI\niFS9qs0c3/h03xat0JgSTr2n/OTYyZgwHpxMWds2P5SOW27Z6m2milns4BN9AOTnvH096XnbN8XF\nb3G/tvq6lB2ejSfpzccM9fxcGkuuqbx1XFp0Z3FtXrF8cl0+3Wfd+vJ2ct5+767WLLZhiz9xZMLH\nPDueFtp1dHlmu2/A73ek93gW231VDyJr2FfMrBa4HxgGeoCXAo34yXnHLuHYRETkEqnaybGIyBl8\nHvgPwCvwxXjjwD8CHwshfPVSDkxERC6dqp0c77rSs6kDgymT29DktcIt8TAQOzWcxVq6PcvbP+CZ\n1bp8yuhetdufNzvltb3rO1MGuDbn2drpWDs8Op4WxLe3rgNgw0Yfw+RERZY4eCY3WMpQN3eWDw3x\nLPFMKpdmS4+nthuaY0a84vCQoeFpH0uDv+a2DanueSo2W7fFM80tG9N/+exsxQ1E1pgQwseBj1/q\ncYiIyOqimmMRERERkUiTYxERERGRqGrLKo4c9EVpxWLaDm1oONYY5Ly8IV+XyipGJj3W3bQdgLm5\n8SyWiwvjSiUvQ5iYSKfabd3WDEDTnJdOHD8+kMXGJwa97xEvtajYtY2mZj/prq6Qfj/J5fwEvUKj\n99/d3ZnFCnV+75EBv8+9j4xksclJf96mWDJxyzO2ZbH5eS+5KG+EVVdIZSZWvf/9IiIiIudEmWMR\nERERkahqU4eTw77obnx6Mrs2MOKp27omf9k9u7qzWG2dp1bn4yK1ieneLDZT8mxrU/0GAEYqFt3N\nzHn74owvsJuamchiuZrYp3lsrvIcg5z/XlKqOBjELB4MUowLBmvSorvygSVf+bKfS3CsP23ldv0v\neYb5b//3fQB0dTZnsd3X+OfFuBhwfi79PtTcVI+IiIiIJMoci4iIiIhEVZs5HhrwjPHsfNo+bffu\nHQD8fN8RAOobUvZ1fYfXJk/0Pg7A1OxQFisUvN3WHZ45bihsyWITE35AyJFjXuPc1NSSxWbjdm31\nTd73TDFt2zY+7Vlsy6XxtbT4lnF58yzx5HQ63jqX8/+qXVft9PGRssoDg32xf69VPnEi/Tu0dnvN\ncVOdP7+lIlucz5UQERERkUSZYxERERGRSJNjEREREZGoassqpmd8UVtjQ9p2rfeYL6QbG/HfCaan\n03ZtO7d76UOpzk+gGxxPC/lCyUsf+vu95MLma7NYe6sveGtt98V9c3OpdKI452UL42O+7ZqRShoK\nNXHRXcWvJ9PlE+vi/0rNdBp7qeQlFoeP+Al+J59M42vr8DF0bdoLwM9+kkpChoZ8PM989lZv05nK\nOOZmU2mGyGphZgcBQgg9l3YkIiKyFilzLCIiIiISVW3muKmlA4DSZMoOl+Y9k7tlk299duj4Y1ks\nb96e4Fnhhsb1Wexk36MADIz5oR7MpQzw7p2erd204woAhof7s1hxzjOzs3NxG7VSyvaWgmeOa1MS\nmkKNf5HL+WPlASF9ff7cE8c8u7x769YsdujYSQA6N3p2uamQFtrV1pRv4H2FkDLHOdPvRiIiIiKV\nqnZyLCJyqT14bISed37zot7z4IdeclHvJyJSbZQ6FJGLztybzOwhM5s2s2Nm9jEzazvNc/69mX3f\nzIbic/aZ2XvMrLBE+z1m9hkzO2JmM2Z2ysz+h5ldvUjbz5hZMLNdZva7ZvYLM5sys7tW8GWLiMhl\noGozx02tXj4wU1FGUIeXHYS41+/xI/ksVhzzWHHef1/o7NmbxRrW+c/ewwcOANDSkvYyno97GZ84\ndBiAQm1dFisvlJue9ZKIsdF0el4N3uf0dBpfwJ8b5v3UvPamdNJdY73vgVzfHMs+Oip+rznm7Qs1\n/rj3uo1ZaOcV3uf8vC9GzFtXup+l0/lELrI7gTcDJ4D/BswCLwNuBuqAYmVjM/sU8HrgKPBVYBh4\nBvDHwPPM7PmhombIzF4U29UC/wd4HNgK/DrwEjN7bgjhgUXG9RfAs4BvAt8CtBm4iMgaU7WTYxFZ\nnczsmfjE+ADw9BDCYLz+buD7wCbgUEX72/GJ8deA14QQpipidwDvBd6IT2wxsw7gi8Ak8OwQwsMV\n7a8F/hH4JHDjIsO7EbghhPDkWbye+5cI7VluHyIisnpU7eR4cmYAgLbWpuzazIQBcPKUL5rbvGFd\nFpud8ZPkauo8m9zclrK21+65BYBNXX5CXl/fQBYbGRkF4PiRRwDYvevKLFYMnjGemBgGIJ9L/9y5\nmLWtrUsr8uZLnqQqxcfR2DdAY30jAFfs8IWDBw+PZLHRGZ8rzMYt504cm85ihXofw46e3D+5B8D8\nXDqBT+Qiel18fH95YgwQQpg2sz/AJ8iV3gLMAa+vnBhHfwy8CXgNcXIM/EegHXhT5cQ43uMhM/vv\nwFvN7JqFceDDZzMxFhGR6lO1k2MRWbXKGdu/XyR2Nz4RBsDMGoHrgX58QrtYfzPA3oqvb4mP18fM\n8kJXxce9wMLJ8Y9PN/DFhBBuWux6zCgvlp0WEZFVrGonx52xTnhda6oPPnbAM6rlUtudPSlz3Hfi\niH8SS4a72lLd7nB/n1/r8C3gavLpn6250WuBmYvZ2lzKzI6Pe3b32GH/C/G11z4li83O+s//fMV2\naq0trf5J3rPJoWIiUFfng962yTPIP7g7Jbd2XOHbyHWu99jjjz+axdo7PNt94w3b/MJ8qnGeLylz\nLJdEedHdqYWBEELJzAYqLnUABqzHyyeWo1xY/1tnaNe8yLWTy7yHiIhUKe1WISIXW7kmaMPCgJnl\nSZPbyrY/DSHY6T4Wec71Z3jOZxcZm1apioiscZoci8jFVt4l4jmLxJ5FxV+0QgjjwEPAtWbWucz+\n763oS0RE5KxUbVnFxg3tAKzvTmUV4yNerjAy46UPcyGdnjdb9HU+zQUvkzh5KJUtjPWfAOAZN18H\nwFW7t2ex8ql7Dz/kfym++x/SwvX1GzYDsGXjFgBam9PJeuNjvq3b3FwqcygVffcqiyfjtXen0o5S\nyduPTvpfnHf2pBP8puZ8gaHlvExkz7VbstimTb6Ar7XFSy6sNJPFavMVx/OJXDyfAX4TeLeZfb1i\nt4p64IOLtP8I8Cng02Z2ewhhuDIYd6fYWbE1218B7wbea2Y/CSH8eEH7HL6LxV0r+JoW9ZQtbdyv\nQzlERC4rVTs5FpHVKYRwj5l9FPhd4EEz+1+kfY6H8L2PK9t/2sxuAv4TcMDMvgMcBjqBncCz8Qnx\nG2L7ATN7Jb71271m9nd49nke2I4v2OsC6hEREVmgaifHLU2eAS6GtK3ZVPw8X+8Z01xNyqK2NPrP\nydycZ4LrQyo9bO/2LG1Lva/Wa0znfDA145nf7du2AtDxaMo4j8bs8NW7dwPQ1JwOHRns9x2sZovp\nPvmYyc3HBX8joymzPTHpybL1m7sBeFpDet4j+/cB0N3lmeonH09j6F7vWWjLeSa9Yr0gIai8Ui6Z\ntwCP4fsT/zYwgE9m3wX8fGHjEMIbzezb+AT4X+JbtQ3ik+Q/Bf56Qfu/M7NfAt4BvBAvsSgCx4Hv\nAV+5IK9KREQue1U7ORaR1Sv4b2Yfix8L9SzxnG8A3ziLexzE90BeTtvbgduX27eIiFSvqp0c954a\nAsBaCtm14XE/EKOz0w8GaW1P6xHnGjwdvP8Bz7pet/fqLLalx7dYKwWvCZ6Ymcxi5d3QOtd5Rnfr\n9t1Z7Oe/eAiAXK1njOsqDvzo7/c64abmjuzafDw2uinvY+k91pvGh8U+PIM8MNiXxfZc6a/n2is9\nez3am0oyr97r2eQQU8Yhl7LXlVvSiYiIiIh2qxARERERyWhyLCIiIiISVe3f1f/ffX4qXdvm9uxa\nV4tvt9a1wV/24HhaFD846Iv1xouzADzy6BNZbD7vJRM15QV5rU3pRjm/dv8vfFHcQ/sOZqEQ/D4t\nTd5+fVdayVdX62UOOZvNrrXFfjs6fPu5ocFUHuFnI8DAwFEACoVUHtEc+3/00ccBuOdHP81itz3f\nyzxqcl7SMTObVuSFWZ2QJyIiIlJJmWMRERERkahqM8fzc76AbejwaHatbZtv71Zq9izqiSePZ7HS\njLfvXNcMQN/gqSzW+wPPQufq/flXXXNtFhsc8f73Pe6Z5qnpqSy2pdsP9NrQ6X1e0bMuiz3vOTcD\nMDyctpPr6Nzk10Y9iz09XcxiIWaOm5tjdjluVQewd8+VPpYhz0K/9GVPz2Ibu/zeNu0Z4/Hh1Gcu\nlxYrioiIiIgyxyIiIiIiGU2ORURERESiqi2ryBW8NKGtYUt27cB+31t4oG8MgG1bd2axmrz/nvDI\n/v0AdHS1ZTHLdQEwXfTT8I4fTXsMn+z1RX1tzb7Ybuvm9Lwbr/Nyh+2b/fmH9qeT654aSzPy+VQe\ncf8DjwFw4qiPs1S0LFYs+b23dm8A4NordmSx1qZGAPr6/Pmv+LWnZLH2WApic76H8lAxLQAsxtcj\nIiIiIk6ZYxERERGRqGozx0+79RoARgfTdmV9I54xDnFHtcGhkSxWmvOLc/jWb2PTaaFcKfiiu62b\n/QS63hPp5LrixDgANbl6AAptjSk26dnrY4d9cV/v0aEsNjL4MwAaGluya7NTnt3taPNrY5PpJL5c\nrf9XdXWWs8RHstihA95u7zW+bVsYS8/72QMP+rX5uBXcqbQgb2rSX+MLn78dEREREVHmWEREREQk\nU7WZ48Ymf2nFuZQB3nWdZ4XbWnx7s/4TKXNsM56tbcn7Y3NLqs1tapkAYGLMs8SzUykzu327b782\nOultZsZTZrb32CAA+e71AHRv3JbFahs9k/vAT36RXeto8HrijZt9y7exyfEsNjfn/R7e/5CPoWKb\nt6uu8NrpHdv9PuNDE1ns0f3HAJie8N+D6gv16XU1VhxmIiIiIiLKHIuIiIiIlGlyLCKrhpn1mFkw\ns88ss/3tsf3tKziG22Kfd6xUnyIicvmo2rKKfQ/5grUtOzZk11ra/US4lhYvnejrHctirV1e5rC+\n1ssOagtpId/AsJdHhDpvU9fZkcWuvN63ZOsfGPAL49NZrGbW+wiz/s88mw/pfs2tAFy1J52299gD\njwNwqvckAPn22izWVvDPW9r81L2TpYEs9tQb9vp98HvXNKTnPfWXnwHAoceHPVaXxlAo6IQ8ERER\nkUpVOzkWkTXha8C9wIlLPRAREakOVTs5bmry7O7+ioM31m/wLcv6T/pWbPl8OmRjS49nk4tTUwCM\nT6SFfOQ9mzwy4lu60ZAWsh0b8sV5U5OeJS5Mp8xse3zecJ/32dBSl8V6Bw8CkEYAW7f7gr0DB/ww\nj43rUoa6vd6fOzPhCwWn56ay2FTRF+A1lDwbPVNxuMdU0RfuNbZ7rKEiqzxRsXhQ5HIUQhgBRs7Y\nUEREZJlUcywiq5KZ7TGzvzGzQTObMLMfmtkLFrRZtObYzA7Gj1Yz+0j8fLayjtjMNpjZp8zslJlN\nmdnPzOy1F+fViYjIalW1meO2dV5P2zuYsrVjQyUAcjGj29yasqi9fV5X3NTo/yTru9PhHLU1ng1u\nqPUt4E72p63cRvq9brk2733X1aZccGuLH91cnPDY2EjKRg+Meg2wWWpfX+P9dk5LSCMAAAjrSURB\nVK33OumWpvVZrBi3ihsY8KOl13V3Z7H+AT9cpKXVx9zb35/FxuO9C/XefmombQ/X2z8YP7sCkVVm\nJ/APwIPAXwKbgFcB3zazV4cQvryMPuqA7wGdwHeBUeBJADPrAn4E7AJ+GD82AZ+IbUVEZI2q2smx\niFzWng38WQjh98sXzOxj+IT5E2b27RDi0ZVL2wQ8DDwnhDCxIPZBfGJ8ZwjhbYvcY9nM7P4lQnvO\nph8REVkdVFYhIqvRCPBHlRdCCPcBXwDagV9bZj+/t3BibGa1wGuAMeCOJe4hIiJrVNVmjmdmvRSi\nUNuZXTt61BfiXXn1bgAGB9KiNsPLD44Xvc3e67dmsdpGL79oKvnvErU16XkzsWzhyp1dAHQ2p1KN\nkVjuYAUfS3shxRqb/RS8odG0nRzB2xXnvM/7HjiUhbo7vf+ubj+Rb64mbRkXSl46Etfe0d6RXvP0\nhCfXjh8+7LHONIaG+hIiq9QDIYSxRa7fBbwWuAH47Bn6mAZ+scj1PUAjcHdc0LfUPZYlhHDTYtdj\nRvnG5fYjIiKrgzLHIrIanVri+sn42LaMPnpDCGGR6+XnnukeIiKyBlVt5vjoEV/wNtSbyhJ3bPXs\na2Odp1hHhlIGeHLSD/jo7z8OQH1jPout644L44Jvo1YspYxrazxQZPtm33atxlJsJrbrO+p9FubS\nwSKb1nmf23avy66Nj/qCvJEBT2bl6xuz2IkTfujHwJhv09ZesWCwPi78m5j033UKNWmR39hoHwDT\nc56EK05uzmKFxuXML0QuiQ1LXN8YH5ezfdtiE+PK557pHiIisgYpcywiq9GNZtayyPXb4uNPz6Pv\nR4BJ4KlmtthviLctck1ERNYITY5FZDVqA/6w8oKZPQ1fSDeCn4x3TkIIs/iiuxYWLMiruIeIiKxR\nVVtW0dbif1Hd3NWVXdu00T+fmfL9hmsrFuvd+5NjAHR2enlEaSKVVYz1eft1m7x9XU0qnWhr99Py\n6hu8/dBAWkM0NuNj6Oz2UoaaqbQ/8nAsnRieHcqubdvsCwXr42l47RVlGAXfmpnH9vvCut6Ts1ks\nV1M+wc/LRUpT6S/ONXkvw7j2Rt9V6rEfD2exkZDKSkRWmR8Av2lmNwP3kPY5zgG/vYxt3M7kXcDz\ngLfGCXF5n+NXAd8C/vV59i8iIpepqp0ci8hl7UngDcCH4mMBeAD4oxDCd8638xBCv5ndCnwA+FXg\nacCjwO8AB1mZyXHPvn37uOmmRTezEBGRM9i3bx9Az8W+ry2+mFtERM6Hmc0AeeDnl3osIksoH1Tz\nyCUdhcjSrgdKIYTCxbypMsciIhfGg7D0Psgil1r5dEe9R2W1Os0JpBeUFuSJiIiIiESaHIuIiIiI\nRJoci4iIiIhEmhyLiIiIiESaHIuIiIiIRNrKTUREREQkUuZYRERERCTS5FhEREREJNLkWEREREQk\n0uRYRERERCTS5FhEREREJNLkWEREREQk0uRYRERERCTS5FhEZBnMbKuZfdrMjpvZjJkdNLM7zazj\nLPvpjM87GPs5HvvdeqHGLmvDSrxHzewuMwun+ai/kK9BqpeZvdLMPmpmd5vZaHw//fU59rUi34+X\nUrMSnYiIVDMz2w38COgGvg48AjwdeAvwIjO7NYQwsIx+umI/VwHfA74E7AFeB7zEzG4JITxxYV6F\nVLOVeo9WeN8S1+fOa6Cylr0HuB4YB47i3/vO2gV4r/8zmhyLiJzZx/FvxG8OIXy0fNHMPgK8DXg/\n8IZl9PMBfGL85yGEt1f082bgL+J9XrSC45a1Y6XeowCEEO5Y6QHKmvc2fFL8OPAc4Pvn2M+KvtcX\no+OjRUROw8x2AQeAg8DuEMJ8RawFOAEY0B1CmDhNP01AHzAPbAohjFXEcvEePfEeyh7Lsq3UezS2\nvwt4TgjBLtiAZc0zs9vwyfEXQgi/cRbPW7H3+umo5lhE5PT+RXz8buU3YoA4wb0HaASecYZ+bgEa\ngHsqJ8axn3ngu/HL5573iGWtWan3aMbMXmVm7zSzt5vZi82ssHLDFTlnK/5eX4wmxyIip3d1fHxs\nifj++HjVRepHZKEL8d76EvBB4L8A3wIOm9krz214Iivmonwf1eRYROT02uLjyBLx8vX2i9SPyEIr\n+d76OvCrwFb8Lx178ElyO/BlM3vxeYxT5HxdlO+jWpAnInJ+yrWZ57uAY6X6EVlo2e+tEMKfL7j0\nKPAuMzsOfBRfVPrtlR2eyIpZke+jyhyLiJxeORPRtkS8dUG7C92PyEIX4731SXwbt6fGhU8il8JF\n+T6qybGIyOk9Gh+XqmG7Mj4uVQO30v2ILHTB31shhGmgvJC06Vz7ETlPF+X7qCbHIiKnV96L8wVx\ny7VMzKDdCkwB956hn3tju1sXZt5ivy9YcD+R5Vqp9+iSzOxqoAOfIPefaz8i5+mCv9dBk2MRkdMK\nIRzAt1nrAd64IPw+PIv2uco9Nc1sj5n9k9OfQgjjwOdj+zsW9POm2P93tMexnK2Veo+a2S4z27Kw\nfzNbB/xV/PJLIQSdkicXlJnVxvfo7srr5/JeP6f76xAQEZHTW+S40n3AzfiexI8Bz6w8rtTMAsDC\ngxQWOT76x8Be4GVAb+znwIV+PVJ9VuI9ama347XFf48ftDAIbAf+FV7jeR/w/BDC8IV/RVJtzOzl\nwMvjlxuBFwJPAHfHa/0hhHfEtj3Ak8ChEELPgn7O6r1+TmPV5FhE5MzMbBvwR/jxzl34SUx/A7wv\nhDC4oO2ik+MY6wTei/+Q2AQM4Kv//zCEcPRCvgapbuf7HjWz64DfA24CNuOLm8aAh4D/CfxlCKF4\n4V+JVCMzuwP/3reUbCJ8uslxjC/7vX5OY9XkWERERETEqeZYRERERCTS5FhEREREJNLkWEREREQk\n0uRYRERERCTS5FhEREREJNLkWEREREQk0uRYRERERCTS5FhEREREJNLkWEREREQk0uRYRERERCTS\n5FhEREREJNLkWEREREQk0uRYRERERCTS5FhEREREJNLkWEREREQk0uRYRERERCTS5FhEREREJPr/\nLpRmw9TDbKsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa39c91eb00>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
